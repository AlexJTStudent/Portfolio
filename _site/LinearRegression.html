<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Linear Regression</title>

<script src="site_libs/header-attrs-2.29/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="site_libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<link href="site_libs/crosstalk-1.2.2/css/crosstalk.min.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.2.2/js/crosstalk.min.js"></script>
<link href="site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="site_libs/plotly-main-2.11.1/plotly-latest.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"><img src='Images/snlogo.png' alt='Statistics Notebook Logo' style='height:30px; margin:-5px 0px'></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    R Help
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="RCommands.html">R Commands</a>
    </li>
    <li>
      <a href="RMarkdownHints.html">R Markdown Hints</a>
    </li>
    <li>
      <a href="RCheatSheetsAndNotes.html">R Cheatsheets &amp; Notes</a>
    </li>
    <li>
      <a href="DataSources.html">Data Sources</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Analyses
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="ANOVA.html">ANOVA</a>
    </li>
    <li>
      <a href="LogisticRegression.html">Logistic Regression</a>
    </li>
    <li>
      <a href="LinearRegression.html">Linear Regression</a>
    </li>
    <li>
      <a href="Kruskal.html">Kruskal-Wallis Test</a>
    </li>
    <li>
      <a href="PermutationTests.html">Permutation Tests</a>
    </li>
    <li>
      <a href="WilcoxonTests.html">Wilcoxon Tests</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    My Work
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="ANOVA/MyTwoWayANOVA.html">My Two-way ANOVA</a>
    </li>
    <li>
      <a href="Logistic Regression/MyLogisticRegression.html">My Logistic Regression</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Linear Regression</h1>

</div>


<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
 
 function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
 }
</script>
<hr />
<p>Determine which explanatory variables have a significant effect on
the mean of the quantitative response variable.</p>
<hr />
<div id="simple-linear-regression"
class="section level2 tabset tabset-fade tabset-pills">
<h2 class="tabset tabset-fade tabset-pills">Simple Linear
Regression</h2>
<div style="float:left;width:125px;" align="center">
<p><img src="Images/QuantYQuantX.png" width=58px;></p>
</div>
<p>Simple linear regression is a good analysis technique when the data
consists of a single quantitative response variable <span
class="math inline">\(Y\)</span> and a single quantitative explanatory
variable <span class="math inline">\(X\)</span>.</p>
<div id="overview" class="section level3 tabset">
<h3 class="tabset">Overview</h3>
<div style="padding-left:125px;">
<p><strong>Mathematical Model</strong></p>
<p>The true regression model assumed by a regression analysis is given
by</p>
<div
style="float:right;font-size:.8em;background-color:lightgray;padding:5px;border-radius:4px;">
<a style="color:darkgray;" href="javascript:showhide('simplelinearlatexrcode')">Math
Code</a>
</div>
<div id="simplelinearlatexrcode" style="display:none;">
<pre><code>$$
  \underbrace{Y_i}_\text{Some Label} = \overbrace{\beta_0}^\text{y-int} + \overbrace{\beta_1}^\text{slope} \underbrace{X_i}_\text{Some Label} + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \sigma^2)
$$</code></pre>
</div>
<center>
<span class="tooltipr"> <span class="math inline">\(Y_i\)</span> <span
class="tooltiprtext">The response variable. The “i” denotes that this is
the y-value for individual “i”, where “i” is 1, 2, 3,… and so on up to
<span class="math inline">\(n\)</span>, the sample size.</span>
</span><span class="tooltipr"> <span class="math inline">\(=\)</span>
<span class="tooltiprtext">This states that we are assuming <span
class="math inline">\(Y_i\)</span> was created, or is “equal to” the
formula that will follow on the right-hand-side of the equation.</span>
</span><span class="tooltipr"> <span
class="math inline">\(\underbrace{\overbrace{\beta_0}^\text{y-intercept}
+ \overbrace{\beta_1}^\text{slope} X_i \ }_\text{true regression
relation}\)</span> <span class="tooltiprtext">The true regression
relation is a line, a line that is typically unknown in real life. It
can be likened to “God’s Law” or “Natural Law”. Something that governs
the way the data behaves, but is unkown to us.</span> </span><span
class="tooltipr"> <span class="math inline">\(+\)</span> <span
class="tooltiprtext">This plus sign emphasizes that the actual data, the
<span class="math inline">\(Y_i\)</span>, is created by adding together
the value from the true line <span class="math inline">\(\beta_0 +
\beta_1 X_i\)</span> and an individual error term <span
class="math inline">\(\epsilon_i\)</span>, which allows each dot in the
regression to be off of the line by a certain amount called <span
class="math inline">\(\epsilon_i\)</span>.</span> </span><span
class="tooltipr"> <span
class="math inline">\(\overbrace{\epsilon_i}^\text{error term}\)</span>
<span class="tooltiprtext">Error term for each individual <span
class="math inline">\(i\)</span>. The error terms are “random” and
unique for each individual. This provides the statistical relationship
of the regression. It is what allows each dot to be different, while
still coming from the same line, or underlying law.</span> </span><span
class="tooltipr"> <span class="math inline">\(\quad
\text{where}\)</span> <span class="tooltiprtext">Some extra comments are
needed about <span class="math inline">\(\epsilon_i\)</span>…</span>
</span><span class="tooltipr"> <span class="math inline">\(\
\overbrace{\epsilon_i \sim N(0, \sigma^2)}^\text{error term normally
distributed}\)</span> <span class="tooltiprtext">The error terms <span
class="math inline">\(\epsilon_i\)</span> are assumed to be normally
distributed with constant variance. Pay special note that the <span
class="math inline">\(\sigma\)</span> does not have an <span
class="math inline">\(i\)</span> in it, so it is the same for each
individual. In other words, the variance is constant. The mean of the
errors is zero, which causes the dots to be spread out symmetrically
both above and below the line.</span> </span>
</center>
<p><br/></p>
<p>The estimated regression line obtained from a regression analysis,
pronounced “y-hat”, is written as</p>
<div
style="float:right;font-size:.8em;background-color:lightgray;padding:5px;border-radius:4px;">
<a style="color:darkgray;" href="javascript:showhide('simplelinearlatexrcodeyhat')">Math
Code</a>
</div>
<div id="simplelinearlatexrcodeyhat" style="display:none;">
<pre><code>$$
  \underbrace{\hat{Y}_i}_\text{Some Label} = \overbrace{b_0}^\text{est. y-int} + \overbrace{b_1}^\text{est. slope} \underbrace{X_i}_\text{Some Label}
$$</code></pre>
</div>
<center>
<span class="tooltipr"> <span class="math inline">\(\hat{Y}_i\)</span>
<span class="tooltiprtext">The estimated average y-value for individual
<span class="math inline">\(i\)</span> is denoted by <span
class="math inline">\(\hat{Y}_i\)</span>. It is important to recognize
that <span class="math inline">\(Y_i\)</span> is the actual value for
individual <span class="math inline">\(i\)</span>, and <span
class="math inline">\(\hat{Y}_i\)</span> is the average y-value for all
individuals with the same <span class="math inline">\(X_i\)</span>
value.</span> </span><span class="tooltipr"> <span
class="math inline">\(=\)</span> <span class="tooltiprtext">The formula
for the average y-value, <span class="math inline">\(\hat{Y}_i\)</span>
is equal to what follows…</span> </span><span class="tooltipr"> <span
class="math inline">\(\underbrace{\overbrace{\ b_0 \
}^\text{y-intercept} + \overbrace{b_1}^\text{slope} X_i \
}_\text{estimated regression relation}\)</span> <span
class="tooltiprtext">Two things are important to notice about this
equation. First, it uses <span class="math inline">\(b_0\)</span> and
<span class="math inline">\(b_1\)</span> instead of <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. This is because <span
class="math inline">\(b_0\)</span> and <span
class="math inline">\(b_1\)</span> are the estimated y-intercept and
slope, respectively, not the true y-intercept <span
class="math inline">\(\beta_0\)</span> and true slope <span
class="math inline">\(\beta_1\)</span>. Second, this equation does not
include <span class="math inline">\(\epsilon_i\)</span>. In other words,
it is the estimated regression line, so it only describes the average
y-values, not the actual y-values.</span> </span>
</center>
<p><br/></p>
<div style="font-size:0.8em;">
<p>Note: see the <strong>Explanation</strong> tab <strong>The
Mathematical Model</strong> for details about these equations.</p>
</div>
<p><strong>Hypotheses</strong></p>
<div
style="float:right;font-size:.8em;background-color:lightgray;padding:5px;border-radius:4px;">
<a style="color:darkgray;" href="javascript:showhide('simplelinearhypecodeslope')">Math
Code</a>
</div>
<div id="simplelinearhypecodeslope" style="display:none;">
<pre><code>$$
\left.\begin{array}{ll}
H_0: \beta_1 = 0 \\  
H_a: \beta_1 \neq 0
\end{array}
\right\} \ \text{Slope Hypotheses}
$$

$$
\left.\begin{array}{ll}
H_0: \beta_0 = 0 \\  
H_a: \beta_0 \neq 0
\end{array}
\right\} \ \text{Intercept Hypotheses}
$$</code></pre>
</div>
<div style="clear:right;">

</div>
<p><span class="math display">\[
\left.\begin{array}{ll}
H_0: \beta_1 = 0 \\  
H_a: \beta_1 \neq 0
\end{array}
\right\} \ \text{Slope Hypotheses}^{\quad \text{(most
common)}}\quad\quad
\]</span></p>
<p><span class="math display">\[
\left.\begin{array}{ll}
H_0: \beta_0 = 0 \\  
H_a: \beta_0 \neq 0
\end{array}
\right\} \ \text{Intercept Hypotheses}^{\quad\text{(sometimes useful)}}
\]</span></p>
<p><br/></p>
<p>If <span class="math inline">\(\beta_1 = 0\)</span>, then the model
reduces to <span class="math inline">\(Y_i = \beta_0 +
\epsilon_i\)</span>, which is a flat line. This means <span
class="math inline">\(X\)</span> does not improve our understanding of
the mean of <span class="math inline">\(Y\)</span> if the null
hypothesis is true.</p>
<p>If <span class="math inline">\(\beta_0 = 0\)</span>, then the model
reduces to <span class="math inline">\(Y_i = \beta_1 X +
\epsilon_i\)</span>, a line going through the origin. This means the
average <span class="math inline">\(Y\)</span>-value is <span
class="math inline">\(0\)</span> when <span
class="math inline">\(X=0\)</span> if the null hypothesis is true.</p>
<p><strong>Assumptions</strong></p>
<p>This regression model is appropriate for the data when five
assumptions can be made.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Linear Relation</strong>: the true regression relation
between <span class="math inline">\(Y\)</span> and <span
class="math inline">\(X\)</span> is linear.</p></li>
<li><p><strong>Normal Errors</strong>: the error terms <span
class="math inline">\(\epsilon_i\)</span> are normally distributed with
a mean of zero.</p></li>
<li><p><strong>Constant Variance</strong>: the variance <span
class="math inline">\(\sigma^2\)</span> of the error terms is constant
(the same) over all <span class="math inline">\(X_i\)</span>
values.</p></li>
<li><p><strong>Fixed X</strong>: the <span
class="math inline">\(X_i\)</span> values can be considered fixed and
measured without error.</p></li>
<li><p><strong>Independent Errors</strong>: the error terms <span
class="math inline">\(\epsilon_i\)</span> are independent.</p></li>
</ol>
<div style="font-size:0.8em;">
<p>Note: see the <strong>Explanation</strong> tab <strong>Residual Plots
&amp; Regression Assumptions</strong> for details about checking the
regression assumptions.</p>
</div>
<p><strong>Interpretation</strong></p>
<p>The slope is interpreted as, “the change in the average y-value for a
one unit change in the x-value.” It <strong>is not</strong> the average
change in y. <strong>It is</strong> the change in the average
y-value.</p>
<p>The y-intercept is interpreted as, “the average y-value when x is
zero.” It is often not meaningful, but is sometimes useful. It just
depends if x being zero is meaningful or not within the context of your
analysis. For example, knowing the average price of a car with zero
miles is useful. However, pretending to know the average height of adult
males that weigh zero pounds, is not useful.</p>
<hr />
</div>
</div>
<div id="r-instructions" class="section level3">
<h3>R Instructions</h3>
<div style="padding-left:125px;">
<p><strong>Console</strong> Help Command: <code>?lm()</code></p>
<p><strong>Perform the Regression</strong></p>
<a href="javascript:showhide('simplelinearrcode')">
<div class="hoverchunk">
<p><span class="tooltipr"> mylm <span class="tooltiprtext">This is some
name you come up with that will become the R object that stores the
results of your linear regression <code>lm(...)</code> command.</span>
</span><span class="tooltipr">  &lt;-  <span class="tooltiprtext">This
is the “left arrow” assignment operator that stores the results of your
<code>lm()</code> code into <code>mylm</code> name.</span> </span><span
class="tooltipr"> lm( <span class="tooltiprtext">lm(…) is an R function
that stands for “Linear Model”. It performs a linear regression analysis
for Y ~ X.</span> </span><span class="tooltipr"> Y  <span
class="tooltiprtext">Y is your quantitative response variable. It is the
name of one of the columns in your data set.</span> </span><span
class="tooltipr"> ~  <span class="tooltiprtext">The tilde symbol ~ is
used to tell R that Y should be treated as the response variable that is
being explained by the explanatory variable X.</span> </span><span
class="tooltipr"> X, <span class="tooltiprtext">X is the quantitative
explanatory variable (at least it is typically quantitative but could be
qualitative) that will be used to explain the average Y-value.</span>
</span><span class="tooltipr">  data = NameOfYourDataset <span
class="tooltiprtext">NameOfYourDataset is the name of the dataset that
contains Y and X. In other words, one column of your dataset would be
your response variable Y and another column would be your explanatory
variable X.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis for the lm(…) function.</span>
</span><br/><span class="tooltipr"> summary(mylm) <span
class="tooltiprtext">The <code>summary</code> command allows you to
print the results of your linear regression that were previously saved
in <code>mylm</code> name.</span> </span><span class="tooltipr"
style="float:right;font-size:.8em;">  Click to Show Output  <span
class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="simplelinearrcode" style="display:none;">
<p>Example output from a regression. Hover each piece to learn more.</p>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Call:<br/> lm(formula = dist ~ speed, data =
cars) <span class="tooltiprouttext">This is simply a statement of your
original lm(…) “call” that you made when performing your regression. It
allows you to verify that you ran what you thought you ran in the
lm(…).</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td colspan="2">
<span class="tooltiprout"> Residuals: <span
class="tooltiprouttext">Residuals are the vertical difference between
each point and the line, <span class="math inline">\(Y_i -
\hat{Y}_i\)</span>. The residuals are supposed to be normally
distributed, so a quick glance at their five-number summary can give us
insight about any skew present in the residuals. </span>
</td>
</tr>
<tr>
<td align="right">
<span class="tooltiprout"> min<br/>   -29.069 <span
class="tooltiprouttext">“min” gives the value of the residual that is
furthest below the regression line. Ideally, the magnitude of this value
would be about equal to the magnitude of the largest positive residual
(the max) because the hope is that the residuals are normally
distributed around the line.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 1Q<br/>   -9.525 <span
class="tooltiprouttext">“1Q” gives the first quartile of the residuals,
which will always be negative, and ideally would be about equal in
magnitude to the third quartile.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> Median<br/>   -2.272 <span
class="tooltiprouttext">“Median” gives the median of the residuals,
which would ideally would be about equal to zero. Note that because the
regression line is the least squares line, the mean of the residuals
will ALWAYS be zero, so it is never included in the output summary. This
particular median value of -2.272 is a little smaller than zero than we
would hope for and suggests a right skew in the data because the mean
(0) is greater than the median (-2.272) witnessing the residuals are
right skewed. This can also be seen in the maximum being much larger in
magnitude than the minimum.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 3Q<br/>   9.215 <span
class="tooltiprouttext">“3Q” gives the third quartile of the residuals,
which would ideally would be about equal in magnitude to the first
quartile. In this case, it is pretty close, which helps us see that the
first quartile of residuals on either side of the line is behaving
fairly normally.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> Max</br>   43.201 <span
class="tooltiprouttext">“Max” gives the maximum positive residuals,
which would ideally would be about equal in magnitude to the minimum
residual. In this case, it is much larger than the minimum, which helps
us see that the residuals are likely right skewed.</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td colspan="2">
<span class="tooltiprout"> Coefficients: <span
class="tooltiprouttext">Notice that in your lm(…) you used only <span
class="math inline">\(Y\)</span> and <span
class="math inline">\(X\)</span>. You did type out any coefficients,
i.e., the <span class="math inline">\(\beta_0\)</span> or <span
class="math inline">\(\beta_1\)</span> of the regression model. These
coefficients are estimated by the lm(…) function and displayed in this
part of the output along with standard errors, t-values, and
p-values.</span> </span>
</td>
</tr>
<tr>
<td align="left">
</td>
<td align="right">
<span class="tooltiprout">   Estimate <span class="tooltiprouttext">To
learn more about the “Estimates” of the “Coefficients” see the
“Explanation” tab, “Estimating the Model Parameters” section for
details.</span>
</td>
<td align="right">
<span class="tooltiprout">   Std. Error <span class="tooltiprouttext">To
learn more about the “Standard Errors” of the “Coefficients” see the
“Explanation” tab, “Inference for the Model Parameters” section.</span>
</span>
</td>
<td align="right">
<span class="tooltiprout">   t value <span class="tooltiprouttext">To
learn more about the “t value” of the “Coefficients” see the
“Explanation” tab, “Inference for the Model Parameters” section.</span>
</span>
</td>
<td align="right">
<span class="tooltiprout">   Pr(&gt;|t|) <span
class="tooltiprouttext">The “Pr” stands for “Probability” and the “(&gt;
|t|)” stands for “more extreme than the observed t-value”. Thus, this is
the p-value for the hypothesis test of each coefficient being zero.<br/>
To learn more about the “p-value” of the “Coefficients” see the
“Explanation” tab, “Inference for the Model Parameters” section. </span>
</span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> (Intercept) <span
class="tooltiprouttext">This always says “Intercept” for any lm(…) you
run in R. That is because R always assumes there is a y-intercept for
your regression function.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   -17.5791 <span class="tooltiprouttext">This
is the estimate of the y-intercept, <span
class="math inline">\(\beta_0\)</span>. It is called <span
class="math inline">\(b_0\)</span>. It is the average y-value when X is
zero.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   6.7584 <span class="tooltiprouttext">This
is the standard error of <span class="math inline">\(b_0\)</span>. It
tells you how much <span class="math inline">\(b_0\)</span> varies from
sample to sample. The closer to zero, the better.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> -2.601 <span class="tooltiprouttext">This is
the test statistic t for the test of <span class="math inline">\(\beta_0
= 0\)</span>. It is calculated by dividing the “Estimate” of the
intercept (-17.5791) by its standard error (6.7584). It gives the
“number of standard errors” away from zero that the “estimate” has
landed. In this case, the estimate of -17.5791 is -2.601 standard errors
(6.7584) from zero, which is a fairly surprising distance as shown by
the p-value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 0.0123 <span class="tooltiprouttext">This is
the p-value of the test of the hypothesis that <span
class="math inline">\(\beta_0 = 0\)</span>. It measures the probability
of observing a t-value as extreme as the one observed. To compute it
yourself in R, use
<code>pt(-abs(your t-value), df of your regression)*2</code>.</span>
</span>
</td>
<td align="left">
<span class="tooltiprout"> * <span class="tooltiprouttext">This is
called a “star”. One star means significant at the 0.1 level of <span
class="math inline">\(\alpha\)</span>.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> speed <span class="tooltiprouttext">This is
always the name of your X-variable in your lm(Y ~ X, …).</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   3.9324 <span class="tooltiprouttext">This
is the estimate of the slope, <span
class="math inline">\(\beta_1\)</span>. It is called <span
class="math inline">\(b_1\)</span>. It is the change in the average
y-value as X is increased by 1 unit.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   0.4155 <span class="tooltiprouttext">This
is the standard error of <span class="math inline">\(b_1\)</span>. It
tells you how much <span class="math inline">\(b_1\)</span> varies from
sample to sample. The closer to zero, the better.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 9.464 <span class="tooltiprouttext">This is
the test statistic t for the test of <span class="math inline">\(\beta_1
= 0\)</span>. It is calculated by dividing the “Estimate” of the slope
(3.9324) by its standard error (0.4155). It gives the “number of
standard errors” away from zero that the “estimate” has landed. In this
case, the estimate of 3.9324 is 9.464 standard errors (0.4155) from
zero, which is a really surprising distance as shown by the smallness of
the p-value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 1.49e-12 <span class="tooltiprouttext">This
is the p-value of the test of the hypothesis that <span
class="math inline">\(\beta_1 = 0\)</span>. To compute it yourself in R,
use <code>pt(-abs(your t-value), df of your regression)*2</code></span>
</span>
</td>
<td align="left">
<span class="tooltiprout"> *** <span class="tooltiprouttext">This is
called a “star”. Three stars means significant at the 0.01 level of
<span class="math inline">\(\alpha\)</span>.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span> --- </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’
0.05 ‘.’ 0.1 ‘ ’ 1 <span class="tooltiprouttext">These “codes” explain
what significance level the p-value is smaller than based on how many
“stars” * the p-value is labeled with in the Coefficients table
above.</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Residual standard error: <span
class="tooltiprouttext">This is the estimate of <span
class="math inline">\(\sigma\)</span> in the regression model <span
class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\)</span>
where <span class="math inline">\(\epsilon_i \sim
N(0,\sigma^2)\)</span>. It is the square root of the MSE.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  15.38 <span class="tooltiprouttext">For this
particular regression, the estimate of <span
class="math inline">\(\sigma\)</span> is 15.38. Squaring this number
gives you the MSE, which is the estimate of <span
class="math inline">\(\sigma^2\)</span>.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  on 48 degrees of freedom <span
class="tooltiprouttext">This is <span class="math inline">\(n-p\)</span>
where <span class="math inline">\(n\)</span> is the sample size and
<span class="math inline">\(p\)</span> is the number of parameters in
the regression model. In this case, there is a sample size of 50 and two
parameters, <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>, so 50-2 = 48.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Multiple R-squared: <span
class="tooltiprouttext">This is <span
class="math inline">\(R^2\)</span>, the percentage of variation in <span
class="math inline">\(Y\)</span> that is explained by the regression
model. It is equal to the SSR/SSTO or, equivalently, 1 -
SSE/SSTO.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  0.6511, <span class="tooltiprouttext">In
this particular regression, 65.11% of the variation in stopping distance
<code>dist</code> is explained by the regression model using speed of
the car.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  Adjusted R-squared: <span
class="tooltiprouttext">The adjusted R-squared will always be at least
slightly smaller than <span class="math inline">\(R^2\)</span>. The
closer to R-squared that it is, the better. When it differs dramatically
from <span class="math inline">\(R^2\)</span>, it is a sign that the
regression model is over-fitting the data.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  0.6438 <span class="tooltiprouttext">In this
case, the value of 0.6438 is quite close to the original <span
class="math inline">\(R^2\)</span> value, so there is no fear of
over-fitting with this particular model. That is good.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> F-statistic: <span
class="tooltiprouttext">The F-statistic is found as the ratio of the
MSR/MSE where MSR = SSR/(p-1) and MSE = SSE/(n-p) where n is the sample
size and p is the number of parameters in the regression model.</span>
</span>
</td>
<td align="right">
<span class="tooltiprout">  89.57 <span class="tooltiprouttext">This is
the value of the F-statistic for the lm(dist ~ speed, data=cars)
regression. Note that SSE = sum( cars.lm$res^2 ) = 11353.52 with n - p =
50 - 2 = 48 degrees of freedom for this data. Further, SSR = sum(
(cars.lm$fit - mean(cars$dist))^2 ) = 21185.46 with p - 1 = 1 degree of
freedom. So MSR = 21185.46 and MSE = 11353.52 / 48 = 236.5317. So MSR /
MSE = 21185.46 / 236.5317 = 89.56711.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  on 1 and 48 DF, <span
class="tooltiprouttext">The 1 degree of freedom is the SSR degrees of
freedom (p-1). The 48 is the SSE degrees of freedom (n-p).</span>
</span>
</td>
<td align="right">
<span class="tooltiprout">  p-value: 1.49e-12 <span
class="tooltiprouttext">The p-value for an F-statistic is found by the
code pf(89.56711, 1, 48, lower.tail=FALSE), which gives the probability
of being more extreme than the observed F-statistic in an F distribution
with 1 and 48 degrees of freedom.</span> </span>
</td>
</tr>
</table>
</div>
<p><br/></p>
<p><strong>Check Assumptions 1, 2, 3, and 5</strong></p>
<a href="javascript:showhide('assumptionplots')">
<div class="hoverchunk">
<p><span class="tooltipr"> par( <span class="tooltiprtext">The par(…)
command stands for “Graphical PARameters”. It allows you to control
various aspects of graphics in Base R.</span> </span><span
class="tooltipr"> mfrow= <span class="tooltiprtext">This stands for
“multiple frames filled by row”, which means, put lots of plots on the
same row, starting with the plot on the left, then working towards the
right as more plots are created.</span> </span><span class="tooltipr">
c( <span class="tooltiprtext">The combine function c(…) is used to
specify how many rows and columns of graphics should be placed
together.</span> </span><span class="tooltipr"> 1, <span
class="tooltiprtext">This specifies that 1 row of graphics should be
produced.</span> </span><span class="tooltipr"> 3 <span
class="tooltiprtext">This states that 3 columns of graphics should be
produced.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis for c(…) function.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis for par(…) function.</span> </span><br/><span
class="tooltipr"> plot( <span class="tooltiprtext">This version of
plot(…) will actually create several regression diagnostic plots by
default.</span> </span><span class="tooltipr"> mylm, <span
class="tooltiprtext">This is the name of an lm object that you created
previously.</span> </span><span class="tooltipr"> which= <span
class="tooltiprtext">This allows you to select “which” regression
diagnostic plots should be drawn.</span> </span><span class="tooltipr">
1 <span class="tooltiprtext">Selecting 1, would give the residuals
vs. fitted values plot only.</span> </span><span class="tooltipr"> :
<span class="tooltiprtext">The colon allows you to select more than just
one plot.</span> </span><span class="tooltipr"> 2 <span
class="tooltiprtext">Selecting 2 also gives the Q-Q Plot of residuals.
If you wanted to instead you could just use which=1 to get the residuals
vs fitted values plot, then you could use qqPlot(mylm$residuals) to
create a fancier Q-Q Plot of the residuals.</span> </span><span
class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for
plot(…) function.</span> </span><br/><span class="tooltipr"> plot( <span
class="tooltiprtext">This version of plot(…) will be used to create a
time-ordered plot of the residuals. The order of the residuals is the
original order of the x-values in the original data set. If the original
data set doesn’t have an order, then this plot is not
interesting.</span> </span><span class="tooltipr"> mylm <span
class="tooltiprtext">The lm object that you created previously.</span>
</span><span class="tooltipr"> $ <span class="tooltiprtext">This allows
you to access various elements from the regression that was
performed.</span> </span><span class="tooltipr"> residuals <span
class="tooltiprtext">This grabs the residuals for each observation in
the regression.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis for plot(…) function.</span>
</span><span class="tooltipr" style="float:right;font-size:.8em;">
 Click to Show Output  <span class="tooltiprtext">Click to View
Output.</span> </span></p>
</div>
<p></a></p>
<div id="assumptionplots" style="display:none;">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<p><br/></p>
<p><strong>Plotting the Regression Line</strong></p>
<div class="tab">
<p><button class="tablinks" onclick="openTab(event, 'BaseScatterplot')">Base
R</button>
<button class="tablinks" onclick="openTab(event, 'ggplotScatterplot')">ggplot2</button></p>
</div>
<div id="BaseScatterplot" class="tabcontent">
<p>
<p>To add the regression line to a scatterplot use the
<code>abline(...)</code> command:</p>
<a href="javascript:showhide('regressionline')">
<div class="hoverchunk">
<p><span class="tooltipr"> plot( <span class="tooltiprtext">The plot(…)
function is used to create a scatterplot with a y-axis (the vertical
axis) and an x-axis (the horizontal axis).</span> </span><span
class="tooltipr"> Y  <span class="tooltiprtext">This is the “response
variable” of your regression. The thing you are interested in
predicting. This is the name of a “numeric” column of data from the data
set called YourDataSet.</span> </span><span class="tooltipr"> ~  <span
class="tooltiprtext">The tilde “~” is used to relate Y to X and can be
found on the top-left key of your keyboard.</span> </span><span
class="tooltipr"> X,  <span class="tooltiprtext">This is the explanatory
variable of your regression. It is the name of a “numeric” column of
data from YourDataSet. .</span> </span><span class="tooltipr"> data=
<span class="tooltiprtext">The data= statement is used to specify the
name of the data set where the columns of “X” and “Y” are
located.</span> </span><span class="tooltipr"> YourDataSet <span
class="tooltiprtext">This is the name of your data set, like KidsFeet or
cars or airquality.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis for plot(…) function.</span>
</span><br/><span class="tooltipr"> abline( <span
class="tooltiprtext">This stands for “a” (intercept) “b” (slope) line.
It is a function that allows you to add a line to a plot by specifying
just the intercept and slope of the line.</span> </span><span
class="tooltipr"> mylm <span class="tooltiprtext">This is the name of an
lm(…) that you created previoiusly. Since mylm contains the slope and
intercept of the estimated line, the abline(…) function will locate
these two values from within mylm and use them to add a line to your
current plot(…).</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis for abline(…) function.</span>
</span><span class="tooltipr" style="float:right;font-size:.8em;">
 Click to Show Output  <span class="tooltiprtext">Click to View
Output.</span> </span></p>
</div>
<p></a></p>
<div id="regressionline" style="display:none;">
<pre><code>mylm &lt;- lm(dist ~ speed, data = cars)
plot(dist ~ speed, data = cars)
abline(mylm)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<p>You can customize the look of the regression line with</p>
<a href="javascript:showhide('regressionlinecolor')">
<div class="hoverchunk">
<p><span class="tooltipr"> abline( <span class="tooltiprtext">This
stands for “a” (intercept) “b” (slope) line. It is a function that
allows you to add a line to a plot by specifying just the intercept and
slope of the line.</span> </span><span class="tooltipr"> mylm, <span
class="tooltiprtext">This is the name of an lm(…) that you created
previoiusly. Since mylm contains the slope and intercept of the
estimated line, the abline(…) function will locate these two values from
within mylm and use them to add a line to your current plot(…).</span>
</span><span class="tooltipr"> lty= <span class="tooltiprtext">The lty=
stands for “line type” and allows you to select between 0=blank, 1=solid
(default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash.</span>
</span><span class="tooltipr"> 1, <span class="tooltiprtext">This
creates a solid line. Remember, other options include: 0=blank, 1=solid
(default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash.</span>
</span><span class="tooltipr"> lwd= <span class="tooltiprtext">The lwd=
allows you to specify the width of the line. The default width is 1.
Using lwd=2 would double the thickness, and so on. Any positive value is
allowed.</span> </span><span class="tooltipr"> 1, <span
class="tooltiprtext">Default line width. To make a thicker line, us 2 or
3… To make a thinner line, try 0.5, but 1 is already pretty thin.</span>
</span><span class="tooltipr"> col= <span class="tooltiprtext">This
allows you to specify the color of the line using either a name of a
color or rgb(.5,.2,.3,.2) where the format is rgb(percentage red,
percentage green, percentage blue, percent opaque).</span> </span><span
class="tooltipr"> “someColor” <span class="tooltiprtext">Type colors()
in R for options.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis for abline(…) function.</span>
</span><span class="tooltipr" style="float:right;font-size:.8em;">
 Click to Show Output  <span class="tooltiprtext">Click to View
Output.</span> </span></p>
</div>
<p></a></p>
<div id="regressionlinecolor" style="display:none;">
<pre><code>mylm &lt;- lm(dist ~ speed, data = cars)
plot(dist ~ speed, data = cars)
abline(mylm, lty=1, lwd=1, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<p>You can add points to the regression with…</p>
<a href="javascript:showhide('regressionaddpoints')">
<div class="hoverchunk">
<p><span class="tooltipr"> points( <span class="tooltiprtext">This is
like plot(…) but adds points to the current plot(…) instead of creating
a new plot.</span> </span><span class="tooltipr"> newY  <span
class="tooltiprtext">newY should be a column of values from some data
set. Or, use points(newX, newY) to add a single point to a graph.</span>
</span><span class="tooltipr"> ~  <span class="tooltiprtext">This links
Y to X in the plot.</span> </span><span class="tooltipr"> newX,  <span
class="tooltiprtext">newX should be a column of values from some data
set. It should be the same length as newY. If just a single value, use
points(newX, newY) instead.</span> </span><span class="tooltipr">
data=YourDataSet,  <span class="tooltiprtext">If newY and newX come from
a dataset, then use data= to tell the points(…) function what data set
they come from. If newY and newX are just single values, then data= is
not needed.</span> </span><span class="tooltipr"> col=“skyblue”, <span
class="tooltiprtext">This allows you to specify the color of the points
using either a name of a color or rgb(.5,.2,.3,.2) where the format is
rgb(percentage red, percentage green, percentage blue, percent
opaque).</span> </span><span class="tooltipr"> pch=16 <span
class="tooltiprtext">This allows you to specify the type of plotting
symbol to be used for the points. Type ?pch and scroll half way down in
the help file that appears to learn about other possible symbols.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis for points(…) function.</span> </span><span class="tooltipr"
style="float:right;font-size:.8em;">  Click to Show Output  <span
class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="regressionaddpoints" style="display:none;">
<pre><code>mylm &lt;- lm(dist ~ speed, data = cars)
plot(dist ~ speed, data = cars)
points(7,40, pch=16, col=&quot;skyblue&quot;, cex=2)
text(7,40, &quot;New Dot&quot;, pos=3, cex=0.5)
points(dist ~ speed, data=filter(cars, mylm$res &gt; 2), cex=.8, col=&quot;red&quot;)
abline(mylm, lty=1, lwd=1, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
</p>
</div>
<div id="ggplotScatterplot" class="tabcontent">
<p>
<p>To add the regression line to a scatterplot using the ggplot2
approach, first ensure:</p>
<p><code>library(ggplot2)</code> or <code>library(tidyverse)</code></p>
<p>is loaded. Then, use the <code>geom_smooth(method = lm)</code>
command:</p>
<a href="javascript:showhide('ggplot')">
<div class="hoverchunk">
<p><span class="tooltipr"> ggplot( <span class="tooltiprtext">Every
ggplot2 graphic begins with the ggplot() command, which creates a
framework, or coordinate system, that you can add layers to. Without
adding any layers, ggplot() produces a blank graphic.</span>
</span><span class="tooltipr"> YourDataSet,  <span
class="tooltiprtext">This is simply the name of your data set, like
KidsFeet or starwars.</span> </span><span class="tooltipr"> aes( <span
class="tooltiprtext">aes stands for aesthetic. Inside of aes(), you
place elements that you want to map to the coordinate system, like x and
y variables.</span> </span><span class="tooltipr"> x =  <span
class="tooltiprtext">“x = ” declares which variable will become the
x-axis of the graphic, your explanatory variable. Both “x= ” and “y= ”
are optional phrasesin the ggplot2 syntax.</span> </span><span
class="tooltipr"> X, <span class="tooltiprtext">This is the explanatory
variable of the regression: the variable used to <em>explain</em> the
mean of y. It is the name of the “numeric” column of YourDataSet.</span>
</span><span class="tooltipr">  y =  <span class="tooltiprtext">“y= ”
declares which variable will become the y-axis of the graphic.</span>
</span><span class="tooltipr"> Y <span class="tooltiprtext">This is the
response variable of the regression: the variable that you are
interested in predicting. It is the name of a “numeric” column of
YourDataSet.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis for aes(…) function.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis for ggplot(…) function.</span> </span><span
class="tooltipr"> + <span class="tooltiprtext">The + allows you to add
more layers to the framework provided by ggplot(). In this case, you use
+ to add a geom_point() layer on the next line.</span> </span><br/><span
class="tooltipr">   geom_point() <span class="tooltiprtext">geom_point()
allows you to add a layer of points, a scatterplot, over the ggplot()
framework. The x and y coordinates are received from the previously
specified x and y variables declared in the ggplot() aesthetic.</span>
</span><span class="tooltipr"> + <span class="tooltiprtext">Here the +
is used to add yet another layer to ggplot().</span> </span><br/><span
class="tooltipr">   geom_smooth( <span
class="tooltiprtext">geom_smooth() is a smoothing function that you can
use to add different lines or curves to ggplot(). In this case, you will
use it to add the least-squares regression line to the
scatterplot.</span> </span><span class="tooltipr"> method =  <span
class="tooltiprtext">Use “method = ” to tell geom_smooth() that you are
going to declare a specific smoothing function, or method, to alter the
line or curve..</span> </span><span class="tooltipr"> “lm”, <span
class="tooltiprtext">lm stands for linear model. Using method = “lm”
tells geom_smooth() to fit a least-squares regression line onto the
graphic. The regression line is modeled using y ~ x, which variables
were declared in the initial ggplot() aesthetic. There are several other
methods that could be used here.</span> </span><span class="tooltipr">
 formula = y~x, <span class="tooltiprtext">This tells geom_smooth to
place a simple linear regression line on the plot. Other formula
statements can be used in the same way as lm(…) to place more
complicated models on the plot.</span> </span><span class="tooltipr">
 se = FALSE <span class="tooltiprtext">se stands for “standard error”.
Specifying FALSE turns this feature off. When TRUE, a gray band showing
the “confidence band” for the regression is shown. Unless you know how
to interpret this confidence band, leave it turned off.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis for the geom_smooth() function.</span> </span><span
class="tooltipr" style="float:right;font-size:.8em;">  Click to Show
Output  <span class="tooltiprtext">Click to View Output.</span>
</span></p>
</div>
<p></a></p>
<div id="ggplot" style="display:none;">
<pre><code>ggplot(cars, aes(x = speed, y = dist)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula=y~x, se=FALSE)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<p>There are a number of ways to customize the appearance of the
regression line:</p>
<a href="javascript:showhide('ggplotline')">
<div class="hoverchunk">
<p><span class="tooltipr"> ggplot( <span class="tooltiprtext">Every
ggplot2 graphic begins with the ggplot() command, which creates a
framework, or coordinate system, that you can add layers to. Without
adding any layers, ggplot() produces a blank graphic.</span>
</span><span class="tooltipr"> cars,  <span class="tooltiprtext">This is
simply the name of your data set, like KidsFeet or starwars.</span>
</span><span class="tooltipr"> aes( <span class="tooltiprtext">aes
stands for aesthetic. Inside of aes(), you place elements that you want
to map to the coordinate system, like x and y variables.</span>
</span><span class="tooltipr"> x =  <span class="tooltiprtext">“x = ”
declares which variable will become the x-axis of the graphic, your
explanatory variable. Both “x= ” and “y= ” are optional phrasesin the
ggplot2 syntax.</span> </span><span class="tooltipr"> speed,  <span
class="tooltiprtext">This is the explanatory variable of the regression:
the variable used to <em>explain</em> the mean of y. It is the name of
the “numeric” column of YourDataSet.</span> </span><span
class="tooltipr"> y =  <span class="tooltiprtext">“y= ” declares which
variable will become the y-axis of the grpahic.</span> </span><span
class="tooltipr"> dist <span class="tooltiprtext">This is the response
variable of the regression: the variable that you are interested in
predicting. It is the name of a “numeric” column of YourDataSet.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis for aes(…) function.</span> </span><span class="tooltipr"> )
<span class="tooltiprtext">Closing parenthesis for ggplot(…)
function.</span> </span><span class="tooltipr">  + <span
class="tooltiprtext">The + allows you to add more layers to the
framework provided by ggplot(). In this case, you use + to add a
geom_point() layer on the next line.</span> </span><br/><span
class="tooltipr">   geom_point() <span class="tooltiprtext">geom_point()
allows you to add a layer of points, a scatterplot, over the ggplot()
framework. The x and y coordinates are received from the previously
specified x and y variables declared in the ggplot() aesthetic.</span>
</span><span class="tooltipr">  + <span class="tooltiprtext">Here the +
is used to add yet another layer to ggplot().</span> </span><br/><span
class="tooltipr">   geom_smooth( <span
class="tooltiprtext">geom_smooth() is a smoothing function that you can
use to add different lines or curves to ggplot(). In this case, you will
use it to add the least-squares regression line to the
scatterplot.</span> </span><span class="tooltipr"> method =  <span
class="tooltiprtext">Use “method = ” to tell geom_smooth() that you are
going to declare a specific smoothing function, or method, to alter the
line or curve..</span> </span><span class="tooltipr"> “lm”, <span
class="tooltiprtext">lm stands for linear model. Using method = “lm”
tells geom_smooth() to fit a least-squares regression line onto the
graphic. The regression line is modeled using y ~ x, which variables
were declared in the initial ggplot() aesthetic.</span> </span><span
class="tooltipr">  formula = y~x, <span class="tooltiprtext">This tells
geom_smooth to place a simple linear regression line on the plot. Other
formula statements can be used in the same way as lm(…) to place more
complicated models on the plot.</span> </span><span class="tooltipr">
 se = FALSE, <span class="tooltiprtext">se stands for “standard error”.
Specifying FALSE turns this feature off. When TRUE, a gray band showing
the “confidence band” for the regression is shown. Unless you know how
to interpret this confidence band, leave it turned off.</span>
</span><span class="tooltipr">  size = 2, <span class="tooltiprtext">Use
<em>size = 2</em> to adjust the thickness of the line to size 2.</span>
</span><span class="tooltipr">  color = “orange”, <span
class="tooltiprtext">Use <em>color = “orange”</em> to change the color
of the line to orange.</span> </span><br><span class="tooltipr">
  linetype = “dashed” <span class="tooltiprtext">Use <em>linetype =
“dashed”</em> to change the solid line to a dashed line. Some linetype
options include “dashed”, “dotted”, “longdash”, “dotdash”, etc.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis for the geom_smooth() function.</span> </span><span
class="tooltipr" style="float:right;font-size:.8em;">  Click to Show
Output  <span class="tooltiprtext">Click to View Output.</span>
</span></p>
</div>
<p></a></p>
<div id="ggplotline" style="display:none;">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<p>In addition to customizing the regression line, you can customize the
points, add points, add lines, and much more.</p>
<a href="javascript:showhide('ggplotpoints')">
<div class="hoverchunk">
<p><span class="tooltipr"> ggplot( <span class="tooltiprtext">Every
ggplot2 graphic begins with the ggplot() command, which creates a
framework, or coordinate system, that you can add layers to. Without
adding any layers, ggplot() produces a blank graphic.</span>
</span><span class="tooltipr"> cars,  <span class="tooltiprtext">This is
simply the name of your data set, like KidsFeet or starwars.</span>
</span><span class="tooltipr"> aes( <span class="tooltiprtext">aes
stands for aesthetic. Inside of aes(), you place elements that you want
to map to the coordinate system, like x and y variables.</span>
</span><span class="tooltipr"> x =  <span class="tooltiprtext">“x = ”
declares which variable will become the x-axis of the graphic, your
explanatory variable. Both “x= ” and “y= ” are optional phrasesin the
ggplot2 syntax.</span> </span><span class="tooltipr"> speed,  <span
class="tooltiprtext">This is the explanatory variable of the regression:
the variable used to <em>explain</em> the mean of y. It is the name of
the “numeric” column of YourDataSet.</span> </span><span
class="tooltipr"> y =  <span class="tooltiprtext">“y= ” declares which
variable will become the y-axis of the grpahic.</span> </span><span
class="tooltipr"> dist <span class="tooltiprtext">This is the response
variable of the regression: the variable that you are interested in
predicting. It is the name of a “numeric” column of YourDataSet.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis for aes(…) function.</span> </span><span class="tooltipr"> )
<span class="tooltiprtext">Closing parenthesis for ggplot(…)
function.</span> </span><span class="tooltipr">  + <span
class="tooltiprtext">The + allows you to add more layers to the
framework provided by ggplot(). In this case, you use + to add a
geom_point() layer on the next line.</span> </span><br/><span
class="tooltipr">   geom_point( <span class="tooltiprtext">geom_point()
allows you to add a layer of points, a scatterplot, over the ggplot()
framework. The x and y coordinates are received from the previously
specified x and y variables declared in the ggplot() aesthetic.</span>
</span><span class="tooltipr"> size = 1.5, <span
class="tooltiprtext">Use <em>size = 1.5</em> to change the size of the
points.</span> </span><span class="tooltipr">  color = “skyblue” <span
class="tooltiprtext">Use <em>color = “skyblue”</em> to change the color
of the points to Brother Saunders’ favorite color.</span> </span><span
class="tooltipr">  alpha = 0.5 <span class="tooltiprtext">Use <em>alpha
= 0.5</em> to change the transparency of the points to 0.5.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis of geom_point() function. </span> </span><span
class="tooltipr">  + <span class="tooltiprtext">The + allows you to add
more layers to the framework provided by ggplot().</span>
</span><br><span class="tooltipr">   geom_smooth( <span
class="tooltiprtext">geom_smooth() is a smoothing function that you can
use to add different lines or curves to ggplot(). In this case, you will
use it to add the least-squares regression line to the
scatterplot.</span> </span><span class="tooltipr"> method =  <span
class="tooltiprtext">Use “method = ” to tell geom_smooth() that you are
going to declare a specific smoothing function, or method, to alter the
line or curve..</span> </span><span class="tooltipr"> “lm”, <span
class="tooltiprtext">lm stands for linear model. Using method = “lm”
tells geom_smooth() to fit a least-squares regression line onto the
graphic.</span> </span><span class="tooltipr">  formula = y~x, <span
class="tooltiprtext">This tells geom_smooth to place a simple linear
regression line on the plot. Other formula statements can be used in
ways similar to lm(…) to place more complicated models on the
plot.</span> </span><span class="tooltipr">  se = FALSE, <span
class="tooltiprtext">se stands for “standard error”. Specifying FALSE
turns this feature off. When TRUE, a gray band showing the “confidence
band” for the regression is shown. Unless you know how to interpret this
confidence band, leave it turned off.</span> </span><span
class="tooltipr">  color = “navy”, <span class="tooltiprtext">Use
<em>color = “navy”</em> to change the color of the line to navy
blue.</span> </span><span class="tooltipr">  size = 1.5 <span
class="tooltiprtext">Use <em>size = 1.5</em> to adjust the thickness of
the line to 1.5.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis of geom_smooth()
function.</span> </span><span class="tooltipr">  + <span
class="tooltiprtext">The + allows you to add more layers to the
framework provided by ggplot().</span> </span><br><span
class="tooltipr">   geom_hline( <span class="tooltiprtext">Use
geom_hline() to add a horizontal line at a specified y-intercept. You
can also use geom_vline(xintercept = some_number) to add a vertical line
to the graph.</span> </span><span class="tooltipr"> yintercept = <span
class="tooltiprtext">Use “yintercept =” to tell geom_hline() that you
are going to declare a y intercept for the horizontal line.</span>
</span><span class="tooltipr">  75 <span class="tooltiprtext">75 is the
value of the y-intercept.</span> </span><span class="tooltipr"> , color
= “firebrick” <span class="tooltiprtext">Use <em>color =
“firebrick”</em> to change the color of the horizontal line to firebrick
red.</span> </span><span class="tooltipr"> , size = 1, <span
class="tooltiprtext">Use <em>size = 1</em> to adjust the thickness of
the horizontal line to size 1.</span> </span><br><span class="tooltipr">
             linetype = “longdash” <span class="tooltiprtext">Use
<em>linetype = “longdash”</em> to change the solid line to a dashed line
with longer dashes. Some linetype options include “dashed”, “dotted”,
“longdash”, “dotdash”, etc.</span> </span><span class="tooltipr"> ,
alpha = 0.5 <span class="tooltiprtext">Use <em>alpha = 0.5</em> to
change the transparency of the horizontal line to 0.5.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis of geom_hline function.</span> </span><span
class="tooltipr">  + <span class="tooltiprtext">The + allows you to add
more layers to the framework provided by ggplot().</span>
</span><br><span class="tooltipr">   geom_segment( <span
class="tooltiprtext">geom_segment() allows you to add a line segment to
ggplot() by using specified start and end points.</span> </span><span
class="tooltipr"> x = <span class="tooltiprtext">“x =” tells
geom_segment() that you are going to declare the x-coordinate for the
starting point of the line segment.</span> </span><span
class="tooltipr">  14, <span class="tooltiprtext">14 is a number on the
x-axis of your graph. It is the x-coordinate of the starting point of
the line segment.</span> </span><span class="tooltipr">  y =<br />
<span class="tooltiprtext">“y =” tells geom_segment() that you are going
to declare the y-coordinate for the starting point of the line
segment.</span> </span><span class="tooltipr">  75, <span
class="tooltiprtext">75 is a number on the y-axis of your graph. It is
the y-coordinate of the starting point of the line segment.</span>
</span><span class="tooltipr">  xend = <span class="tooltiprtext">“xend
=” tells geom_segment() that you are going to declare the x-coordinate
for the end point of the line segment.</span> </span><span
class="tooltipr">  14, <span class="tooltiprtext">14 is a number on the
x-axis of your graph. It is the x-coordinate of the end point of the
line segment.</span> </span><span class="tooltipr">  yend = <span
class="tooltiprtext">“yend =” tells geom_segment() that you are going to
declare the y-coordinate for the end point of the line segment.</span>
</span><span class="tooltipr">  38, <span class="tooltiprtext">38 is a
number on the y-axis of your graph. It is the y-coordinate of the end
point of the line segment.</span> </span><br><span class="tooltipr">
               size = 1 <span class="tooltiprtext">Use <em>size = 1</em>
to adjust the thickness of the line segment.</span> </span><span
class="tooltipr"> , color = “lightgray” <span class="tooltiprtext">Use
<em>color = “lightgray”</em> to change the color of the line segment to
light gray.</span> </span><span class="tooltipr"> , linetype =
“longdash” <span class="tooltiprtext">Use *linetype = “longdash* to
change the solid line segment to a dashed one. Some linetype options
include”dashed”, “dotted”, “longdash”, “dotdash”, etc.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis for geom_segment() function.</span> </span><span
class="tooltipr">  + <span class="tooltiprtext">The + allows you to add
more layers to the framework provided by ggplot().</span>
</span><br><span class="tooltipr">   geom_point( <span
class="tooltiprtext">geom_point() can also be used to add individual
points to the graph. Simply declare the x and y coordinates of the point
you want to plot.</span> </span><span class="tooltipr"> x = <span
class="tooltiprtext">“x =” tells geom_point() that you are going to
declare the x-coordinate for the point.</span> </span><span
class="tooltipr">  14, <span class="tooltiprtext">14 is a number on the
x-axis of your graph. It is the x-coordinate of the point.</span>
</span><span class="tooltipr">  y = <span class="tooltiprtext">“y =”
tells geom_point() that you are going to declare the y-coordinate for
the point.</span> </span><span class="tooltipr">  75 <span
class="tooltiprtext">75 is a number on the y-axis of your graph. It is
the y-coordinate of the point.</span> </span><span class="tooltipr"> ,
size = 3 <span class="tooltiprtext">Use <em>size = 3</em> to make the
point stand out more.</span> </span><span class="tooltipr"> , color =
“firebrick” <span class="tooltiprtext">Use <em>color = “firebrick”</em>
to change the color of the point to firebrick red.</span> </span><span
class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis of
the geom_point() function.</span> </span><span class="tooltipr">  +
<span class="tooltiprtext">The + allows you to add more layers to the
framework provided by ggplot().</span> </span><br><span
class="tooltipr">   geom_text( <span class="tooltiprtext">geom_text()
allows you to add customized text anywhere on the graph. It is very
similar to the base R equivalent, text(…).</span> </span><span
class="tooltipr"> x = <span class="tooltiprtext">“x =” tells geom_text()
that you are going to declare the x-coordinate for the text.</span>
</span><span class="tooltipr">  14, <span class="tooltiprtext">14 is a
number on the x-axis of your graph. It is the x-coordinate of the
text.</span> </span><span class="tooltipr">  y = <span
class="tooltiprtext">“y =” tells geom_text() that you are going to
declare the y-coordinate for the text.</span> </span><span
class="tooltipr">  84, <span class="tooltiprtext">84 is a number on the
y-axis of your graph. It is the y-coordinate of the text.</span>
</span><span class="tooltipr">  label = <span
class="tooltiprtext">“label =” tells geom_text() that you are going to
give it the label.</span> </span><span class="tooltipr">  “My Point (14,
75)”, <span class="tooltiprtext"><em>“My Point (14, 75)”</em> is the
text that will appear on the graph.</span> </span><br><span
class="tooltipr">             color = “navy” <span
class="tooltiprtext">Use <em>color = “navy”</em> to change the color of
the text to navy blue.</span> </span><span class="tooltipr"> , size = 3
<span class="tooltiprtext">Use <em>size = 3</em> to change the size of
the text.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis of the geom_text()
function.</span> </span><span class="tooltipr">  + <span
class="tooltiprtext">The + allows you to add more layers to the
framework provided by ggplot().</span> </span><br><span
class="tooltipr">   theme_minimal() <span class="tooltiprtext">Add a
minimalistic theme to the graph. There are many other themes that you
can try out.</span> </span><span class="tooltipr"
style="float:right;font-size:.8em;">  Click to Show Output  <span
class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
<p></a></p>
<div id="ggplotpoints" style="display:none;">
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
</p>
</div>
<p><br/></p>
<p><strong>Accessing Parts of the Regression</strong></p>
<p>Finally, note that the <code>mylm</code> object contains the
<code>names(mylm)</code> of</p>
<a href="javascript:showhide('coeff')">
<div class="hoverchunk">
<p><span class="tooltipr"> mylm$coefficients <span
class="tooltiprtext">Contains two values. The first is the estimated
<span class="math inline">\(y\)</span>-intercept. The second is the
estimated slope.</span> </span></p>
</div>
<p></a></p>
<div id="coeff" style="display:none;">
<pre><code>## (Intercept)       speed 
##  -17.579095    3.932409</code></pre>
</div>
<a href="javascript:showhide('resid')">
<div class="hoverchunk">
<p><span class="tooltipr"> mylm$residuals <span
class="tooltiprtext">Contains the residuals from the regression in the
same order as the actual dataset.</span> </span></p>
</div>
<p></a></p>
<div id="resid" style="display:none;">
<pre><code>##          1          2          3          4          5          6          7 
##   3.849460  11.849460  -5.947766  12.052234   2.119825  -7.812584  -3.744993 
##          8          9         10         11         12         13         14 
##   4.255007  12.255007  -8.677401   2.322599 -15.609810  -9.609810  -5.609810 
##         15         16         17         18         19         20         21 
##  -1.609810  -7.542219   0.457781   0.457781  12.457781 -11.474628  -1.474628 
##         22         23         24         25         26         27         28 
##  22.525372  42.525372 -21.407036 -15.407036  12.592964 -13.339445  -5.339445 
##         29         30         31         32         33         34         35 
## -17.271854  -9.271854   0.728146 -11.204263   2.795737  22.795737  30.795737 
##         36         37         38         39         40         41         42 
## -21.136672 -11.136672  10.863328 -29.069080 -13.069080  -9.069080  -5.069080 
##         43         44         45         46         47         48         49 
##   2.930920  -2.933898 -18.866307  -6.798715  15.201285  16.201285  43.201285 
##         50 
##   4.268876</code></pre>
</div>
<a href="javascript:showhide('fit')">
<div class="hoverchunk">
<p><span class="tooltipr"> mylm$fitted.values <span
class="tooltiprtext">The values of <span
class="math inline">\(\hat{Y}\)</span> in the same order as the original
dataset.</span> </span></p>
</div>
<p></a></p>
<div id="fit" style="display:none;">
<pre><code>##         1         2         3         4         5         6         7         8 
## -1.849460 -1.849460  9.947766  9.947766 13.880175 17.812584 21.744993 21.744993 
##         9        10        11        12        13        14        15        16 
## 21.744993 25.677401 25.677401 29.609810 29.609810 29.609810 29.609810 33.542219 
##        17        18        19        20        21        22        23        24 
## 33.542219 33.542219 33.542219 37.474628 37.474628 37.474628 37.474628 41.407036 
##        25        26        27        28        29        30        31        32 
## 41.407036 41.407036 45.339445 45.339445 49.271854 49.271854 49.271854 53.204263 
##        33        34        35        36        37        38        39        40 
## 53.204263 53.204263 53.204263 57.136672 57.136672 57.136672 61.069080 61.069080 
##        41        42        43        44        45        46        47        48 
## 61.069080 61.069080 61.069080 68.933898 72.866307 76.798715 76.798715 76.798715 
##        49        50 
## 76.798715 80.731124</code></pre>
</div>
<div class="hoverchunk">
<p><span class="tooltipr"> mylm$… <span class="tooltiprtext">several
other things that will not be explained here.</span> </span></p>
</div>
<p><br/></p>
<p><strong>Making Predictions</strong></p>
<a href="javascript:showhide('prediction')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R
function predict(…) allows you to use an lm(…) object to make
predictions for specified x-values.</span> </span><span
class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a
previously performed lm(…) that was saved into the name
<code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">
 data.frame( <span class="tooltiprtext">To specify the values of <span
class="math inline">\(x\)</span> that you want to use in the prediction,
you have to put those x-values into a data set, or more specifally, a
data.frame(…).</span> </span><span class="tooltipr"> X= <span
class="tooltiprtext">The value for <code>X=</code> should be whatever
x-variable name was used in the original regression. For example, if
<code>mylm &lt;- lm(dist ~ speed, data=cars)</code> was the original
regression, then this code would read <code>speed =</code> instead of
<code>X=</code>… Further, the value of <span
class="math inline">\(Xh\)</span> should be some specific number, like
<code>speed=12</code> for example.</span> </span><span class="tooltipr">
Xh <span class="tooltiprtext">The value of <span
class="math inline">\(Xh\)</span> should be some specific number, like
<code>12</code>, as in <code>speed=12</code> for example.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis for the data.frame(…) function.</span> </span><span
class="tooltipr"> ) <span class="tooltiprtext">Closing parenthesis for
the predict(…) function.</span> </span></p>
</div>
<p></a></p>
<div id="prediction" style="display:none;">
<p><code>mylm &lt;- lm(dist ~ speed, data = cars)</code></p>
<p><code>predict(mylm, data.frame(speed = 12))</code></p>
<table class="rconsole">
<tr>
<td align="right">
<span class="tooltiprout"> 1<br/>   29.60981 <span
class="tooltiprouttext">The value given is the “fitted-value” or
“predicted-value” for the specified x-value. In this case, a car with a
speed of 12 is predicted to have a stopping distance of 29.60981
feet.</span> </span>
</td>
</tr>
</table>
</div>
<a href="javascript:showhide('predictionInterval')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R
function predict(…) allows you to use an lm(…) object to make
predictions for specified x-values.</span> </span><span
class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a
previously performed lm(…) that was saved into the name
<code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">
 data.frame( <span class="tooltiprtext">To specify the values of <span
class="math inline">\(x\)</span> that you want to use in the prediction,
you have to put those x-values into a data set, or more specifally, a
data.frame(…).</span> </span><span class="tooltipr"> X= <span
class="tooltiprtext">The value for <code>X=</code> should be whatever
x-variable name was used in the original regression. For example, if
<code>mylm &lt;- lm(dist ~ speed, data=cars)</code> was the original
regression, then this code would read <code>speed =</code> instead of
<code>X=</code>… Further, the value of <span
class="math inline">\(Xh\)</span> should be some specific number, like
<code>speed=12</code> for example.</span> </span><span class="tooltipr">
Xh <span class="tooltiprtext">The value of <span
class="math inline">\(Xh\)</span> should be some specific number, like
<code>12</code>, as in <code>speed=12</code> for example.</span>
</span><span class="tooltipr"> ), <span class="tooltiprtext">Closing
parenthesis for the data.frame(…) function.</span> </span><span
class="tooltipr">  interval= <span class="tooltiprtext">This optional
command allows you to specify if the predicted value should be
accompanied by either a confidence interval or a prediction
interval.</span> </span><span class="tooltipr"> “prediction” <span
class="tooltiprtext">This specifies that a prediction interval will be
included with the predicted value. A prediction interval gives you a 95%
confidence interval that captures 95% of the data, or <span
class="math inline">\(Y_i\)</span> values for the specific <span
class="math inline">\(X\)</span>-value specified in the
prediction.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis of the predict(…)
function.</span> </span></p>
</div>
<p></a></p>
<div id="predictionInterval" style="display:none;">
<p><code>mylm &lt;- lm(dist ~ speed, data = cars)</code></p>
<p><code>predict(mylm, data.frame(speed = 12), interval = "prediction")</code></p>
<table class="rconsole">
<tr>
<td align="right">
<span class="tooltiprout">   fit <span class="tooltiprouttext">The “fit”
is the predicted value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   lwr <span class="tooltiprouttext">The “lwr”
is the lower bound.</span>
</td>
<td align="right">
<span class="tooltiprout">   upr <span class="tooltiprouttext">The “upr”
is the upper bound.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> 1 29.60981 <span class="tooltiprouttext">In
this case, a car with a speed of 12 mph is predicted to have a stopping
distance of 29.60981 feet. However, we are wise enough to recognize that
the stopping distance for individual cars will vary anywhere from
-1.749529 (or 0 because distance can’t go negative) feet to 60.96915
feet.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> -1.749529 <span class="tooltiprouttext">This
is the lower bound of the prediction interval. While we predict a
stopping distance of 29.60981 feet, this prediction interval reminds us
the stopping distance could be as quick as -1.749529 feet (or 0 because
distance can’t go negative).</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> 60.96915 <span class="tooltiprouttext">This
is the upper bound of the prediction interval. While we predict a
stopping distance of 29.60981 feet, this prediction interval reminds us
that the actual stopping distance could be as high as 60.96915
feet.</span> </span>
</td>
</tr>
</table>
</div>
<a href="javascript:showhide('predictionConfidence')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R
function predict(…) allows you to use an lm(…) object to make
predictions for specified x-values.</span> </span><span
class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a
previously performed lm(…) that was saved into the name
<code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">
 data.frame( <span class="tooltiprtext">To specify the values of <span
class="math inline">\(x\)</span> that you want to use in the prediction,
you have to put those x-values into a data set, or more specifally, a
data.frame(…).</span> </span><span class="tooltipr"> X= <span
class="tooltiprtext">The value for <code>X=</code> should be whatever
x-variable name was used in the original regression. For example, if
<code>mylm &lt;- lm(dist ~ speed, data=cars)</code> was the original
regression, then this code would read <code>speed =</code> instead of
<code>X=</code>… Further, the value of <span
class="math inline">\(Xh\)</span> should be some specific number, like
<code>speed=12</code> for example.</span> </span><span class="tooltipr">
Xh <span class="tooltiprtext">The value of <span
class="math inline">\(Xh\)</span> should be some specific number, like
<code>12</code>, as in <code>speed=12</code> for example.</span>
</span><span class="tooltipr"> ), <span class="tooltiprtext">Closing
parenthesis for the data.frame(…) function.</span> </span><span
class="tooltipr">  interval= <span class="tooltiprtext">This optional
command allows you to specify if the predicted value should be
accompanied by either a confidence interval or a prediction
interval.</span> </span><span class="tooltipr"> “confidence” <span
class="tooltiprtext">This specifies that a confidence interval for the
prediction should be provided. This is of use whenever your interest is
in just estimating the average y-value, not the actual y-values.</span>
</span><span class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis of the predict(…) function.</span> </span></p>
</div>
<p></a></p>
<div id="predictionConfidence" style="display:none;">
<p><code>mylm &lt;- lm(dist ~ speed, data = cars)</code></p>
<p><code>predict(mylm, data.frame(speed = 12), interval = "confidence")</code></p>
<table class="rconsole">
<tr>
<td align="right">
<span class="tooltiprout">   fit <span class="tooltiprouttext">The “fit”
is the predicted value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   lwr <span class="tooltiprouttext">The “lwr”
is the lower bound.</span>
</td>
<td align="right">
<span class="tooltiprout">   upr <span class="tooltiprouttext">The “upr”
is the upper bound.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> 1 29.60981 <span class="tooltiprouttext">In
this case, cars with a speed of 12 mph are predicted to have an average
stopping distance of 29.60981 feet, where the average could be anywhere
from 24.39514 feet to 34.82448 feet.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> 24.39514 <span class="tooltiprouttext">This
is the lower bound of the confidence interval. We are 95% confident that
the average stopping distance of cars going 12 mph is greater than this
value.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> 34.82448 <span class="tooltiprouttext">This
is the upper bound of the confidence interval. We are 95% confident that
the average stopping distance of cars going 12 mph is less than this
value.</span> </span>
</td>
</tr>
</table>
</div>
<p><br/></p>
<p><strong>Finding Confidence Intervals for Model
Parameters</strong></p>
<a href="javascript:showhide('confint')">
<div class="hoverchunk">
<p><span class="tooltipr"> confint( <span class="tooltiprtext">The R
function confint(…) allows you to use an lm(…) object to compute
confidence intervals for one or more parameters (like <span
class="math inline">\(\beta_0\)</span> or <span
class="math inline">\(\beta_1\)</span>) in your model.</span>
</span><span class="tooltipr"> mylm, <span class="tooltiprtext">This is
the name of a previously performed lm(…) that was saved into the name
<code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">
 level = <span class="tooltiprtext">“level =” tells the confint(…)
function that you are going to declare at what level of confidence you
want the interval. The default is “level = 0.95.” If you want to find
95% confidence intervals for your parameters, then just run
<code>confint(mylm)</code>.</span> </span><span class="tooltipr">
 someConfidenceLevel <span class="tooltiprtext">someConfidenceLevel is
simply a confidence level you choose when you want something other than
a 95% confidence interval. Some examples of appropriate levels include
0.90 and 0.99.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis for confint(..)
function.</span> </span></p>
</div>
<p></a></p>
<div id="confint" style="display:none;">
<p><code>mylm &lt;- lm(dist ~ speed, data = cars)</code></p>
<p><code>confint(mylm, level = 0.90)</code></p>
<table class="rconsole">
<tr>
<td align="left">
 
</td>
<td align="right">
<span class="tooltiprout">   5 % <span class="tooltiprouttext">The lower
bound of a 90% confidence interval occurs at the 5th percentile. This is
because at 90% confidence, 10% is left in the tails, with 5% on each
end. The upper bound will thus end at the 95th percentile, hence the 5%
and 95% as the column names.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   95 % <span class="tooltiprouttext">The
upper bound of a 90% confidence interval ends at the 95th
percentile.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> (Intercept) <span
class="tooltiprouttext">This row of output specifies a confidence
interval for <span class="math inline">\(\beta_0\)</span>, the true
y-intercept.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> -28.914514 <span class="tooltiprouttext">This
is the lower bound for the confidence interval of the y-intercept, <span
class="math inline">\(\beta_0\)</span>. In this example, the confidence
interval for the y-intercept does not make sense because you cannot have
negative distance.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> -6.243676 <span class="tooltiprouttext">This
is the upper bound for the confidence interval for <span
class="math inline">\(\beta_0\)</span>, the y-intercpet. In this
example, the confidence interval for the y-intercept does not make sense
because you cannot have negative distance.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> speed <span class="tooltiprouttext">This row
of the output provides the upper and lower bound for the confidence
interval for <span class="math inline">\(\beta_1\)</span>, the true
slope. In this case, you can be 90% confident that the true slope lies
between 3.235501 and 4.629317.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 3.235501 <span class="tooltiprouttext">This
is the lower bound of the confidence interval. In this case, you can be
90% confident that the slope lies between 3.235501 and 4.629317.</span>
</span>
</td>
<td align="right">
<span class="tooltiprout"> 4.629317 <span class="tooltiprouttext">This
is the upper bound of the confidence interval. In this case, you can be
90% confident that the slope lies between 3.235501 and 4.629317.</span>
</span>
</td>
</td>
</tr>
</table>
<p><br/> <br/></p>
<p><code>mylm &lt;- lm(dist ~ speed, data = cars)</code></p>
<p><code>confint(mylm, level = 0.95)</code></p>
<table class="rconsole">
<tr>
<td align="left">
 
</td>
<td align="right">
<span class="tooltiprout">   2.5 % <span class="tooltiprouttext">The
lower bound of a 95% confidence interval occurs at the 2.5th percentile.
This is because at 95% confidence, 5% is left in the tails, with 2.5% on
each end. The upper bound will thus end at the 97.5th percentile, hence
the 2.5% and 97.5% as the column names for the lower and upper bounds,
respectively.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   97.5 % <span class="tooltiprouttext">The
upper bound of a 95% confidence interval ends at the 97.5th
percentile.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> (Intercept) <span
class="tooltiprouttext">This row of output specifies a confidence
interval for <span class="math inline">\(\beta_0\)</span>, the true
y-intercept.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> -31.167850 <span class="tooltiprouttext">This
is the lower bound for the confidence interval of the y-intercept, <span
class="math inline">\(\beta_0\)</span>. In this example, the confidence
interval for the y-intercept does not make sense because you cannot have
negative distance.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> -3.990340 <span class="tooltiprouttext">This
is the upper bound for the confidence interval for <span
class="math inline">\(\beta_0\)</span>, the y-intercpet. In this
example, the confidence interval for the y-intercept does not make sense
because you cannot have negative distance.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> speed <span class="tooltiprouttext">This row
of the output provides the upper and lower bound for the confidence
interval for <span class="math inline">\(\beta_1\)</span>, the true
slope. In this case, you can be 90% confident that the true slope lies
between 3.096964 and 4.767853.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 3.096964 <span class="tooltiprouttext">This
is the lower bound of the confidence interval. In this case, you can be
90% confident that the slope lies between 3.096964 and 4.767853</span>
</span>
</td>
<td align="right">
<span class="tooltiprout"> 4.767853 <span class="tooltiprouttext">This
is the upper bound of the confidence interval. In this case, you can be
95% confident that the slope lies between 3.096964 and 4.767853</span>
</span>
</td>
</td>
</tr>
</table>
</div>
<hr />
</div>
</div>
<div id="explanation" class="section level3">
<h3>Explanation</h3>
<div style="padding-left:125px;">
<p>Linear regression has a rich mathematical theory behind it. This is
because it uses a mathematical function and a random error term to
describe the regression relation between a response variable <span
class="math inline">\(Y\)</span> and an explanatory variable called
<span class="math inline">\(X\)</span>.</p>
<div style="padding-left:30px;color:darkgray;">
<p>Expand each element below to learn more.</p>
</div>
<p><span
style="color:steelblue;font-size:.8em;padding-left:160px;">Regression
Cheat Sheet</span>
<a href="javascript:showhide('regressioncheatsheet')" style="font-size:.6em;color:skyblue;">(Expand)</a></p>
<div id="regressioncheatsheet" style="display:none;font-size:.7em;">
<table>
<colgroup>
<col width="13%" />
<col width="34%" />
<col width="15%" />
<col width="17%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Pronunciation</th>
<th>Meaning</th>
<th>Math</th>
<th>R Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(Y_i\)</span><span
class="tooltiprtext"> <code>$Y_i$</code></span>
</span><span class="tooltipr"></td>
<td>“why-eye”</td>
<td>The data</td>
<td><span class="tooltipr"> <span class="math inline">\(Y_i = \beta_0 +
\beta_1 X_i + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0,
\sigma^2)\)</span><span class="tooltiprtext">
<code>$Y_i = \beta_0 + \beta_1 X_i +</code>
<code>\epsilon_i \quad \text{where} \</code>
<code>\epsilon_i \sim N(0, \sigma^2)$</code></span>
</span><span class="tooltipr"></td>
<td><code>YourDataSet$YourYvariable</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span
class="math inline">\(\hat{Y}_i\)</span><span class="tooltiprtext">
<code>$\hat{Y}_i$</code></span> </span><span class="tooltipr"></td>
<td>“why-hat-eye”</td>
<td>The fitted line</td>
<td><span class="tooltipr"> <span class="math inline">\(\hat{Y}_i = b_0
+ b_1 X_i\)</span><span class="tooltiprtext">
<code>$\hat{Y}_i = b_0 + b_1 X_i$</code></span></td>
<td><code>lmObject$fitted.values</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span
class="math inline">\(E\{Y_i\}\)</span><span class="tooltiprtext">
<code>$E\{Y_i\}$</code></span> </span><span class="tooltipr"></td>
<td>“expected value of why-eye”</td>
<td>True mean y-value</td>
<td><span class="tooltipr"><span class="math inline">\(E\{Y_i\} =
\beta_0 + \beta_1 X_i\)</span><span class="tooltiprtext">
<code>$E\{Y_i\} = \beta_0 + \beta_1 X_i$</code></span></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span
class="math inline">\(\beta_0\)</span><span class="tooltiprtext">
<code>$\beta_0$</code></span> </span><span class="tooltipr"></td>
<td>“beta-zero”</td>
<td>True y-intercept</td>
<td><code>&lt;none&gt;</code></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span
class="math inline">\(\beta_1\)</span><span class="tooltiprtext">
<code>$\beta_1$</code></span> </span><span class="tooltipr"></td>
<td>“beta-one”</td>
<td>True slope</td>
<td><code>&lt;none&gt;</code></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(b_0\)</span><span
class="tooltiprtext"> <code>$b_0$</code></span>
</span><span class="tooltipr"></td>
<td>“b-zero”</td>
<td>Estimated y-intercept</td>
<td><span class="tooltipr"><span class="math inline">\(b_0 = \bar{Y} -
b_1\bar{X}\)</span><span class="tooltiprtext">
<code>$b_0 = \bar{Y} - b_1\bar{X}</code></span></td>
<td><code>b_0 &lt;- mean(Y) - b_1*mean(X)$</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(b_1\)</span><span
class="tooltiprtext"> <code>$b_1$</code></span>
</span><span class="tooltipr"></td>
<td>“b-one”</td>
<td>Estimated slope</td>
<td><span class="tooltipr"><span class="math inline">\(b_1 = \frac{\sum
X_i(Y_i - \bar{Y})}{\sum(X_i - \bar{X})^2}\)</span><span
class="tooltiprtext"> <code>$b_1 = \frac{\sum X_i(Y_i - \bar{Y})}</code>
<code>{\sum(X_i - \bar{X})^2}$</code></span></td>
<td><code>b_1 &lt;- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 )</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span
class="math inline">\(\epsilon_i\)</span><span class="tooltiprtext">
<code>$\epsilon_i$</code></span> </span><span class="tooltipr"></td>
<td>“epsilon-eye”</td>
<td>Distance of dot to true line</td>
<td><span class="tooltipr"><span class="math inline">\(\epsilon_i = Y_i
- E\{Y_i\}\)</span><span class="tooltiprtext">
<code>$\epsilon_i = Y_i - E\{Y_i\}$</code></span></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(r_i\)</span><span
class="tooltiprtext"> <code>$r_i$</code></span>
</span><span class="tooltipr"></td>
<td>“r-eye” or “residual-eye”</td>
<td>Distance of dot to estimated line</td>
<td><span class="tooltipr"><span class="math inline">\(r_i = Y_i -
\hat{Y}_i\)</span><span class="tooltiprtext">
<code>$r_i = Y_i - \hat{Y}_i$</code></span></td>
<td><code>lmObject$residuals</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span
class="math inline">\(\sigma^2\)</span><span class="tooltiprtext">
<code>$\sigma^2$</code></span> </span><span class="tooltipr"></td>
<td>“sigma-squared”</td>
<td>Variance of the <span class="math inline">\(\epsilon_i\)</span></td>
<td><span class="tooltipr"><span class="math inline">\(Var\{\epsilon_i\}
= \sigma^2\)</span><span
class="tooltiprtext"><code>$Var\{\epsilon_i\} = \sigma^2$</code></span></td>
<td><code>&lt;none&gt;</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(MSE\)</span><span
class="tooltiprtext"> <code>$MSE$</code></span>
</span><span class="tooltipr"></td>
<td>“mean squared error”</td>
<td>Estimate of <span class="math inline">\(\sigma^2\)</span></td>
<td><span class="tooltipr"><span class="math inline">\(MSE =
\frac{SSE}{n-p}\)</span><span
class="tooltiprtext"><code>$MSE = \frac{SSE}{n-p}$</code></span></td>
<td><code>sum( lmObject$res^2 ) / (n - p)</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(SSE\)</span><span
class="tooltiprtext"> <code>$SSE$</code></span>
</span><span class="tooltipr"></td>
<td>“sum of squared error” (residuals)</td>
<td>Measure of dot’s total deviation from the line</td>
<td><span class="tooltipr"><span class="math inline">\(SSE =
\sum_{i=1}^n (Y_i - \hat{Y}_i)^2\)</span><span
class="tooltiprtext"><code>$SSE = \sum_{i=1}^n</code>
<code>(Y_i - \hat{Y}_i)^2$</code></span></td>
<td><code>sum( lmObject$res^2 )</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(SSR\)</span><span
class="tooltiprtext"> <code>$SSR$</code></span>
</span><span class="tooltipr"></td>
<td>“sum of squared regression error”</td>
<td>Measure of line’s deviation from y-bar</td>
<td><span class="tooltipr"> <span class="math inline">\(SSR =
\sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2\)</span><span
class="tooltiprtext"><code>$SSR = \sum_{i=1}^n</code>
<code>(\hat{Y}_i - \bar{Y})^2$</code></span></td>
<td><code>sum( (lmObject$fit - mean(YourData$Y))^2 )</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span
class="math inline">\(SSTO\)</span><span class="tooltiprtext">
<code>$SSTO$</code></span> </span><span class="tooltipr"></td>
<td>“total sum of squares”</td>
<td>Measure of total variation in Y</td>
<td><span class="tooltipr"><span class="math inline">\(SSR + SSE = SSTO
= \sum_{i=1}^n (Y_i - \bar{Y})^2\)</span><span
class="tooltiprtext"><code>$SSR + SSE = SSTO = \sum_{i=1}^n</code>
<code>(Y_i - \bar{Y})^2$</code></span></td>
<td><code>sum( (YourData$Y - mean(YourData$Y))^2 )</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span class="math inline">\(R^2\)</span><span
class="tooltiprtext"> <code>$R^2$</code></span>
</span><span class="tooltipr"></td>
<td>“R-squared”</td>
<td>Proportion of variation in Y explained by the regression</td>
<td><span class="tooltipr"><span class="math inline">\(R^2 =
\frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}\)</span><span
class="tooltiprtext"><code>$R^2 = \frac{SSR}{SSTO} = 1</code>
<code>- \frac{SSE}{SSTO}$</code></span></td>
<td><code>SSR/SSTO</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(r\)</span><span
class="tooltiprtext"> <code>$r$</code></span>
</span><span class="tooltipr"></td>
<td>“r”</td>
<td>Correlation between X and Y.</td>
<td><span class="tooltipr"><span class="math inline">\(r =
\sqrt{R^2}\)</span><span
class="tooltiprtext"><code>$r = \sqrt{R^2}$</code></span></td>
<td><code>sqrt(R^2)</code></td>
</tr>
<tr class="odd">
<td><span class="tooltipr"><span
class="math inline">\(\hat{Y}_h\)</span><span class="tooltiprtext">
<code>$\hat{Y}_h$</code></span></span></td>
<td>“why-hat-aitch”</td>
<td>Estimated mean y-value for some x-value called <span
class="math inline">\(X_h\)</span></td>
<td><span class="tooltipr"><span class="math inline">\(\hat{Y}_h = b_0 +
b_1 X_h\)</span><span
class="tooltiprtext"><code>$\hat{Y}_h = b_0 + b_1 X_h$</code></span></span></td>
<td><code>predict(lmObject, data.frame(XvarName=#))</code></td>
</tr>
<tr class="even">
<td><span class="tooltipr"><span class="math inline">\(X_h\)</span><span
class="tooltiprtext"> <code>$X_h$</code></span> </span></td>
<td>“ex-aitch”</td>
<td>Some x-value, not necessarily one of the <span
class="math inline">\(X_i\)</span> values used in the regression</td>
<td><span class="tooltipr"><span class="math inline">\(X_h =\)</span>
some number<span
class="tooltiprtext"><code>$X_h = $</code></span></span></td>
<td><code>Xh = #</code></td>
</tr>
<tr class="odd">
<td>Confidence Interval</td>
<td>“confidence interval”</td>
<td>Estimated bounds at a certain level of confidence for a
parameter</td>
<td><span class="tooltipr"><span class="math inline">\(b_0 \pm t^* \cdot
s_{b_0}\)</span><span
class="tooltiprtext"><code>b_0 \pm t^* \cdot s_{b_0}</code></span></span>
or <span class="tooltipr"><span class="math inline">\(b_1 \pm t^* \cdot
s_{b_1}\)</span><span
class="tooltiprtext"><code>b_1 \pm t^* \cdot s_{b_1}</code></span></span></td>
<td><code>confint(mylm, level = someConfidenceLevel)</code></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td><span class="math inline">\(b_0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td><span class="math inline">\(b_1\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\epsilon_i\)</span></td>
<td><span class="math inline">\(r_i\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(MSE\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\sigma\)</span></td>
<td><span class="math inline">\(\sqrt{MSE}\)</span>, the Residual
standard error</td>
</tr>
</tbody>
</table>
</div>
<p><br /></p>
<div id="the-mathematical-model-expand" class="section level4">
<h4>The Mathematical Model
<a href="javascript:showhide('mathmodel1')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span
class="math inline">\(Y_i\)</span>, <span
class="math inline">\(\hat{Y}_i\)</span>, and <span
class="math inline">\(E\{Y_i\}\)</span>…</span></p>
<div id="mathmodel1" style="display:none;">
<p>There are three main elements to the mathematical model of
regression. Each of these three elements is pictured below in the
“Regression Relation Diagram.”</p>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>Study both the three bullet points and their visual representations
in the plot below for a clearer understanding.</p>
</div>
<ol style="list-style-type: decimal">
<li>The <strong>true line</strong>, i.e., the regression relation:</li>
</ol>
<div style="padding-left:60px;color:darkgray;">
<div style="color:steelblue;">
<p><span class="math inline">\(\underbrace{E\{Y\}}_{\substack{\text{true
mean} \\ \text{y-value}}} =
\underbrace{\overbrace{\beta_0}^\text{y-intercept} +
\overbrace{\beta_1}^\text{slope} X}_\text{equation of a
line}\)</span></p>
</div>
<p><a href="javascript:showhide('readmoretrueline')" style="font-size:.8em;color:skyblue;">(Read
more…)</a></p>
<div id="readmoretrueline" style="display:none;">
<p>The true line is shown by the dotted line in the graph pictured
below. This is typically unobservable. Think of it as “natural law” or
“God’s law”. It is some true line that is unknown to us.</p>
<p>The regression relation <span class="math inline">\(E\{Y\} = \beta_0
+ \beta_1 X\)</span> creates the line of regression where <span
class="math inline">\(\beta_0\)</span> is the <span
class="math inline">\(y\)</span>-intercept of the line and <span
class="math inline">\(\beta_1\)</span> is the slope of the line. The
regression relationship provides the average <span
class="math inline">\(Y\)</span>-value, denoted <span
class="math inline">\(E\{Y_i\}\)</span>, for a given <span
class="math inline">\(X\)</span>-value, denoted by <span
class="math inline">\(X_i\)</span>.</p>
<p>Note: <span class="math inline">\(E\{Y\}\)</span> is pronounced “the
expected value of y” because, well… the mean is the typical, average, or
“expected” value.</p>
</div>
</div>
<ol start="2" style="list-style-type: decimal">
<li>The <strong>dots</strong>, i.e., the regression relation plus an
error term:</li>
</ol>
<div style="padding-left:60px;color:darkgray;">
<div style="color:steelblue;">
<p><span class="math inline">\(Y_i = \underbrace{\beta_0 + \beta_1
X_i}_{E\{Y_i\}} + \underbrace{\epsilon_i}_\text{error term} \quad
\text{where} \ \epsilon_i\sim N(0,\sigma^2)\)</span></p>
</div>
<p><a href="javascript:showhide('readmoredots')" style="font-size:.8em;color:skyblue;">(Read
more…)</a></p>
<div id="readmoredots" style="display:none;">
<p>This is shown by the dots in the graph below. This is the data. In
regression, the assumption is that the y-value for individual <span
class="math inline">\(i\)</span>, denoted by <span
class="math inline">\(Y_i\)</span>, was “created” by adding an error
term <span class="math inline">\(\epsilon_i\)</span> to each
individual’s “expected” value <span class="math inline">\(\beta_0 +
\beta_1 X_i\)</span>. Note the “order of creation” would require first
knowing an indivual’s x-value, <span class="math inline">\(X_i\)</span>,
then their expected value from the regression relation <span
class="math inline">\(E\{Y_i\} = \beta_0 + \beta_1 X_i\)</span> and then
adding their <span class="math inline">\(\epsilon_i\)</span> value to
the result. The <span class="math inline">\(\epsilon_i\)</span> allows
each individual to deviate from the line. Some individuals deviate
dramatically, some deviate only a little, but all dots vary some
distance <span class="math inline">\(\epsilon_i\)</span> from the
line.</p>
<p>Note: <span class="math inline">\(Y_i\)</span> is pronounced
“why-eye” because it is the y-value for individual <span
class="math inline">\(i\)</span>. Sometimes also called “why-sub-eye”
because <span class="math inline">\(i\)</span> is in the subscript of
<span class="math inline">\(Y\)</span>.</p>
</div>
</div>
<ol start="3" style="list-style-type: decimal">
<li>The <strong>estimated line</strong>, i.e., the line we get from a
sample of data.</li>
</ol>
<div style="padding-left:60px;color:darkgray;">
<div style="color:steelblue;">
<p><span
class="math inline">\(\underbrace{\hat{Y}_i}_{\substack{\text{estimated
mean} \\ \text{y-value}}} = \underbrace{b_0 + b_1 X_i}_\text{estimated
regression equation}\)</span></p>
</div>
<p><a href="javascript:showhide('readmoreestimatedline')" style="font-size:.8em;color:skyblue;">(Read
more…)</a></p>
<div id="readmoreestimatedline" style="display:none;">
<p>The estimated line is shown by the solid line in the graph below.
<span class="math inline">\(\hat{Y}\)</span> is the estimated regression
equation obtained from the sample of data. It is the estimator of the
true regression equation <span class="math inline">\(E\{Y\}\)</span>. So
<span class="math inline">\(\hat{Y}\)</span> is interpreted as the
estimated average (or mean) <span class="math inline">\(Y\)</span>-value
for any given <span class="math inline">\(X\)</span>-value. Thus, <span
class="math inline">\(b_0\)</span> is the estimated y-intercept and
<span class="math inline">\(b_1\)</span> is the estimated slope. The b’s
are sample statistics, like <span class="math inline">\(\bar{x}\)</span>
and the <span class="math inline">\(\beta\)</span>’s are population
parameters like <span class="math inline">\(\mu\)</span>. The <span
class="math inline">\(b\)</span>’s estimate the <span
class="math inline">\(\beta\)</span>’s.</p>
<p>Note: <span class="math inline">\(\hat{Y}_i\)</span> is pronounced
“why-hat-eye” and is known as the “estimated y-value” or “fitted
y-value” because it is the y-value you get from <span
class="math inline">\(b_0 + b_1 X_i\)</span>. It is always different
from <span class="math inline">\(Y_i\)</span> because dots are rarely if
ever exactly on the estimated regression line.</p>
</div>
</div>
<p>This graphic depicts the true, but typically unknown, regression
relation (dotted line). It also shows how a sample of data from the true
regression relation (the dots) can be used to obtain an estimated
regression equation (solid line) that is fairly close to the truth
(dotted line).</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Something to ponder: The true line, when coupled with the error
terms, “creates” the data. The estimated (or fitted) line uses the
sampled data to try to “re-create” the true line.</p>
<p>We could loosely call this the “order of creation” as shown by the
following diagram.</p>
<pre class="r"><code>par(mfrow=c(1,3), mai=c(.2,.2,.4,.1))
plot(y ~ x, col=&quot;white&quot;,  main=&quot;A Law is Given&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;)
curve(beta0 + beta1*x, add=TRUE, lty=2)
plot(y ~ x, pch=16, main=&quot;Data is Created&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;)
curve(beta0 + beta1*x, add=TRUE, lty=2)
plot(y ~ x, pch=16, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;The Law is Estimated&quot;)
curve(xylm$coef[1] + xylm$coef[2]*x, add=TRUE, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;)
curve(beta0 + beta1*x, add=TRUE, lty=2)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<table>
<colgroup>
<col width="29%" />
<col width="31%" />
<col width="38%" />
</colgroup>
<thead>
<tr class="header">
<th>A Law is Given</th>
<th>Data is Created</th>
<th>The Law is Estimated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(E\{Y_i\} = \beta_0 + \beta_1
X_i\)</span></td>
<td><span class="math inline">\(Y_i = E\{Y_i\} +
\epsilon_i\)</span></td>
<td><span class="math inline">\(\hat{Y}_i = b_0 + b_1 X_i\)</span></td>
</tr>
<tr class="even">
<td>The true line is the “law”.</td>
<td>The <span class="math inline">\(Y_i\)</span> are created by adding
<span class="math inline">\(\epsilon_i\)</span> to <span
class="math inline">\(E\{Y_i\}\)</span> where <span
class="math inline">\(E\{Y_i\} = \beta_0 + \beta_1 X_i\)</span>.</td>
<td>The law is estimated with <span
class="math inline">\(\hat{Y}_i\)</span> which is given with
<code>lm(...)</code>.</td>
</tr>
</tbody>
</table>
<p>Click open the “Code” buttom below to the right to find code that
runs a simulation demonstrating this “order of creation”.</p>
<pre class="r"><code>## Simulating Data from a Regression Model
## This R-chunk is meant to be played in your R Console.
## It allows you to explore how the various elements
## of the regression model combine together to &quot;create&quot;
## data and then use the data to &quot;re-create&quot; the line.

set.seed(101) #Allows us to always get the same &quot;random&quot; sample
              #Change to a new number to get a new sample

  n &lt;- 3 #set the sample size

  X_i &lt;- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45.

  beta0 &lt;- 3 #Our choice for the y-intercept. 

  beta1 &lt;- .1 #Our choice for the slope. 

  sigma &lt;- 12.5 #Our choice for the std. deviation of the error terms.

  epsilon_i &lt;- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

  Y_i &lt;- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model

  fabData &lt;- data.frame(y=Y_i, x=X_i) #Store the data as data

  View(fabData) 
  
  #In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.

  fab.lm &lt;- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData.

  summary(fab.lm) #Summarize your model. 

  plot(y ~ x, data=fabData) #Plot the data.

  abline(fab.lm) #Add the estimated regression line to your plot.

# Now for something you can&#39;t do in real life... but since we created the data...

  abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). 

  legend(&quot;topleft&quot;, legend=c(&quot;True Line&quot;, &quot;Estimated Line&quot;), lty=c(2,1), bty=&quot;n&quot;) #Add a legend to your plot specifying which line is which.</code></pre>
</div>
<p><br /></p>
</div>
<div id="interpreting-the-model-parameters-expand"
class="section level4">
<h4>Interpreting the Model Parameters
<a href="javascript:showhide('interpretingparameters')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span
class="math inline">\(\beta_0\)</span> (intercept) and <span
class="math inline">\(\beta_1\)</span> (slope), estimated by <span
class="math inline">\(b_0\)</span> and <span
class="math inline">\(b_1\)</span>, interpreted as…</span></p>
<div id="interpretingparameters" style="display:none;">
<p>The interpretation of <span class="math inline">\(\beta_0\)</span> is
only meaningful if <span class="math inline">\(X=0\)</span> is in the
scope of the model. If <span class="math inline">\(X=0\)</span> is in
the scope of the model, then the intercept is interpreted as the average
y-value, denoted <span class="math inline">\(E\{Y\}\)</span>, when <span
class="math inline">\(X=0\)</span>.</p>
<p>The interpretation of <span class="math inline">\(\beta_1\)</span> is
the amount of increase (or decrease) in the average y-value, denoted
<span class="math inline">\(E\{Y\}\)</span>, per unit change in <span
class="math inline">\(X\)</span>. It is often misunderstood to be the
“average change in y” or just “the change in y” but it is more correctly
referred to as the “change in the average y”.</p>
<p>To better see this, consider the three graphics shown below.</p>
<pre class="r"><code>par(mfrow=c(1,3))
hist(mtcars$mpg, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Number of Vehicles&quot;, xlab=&quot;Gas Mileage (mpg)&quot;, col=&quot;skyblue&quot;)
boxplot(mpg ~ cyl, data=mtcars, border=&quot;skyblue&quot;, boxwex=0.5, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Gas Mileage (mpg)&quot;, xlab=&quot;Number of Cylinders of Engine (cyl)&quot;)
plot(mpg ~ qsec, data=subset(mtcars, am==0), pch=16, col=&quot;skyblue&quot;, main=&quot;Gas Mileage of mtcars Vehicles&quot;, ylab=&quot;Gas Mileage (mpg)&quot;, xlab=&quot;Quarter Mile Time (qsec)&quot;)
abline(lm(mpg ~ qsec, data=subset(mtcars, am==0)), col=&quot;darkgray&quot;)
mtext(side=3, text=&quot;Automatic Transmissions Only (am==0)&quot;, cex=0.5)
abline(v = seq(16,22,2), h=seq(10,30,5), lty=3, col=&quot;gray&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<table>
<colgroup>
<col width="31%" />
<col width="31%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th>The Histogram</th>
<th>The Boxplot</th>
<th>The Scatterplot</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>The <strong>histogram</strong> on the left shows gas mileages of
vehicles from the mtcars data set. The average gas mileage is
20.09.</td>
<td>The <strong>boxplot</strong> in the middle shows that if we look at
gas mileage for 4, 6, and 8 cylinder vehicles separately, we find the
means to be 26.66, 19.74, and 15.1, respectively. If we wanted to, we
could talk about the change in the means across cylinders, and would see
that the mean is decreasing, first by <span class="math inline">\(26.66
- 19.74 = 6.92\)</span> mpg, then by <span class="math inline">\(19.74 -
15.1 = 4.64\)</span> mpg.</td>
<td>The <strong>scatterplot</strong> on the right shows that the average
gas mileage (for just automatic transmission vehicles) increases by a
slope of 1.44 for each 1 second increase in quarter mile time. In other
words, the line gives the average y-value for any x-value. Thus, the
slope of the line is the change in the average y-value.</td>
</tr>
</tbody>
</table>
</div>
<p><br /></p>
</div>
<div id="residuals-and-errors-expand" class="section level4">
<h4>Residuals and Errors
<a href="javascript:showhide('residualsanderrors')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span
class="math inline">\(r_i\)</span>, the residual, estimates <span
class="math inline">\(\epsilon_i\)</span>, the true error…</span></p>
<div id="residualsanderrors" style="display:none;">
<p>Residuals are the difference between the observed value of <span
class="math inline">\(Y_i\)</span> (the point) and the predicted, or
estimated value, for that point called <span
class="math inline">\(\hat{Y_i}\)</span>. The errors are the true
distances between the observed <span class="math inline">\(Y_i\)</span>
and the actual regression relation for that point, <span
class="math inline">\(E\{Y_i\}\)</span>.</p>
<p>We will denote a <strong>residual</strong> for individual <span
class="math inline">\(i\)</span> by <span
class="math inline">\(r_i\)</span>, <span class="math display">\[
  r_i = \underbrace{Y_i}_{\substack{\text{Observed} \\ \text{Y-value}}}
- \underbrace{\hat{Y}_i}_{\substack{\text{Predicted} \\ \text{Y-value}}}
\quad \text{(residual)}
\]</span> The residual <span class="math inline">\(r_i\)</span>
estimates the true <strong>error</strong> for individual <span
class="math inline">\(i\)</span>, <span
class="math inline">\(\epsilon_i\)</span>, <span class="math display">\[
  \epsilon_i = \underbrace{Y_i}_{\substack{\text{Observed} \\
\text{Y-value}}} - \underbrace{E\{Y_i\}}_{\substack{\text{True Mean} \\
\text{Y-value}}} \quad \text{(error)}
\]</span></p>
<p>In summary…</p>
<div style="padding-left:30px;">
<table>
<colgroup>
<col width="44%" />
<col width="55%" />
</colgroup>
<thead>
<tr class="header">
<th>Residual <span class="math inline">\(r_i\)</span></th>
<th>Error <span class="math inline">\(\epsilon_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Distance between the dot <span class="math inline">\(Y_i\)</span>
and the estimated line <span
class="math inline">\(\hat{Y}_i\)</span></td>
<td>Distance between the dot <span class="math inline">\(Y_i\)</span>
and the true line <span class="math inline">\(E\{Y_i\}\)</span>.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(r_i = Y_i - \hat{Y}_i\)</span></td>
<td><span class="math inline">\(\epsilon_i = Y_i -
E\{Y_i\}\)</span></td>
</tr>
<tr class="odd">
<td>Known</td>
<td>Typically Unknown</td>
</tr>
</tbody>
</table>
</div>
<p>As shown in the graph below, the residuals are known values and they
estimate the unknown (but true) error terms.</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Keep in mind the idea that the errors <span
class="math inline">\(\epsilon_i\)</span> “created” the data and that
the residuals <span class="math inline">\(r_i\)</span> are computed
after using the data to “re-create” the line.</p>
<p>Residuals have many uses in regression analysis. They allow us to</p>
<ol style="list-style-type: decimal">
<li>diagnose the regression assumptions,</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Assumptions” section below for more details.</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>estimate the regression relation,</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Estimating the Model Parameters” section below for more
details.</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>estimate the variance of the error terms,</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Estimating the Model Variance” section below for more
details.</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li>and assess the fit of the regression relation.</li>
</ol>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>See the “Assessing the Fit of a Regression” section below for more
details.</p>
</div>
</div>
<p><br /></p>
</div>
<div id="assessing-the-fit-of-a-regression-expand"
class="section level4">
<h4>Assessing the Fit of a Regression
<a href="javascript:showhide('assessingthefit')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span
class="math inline">\(R^2\)</span>, SSTO, SSR, and SSE…</span></p>
<div id="assessingthefit" style="display:none;">
<p>Not all regressions are created equally as the three plots below
show. Sometimes the dots are a clustered very tightly to the line. At
other times, the dots spread out fairly dramatically from the line.</p>
<pre class="r"><code>par(mfrow=c(1,3), mai=c(.1,.1,.5,.1))
set.seed(2)
x &lt;- runif(30,0,20)
y1 &lt;- 2 + 3.5*x + rnorm(30,0,2)
y2 &lt;- 2 + 3.5*x + rnorm(30,0,8)
y3 &lt;- 2 + 3.5*x + rnorm(30,0,27)
plot(y1 ~ x, pch=16, col=&quot;darkgray&quot;, xlim=c(-1,21), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylim=c(-10,100), main=&quot;Excellent Fit&quot;)
abline(lm(y1 ~ x), col=&quot;gray&quot;)
plot(y2 ~ x, pch=16, col=&quot;darkgray&quot;, xlim=c(-1,21), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylim=c(-10,100), main=&quot;Good Fit&quot;)
abline(lm(y2 ~ x), col=&quot;gray&quot;)
plot(y3 ~ x, pch=16, col=&quot;darkgray&quot;, xlim=c(-1,21), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylim=c(-10,100), main=&quot;Poor Fit&quot;)
abline(lm(y3 ~ x), col=&quot;gray&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>A common way to measure the fit of a regression is with <a
href="NumericalSummaries.html#correlation">correlation</a>. While this
can be a useful measurement, there is greater insight in using the
square of the correlation, called <span
class="math inline">\(R^2\)</span>. (If you are a Math 325 student, just
stick with correlation for now and skip on to the next section of this
Explanation tab. If you are a Math 425 student, it is critical that you
come to understand <span class="math inline">\(R^2\)</span> deeply, so
read on.)</p>
<p>Before you can understand <span class="math inline">\(R^2\)</span>,
you must understand three important “sums of squares”.</p>
<div style="padding-left:30px;">
<p><a href="javascript:showhide('sumsofsquaresread')" style="font-size:.8em;color:skyblue;">(Read
more about sums…)</a></p>
<div id="sumsofsquaresread" style="display:none;">
<p>A sum is just a fancy word for adding things together. <span
class="math display">\[
  1 + 2 + 3 + 4 + 5 + 6 = 21
\]</span></p>
<p>Long sums get tedious to write out by hand. So we use the symbol
<span class="math inline">\(\Sigma\)</span> to denote the word “sum”.
Further, we use a subscript <span
class="math inline">\(\Sigma_{i=1}\)</span> to state what value the sum
is beginning with, and a superscript <span
class="math inline">\(\Sigma_{i=1}^6\)</span> to state the value we are
ending at. This gives <span class="math display">\[
  \sum_{i=1}^6 i = 1 + 2 + 3 + 4 + 5 + 6 = 21
\]</span></p>
<p>Test your knowledge, do you see why the answer is 6 to the sum below?
<span class="math display">\[
  \sum_{i=1}^3 i = 6
\]</span></p>
<p>Computing sums in R is fairly easy. Type the following codes in your
R Console.</p>
<p><code>sum(1:6) #gives the answer of 21</code></p>
<p><code>sum(1:3) #gives the answer of 6</code></p>
<p>However, sums really become useful when used with a data set.</p>
<p>Each row of a data set represents an “individual’s” data. We can
reference each individual with a row number. In the data below,
individual 3, denoted by <span class="math inline">\(i=3\)</span>, has a
<code>speed</code> of 7 and a <code>dist</code> of 4.</p>
<pre class="r"><code>pander(head(cbind(Individual = 1:50, cars), 6), emphasize.strong.rows=3)</code></pre>
<table style="width:40%;">
<colgroup>
<col width="18%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Individual</th>
<th align="center">speed</th>
<th align="center">dist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">4</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">4</td>
<td align="center">10</td>
</tr>
<tr class="odd">
<td align="center"><strong>3</strong></td>
<td align="center"><strong>7</strong></td>
<td align="center"><strong>4</strong></td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">7</td>
<td align="center">22</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">8</td>
<td align="center">16</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center">9</td>
<td align="center">10</td>
</tr>
</tbody>
</table>
<p>To compute the sum of the <strong>speed</strong> column, use
<code>sum(speed)</code>. If we divided this sum by 6, we would get the
mean of speed <code>mean(speed)</code>. In fact, the two most used
statistics <code>mean(...)</code> and <code>sd(...)</code> both use
sums. Take a moment to review the formulas for <a
href="NumericalSummaries.html#mean">mean</a> and <a
href="NumericalSummaries.html#standard-deviation">standard
deviation</a>. It is strongly recommended that you study the Explanation
tab for both as well. We’ll wait. See you back here shortly.</p>
<p>…</p>
<p>Welcome back.</p>
<p>Suppose we let <code>X = speed</code> and <code>Y = dist</code>. Then
<span class="math inline">\(X_3 = 7\)</span> and <span
class="math inline">\(Y_3 = 4\)</span> because we are accessing row 3 of
both the <span class="math inline">\(X\)</span> (or speed) column and
<span class="math inline">\(Y\)</span> (or dist) column. (Remember from
the above discussion that for individual #3, the speed was 7 and the
dist was 4.) Further, <code>sum(speed)</code> would be written
mathematically as <span class="math inline">\(\sum_{i=1}^6 X_i\)</span>
and <code>sum(dist)</code> would be written as <span
class="math inline">\(\sum_{i=1}^6 Y_i\)</span>.</p>
</div>
</div>
<table>
<colgroup>
<col width="32%" />
<col width="36%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Sum of Squared Errors</strong></th>
<th><strong>Sum of Squares Regression</strong></th>
<th><strong>Total Sum of Squares</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\text{SSE} = \sum_{i=1}^n \left(Y_i -
\hat{Y}_i\right)^2\)</span></td>
<td><span class="math inline">\(\text{SSR} = \sum_{i=1}^n
\left(\hat{Y}_i - \bar{Y}\right)^2\)</span></td>
<td><span class="math inline">\(\text{SSTO} = \sum_{i=1}^n \left(Y_i -
\bar{Y}\right)^2\)</span></td>
</tr>
<tr class="even">
<td>Measures how much the residuals deviate from the line.</td>
<td>Measures how much the regression line deviates from the average
y-value.</td>
<td>Measures how much the y-values deviate from the average
y-value.</td>
</tr>
<tr class="odd">
<td>Equals SSTO - SSR</td>
<td>Equals SSTO - SSE</td>
<td>Equals SSE + SSR</td>
</tr>
<tr class="even">
<td><code>sum( (Y - mylm$fit)^2 )</code></td>
<td><code>sum( (mylm$fit - mean(Y))^2 )</code></td>
<td><code>sum( (Y - mean(Y))^2 )</code></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<hr style="border-color:#d5d5d5; border-style:solid;"/>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>It is important to remember that SSE and SSR split up SSTO, so that
<span class="math display">\[
  \text{SSTO} = \text{SSE} + \text{SSR}
\]</span> This implies that if SSE is large (close to SSTO) then SSR is
small (close to zero) and visa versa. The following three graphics
demonstrate how this works.</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>The above graphs reveal that the idea of correlation is tightly
linked with sums of squares. In fact, the correlation squared is equal
to SSR/SSTO. And this fraction, SSR/SSTO is called <span
class="math inline">\(R^2\)</span> (“r-squared”).</p>
<p><strong>R-Squared (<span class="math inline">\(R^2\)</span>)</strong>
<span class="math display">\[
  \underbrace{R^2 = \frac{SSR}{SSTO} = 1 -
\frac{SSE}{SSTO}}_\text{Interpretation: Proportion of variation in Y
explained by the regression.}
\]</span></p>
<p>The smallest <span class="math inline">\(R^2\)</span> can be is zero,
and the largest it can be is 1. This is because <span
class="math inline">\(SSR\)</span> must be between 0 and SSTO,
inclusive.</p>
</div>
<p><br /></p>
</div>
<div id="residual-plots-regression-assumptions-expand"
class="section level4">
<h4>Residual Plots &amp; Regression Assumptions
<a href="javascript:showhide('assumptions1')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">Residuals vs. fitted-values, Q-Q Plot of
the residuals, and residuals vs. order plots…</span></p>
<div id="assumptions1" style="display:none;">
<p>There are five assumptions that should be met for the mathematical
model of simple linear regression to be appropriate.</p>
<div style="padding-left:60px;color:darkgray;font-size:.8em;">
<p>Each assumption is labeled in the regression equation below.</p>
</div>
<ol style="list-style-type: decimal">
<li>The regression relation between <span
class="math inline">\(Y\)</span> and <span
class="math inline">\(X\)</span> is linear.</li>
<li>The error terms are normally distributed with <span
class="math inline">\(E\{\epsilon_i\}=0\)</span>.</li>
<li>The variance of the error terms is constant over all <span
class="math inline">\(X\)</span> values.</li>
<li>The <span class="math inline">\(X\)</span> values can be considered
fixed and measured without error.</li>
<li>The error terms are independent.</li>
</ol>
<p><span style="color:darkgray;">Regression Equation</span> <span
class="math display">\[
  Y_i = \underbrace{\beta_0 + \beta_1
\overbrace{X_i}^\text{#4}}_{\text{#1}} + \epsilon_i \quad \text{where} \
\overbrace{\epsilon_i \sim}^\text{#5} \overbrace{N(0}^\text{#2},
\overbrace{\sigma^2}^\text{#3})
\]</span></p>
<p>Residuals are used to diagnose departures from the regression
assumptions.</p>
<p><a href="javascript:showhide('moreassumptionsdetail')" style="font-size:.8em;color:skyblue;">(Read
more…)</a></p>
<div id="moreassumptionsdetail" style="display:none;">
<p>As shown above, the regression equation makes several claims, or
assumptions, about the error terms <span
class="math inline">\(\epsilon_i\)</span>, specifically 2, 3, and 5 of
the regression assumptions are hidden inside the statement <span
class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> as shown
here <span class="math display">\[
  \epsilon_i \underbrace{\sim}_{\substack{\text{Independent} \\
\text{Errors}}} \overbrace{N}^{\substack{\text{Normally} \\
\text{distributed}}}(\underbrace{0}_{\substack{\text{mean of} \\
\text{zero}}}, \underbrace{\sigma^2}_{\substack{\text{Constant} \\
\text{Variance}}})
\]</span></p>
<p>While the actual error terms (<span
class="math inline">\(\epsilon_i\)</span>) are unknown in real life, the
residuals (<span class="math inline">\(r_i\)</span>) are known. Thus, we
can use the residuals to check if the assumptions of the regression
appear to be satisfied or not.</p>
</div>
<p><br /></p>
<div style="padding-left:15px;">
<div id="residuals-versus-fitted-values-plot-checks-assumptions-1-and-3"
class="section level5">
<h5>Residuals versus Fitted-values Plot: Checks Assumptions #1 and
#3</h5>
<table width="90%">
<tr>
<td with="15%">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-21-1.png" width="144" /></p>
</td>
<td width="75%">
<p>The linear relationship and constant variance assumptions can be
diagnosed using a residuals versus fitted-values plot. The fitted values
are the <span class="math inline">\(\hat{Y}_i\)</span>. The residuals
are the <span class="math inline">\(r_i\)</span>. This plot compares the
residual to the magnitude of the fitted-value. No discernable pattern in
this plot is desirable.</p>
<p>|
<a href="javascript:showhide('residualsvsfittedvalues')" style="font-size:.8em;color:steelblue2;">Show
Examples</a> |</p>
</td>
</tr>
</table>
<div id="residualsvsfittedvalues" style="display:none;">
<p><a href="javascript:showhide('residualsvsfittedvaluesread')" style="font-size:.8em;color:skyblue;">(Read
more…)</a></p>
<div id="residualsvsfittedvaluesread" style="display:none;">
<p>The residuals versus fitted values plot checks for departures from
the linear relation assumption and the constant variance assumption.</p>
<ul>
<li><p>The linear relation is assumed to be satisfied if there are no
apparent trends in the plot.</p></li>
<li><p>The constant variance assumption is assumed to be satisfied if
the vertical spread of the residuals remains roughly consistent across
all fitted values.</p></li>
</ul>
<p>The left column of plots below show scenarios that would be
considered not linear. The right column of plots show scenarios that
would be considered linear, but lacking constant variance. The middle
column of plots shows scenarios that would satisfy both assumptions,
linear and constant variance.</p>
</div>
<pre class="r"><code>set.seed(2)
X &lt;- rnorm(30,15,3)
notLin &lt;- data.frame(X = X, Y = 500-X^2+rnorm(30,1,8))
notLin.lm &lt;- lm(Y~X, data=notLin)
set.seed(15)
Lin &lt;- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3))
Lin.lm &lt;- lm(Y~X, data=Lin)
par(mfrow=c(3,3),  mai=c(.25,.25,.25,.25), mgp=c(1,.75,0))
  plot(notLin.lm$fitted.values,notLin.lm$residuals, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Not Linear&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  mycurve &lt;- lowess(notLin.lm$fitted.values,notLin.lm$residuals)
  mycurveOrder &lt;- order(mycurve$x)
  mycurve$x &lt;- mycurve$x[mycurveOrder]
  mycurve$y &lt;- mycurve$y[mycurveOrder]
  polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+10, rev(mycurve$y-10)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0)
  plot(Lin.lm$fitted.values,Lin.lm$residuals, pch=20, 
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Good: Linear, Constant Variance&quot;, 
       cex.main=0.95, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  abline(h=0)

  set.seed(6)
notCon &lt;- data.frame(X = X, Y = 5+1.8*X + rnorm(30,2,X^1.5))
notCon.lm &lt;- lm(Y~X, data=notCon)
LinO &lt;- data.frame(X=X, Y = 5+1.8*X+rnorm(30,2,1.3))
LinO[1] &lt;- LinO[1]^2
LinO.lm &lt;- lm(Y~X, data=LinO)
  plot(notCon.lm$fitted.values,notCon.lm$residuals, pch=20, xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, main=&quot;Unconstant Variance&quot;, cex.main=0.95, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  polygon(c(rep(min(notCon.lm$fit),2), rep(max(notCon.lm$fit), 2)), c(-30,30,1.2*max(notCon.lm$res),1.2*min(notCon.lm$res)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0)
#  plot(LinO.lm$fitted.values,LinO.lm$residuals, pch=20, xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, main=&quot;Outliers&quot;, cex.main=0.95)
#  abline(h=0)

  
  tmp &lt;- lm(height ~ age, data=Loblolly)
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  mycurve &lt;- lowess(tmp$fitted.values,tmp$residuals)
  mycurveOrder &lt;- order(mycurve$x)
  mycurve$x &lt;- mycurve$x[mycurveOrder]
  mycurve$y &lt;- mycurve$y[mycurveOrder]
  polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3, rev(mycurve$y-1)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0)
  
  tmp &lt;- lm(Girth ~ Volume, data=trees[-31,])
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  abline(h=0)

  tmp &lt;- lm(Height ~ Volume, data=trees)
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  polygon(c(rep(min(tmp$fit), 2), max(tmp$fit)), c(1.3*max(tmp$res),1.2*min(tmp$res),0), col=rgb(.8,.8,.8,.2), border=NA) 
  abline(h=0)
  
  
  
  
  tmp &lt;- lm(mpg ~ disp, data=mtcars)
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  mycurve &lt;- lowess(tmp$fitted.values,tmp$residuals, f=.4)
  mycurveOrder &lt;- order(mycurve$x)
  mycurve$x &lt;- mycurve$x[mycurveOrder]
  mycurve$y &lt;- mycurve$y[mycurveOrder]
  polygon(c(mycurve$x,rev(mycurve$x)), c(mycurve$y+3.5, rev(mycurve$y-2)), col=rgb(.7,.7,.7,.2), border=NA) 
  abline(h=0) 
  
  
  tmp &lt;- lm(weight ~ repwt, data=Davis[-12,])
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  abline(h=0) 

  tmp &lt;- lm(weight ~ repht, data=Davis[-12,])
  plot(tmp$residuals ~ tmp$fitted.values, pch=20,
       xlab=&quot;Fitted Values&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  polygon(c(min(tmp$fit),rep(max(tmp$fit), 2)), c(2,max(tmp$res),1.6*min(tmp$res)), col=rgb(.85,.85,.85,.2), border=NA) 
  abline(h=0) </code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
<p><br /></p>
</div>
<div id="q-q-plot-of-the-residuals-checks-assumption-2"
class="section level5">
<h5>Q-Q Plot of the Residuals: Checks Assumption #2</h5>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-23-1.png" width="144" />
</td>
<td width="75%">
<p>The normality of the error terms can be assessed by considering a
normal probability plot (Q-Q Plot) of the residuals. If the residuals
appear to be normal, then the error terms are also considered to be
normal. If the residuals do not appear to be normal, then the error
terms are also assumed to violate the normality assumption.</p>
<p>|
<a href="javascript:showhide('qqplots')" style="font-size:.8em;color:steelblue2;">Show
Examples</a> |</p>
</td>
</tr>
</table>
<div id="qqplots" style="display:none;">
<p><a href="javascript:showhide('qqplotsread')" style="font-size:.8em;color:skyblue;">(Read
more…)</a></p>
<div id="qqplotsread" style="display:none;">
<p>There are four main trends that occur in a normal probability plot.
Examples of each are plotted below with a histogram of the data next to
the normal probability plot.</p>
<p>Often the plot is called a Q-Q Plot, which stands for
quantile-quantile plot. The idea is to compare the observed distribution
of data to what the distribution should look like in theory if it was
normal. Q-Q Plots are more general than normal probability plots because
they can be used with any theoretical distribution, not just the normal
distribution.</p>
</div>
<pre class="r"><code>par(mfrow=c(2,2),  mai=c(.5,.5,.25,.25), mgp=c(1,.75,0))

set.seed(123)

  tmp &lt;- rnorm(100)
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Normal&quot;, col=&quot;skyblue&quot;)
  
  tmp &lt;- Davis$weight
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Right-skewed&quot;,
       breaks=15, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(2,2),  mai=c(.5,.5,.25,.25), mgp=c(1,.75,0))

  tmp &lt;- rbeta(100, 5,1)
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Left-skewed&quot;,
       breaks=seq(min(tmp),max(tmp), length.out=13), col=&quot;firebrick&quot;)
  
  tmp &lt;- rbeta(100,2,2)
  qqnorm(tmp, pch=20, ylab=&quot;Observed&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)
  qqline(tmp)
  hist(tmp, xlab=&quot;&quot;, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, main=&quot;Heavy-tailed&quot;, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-24-2.png" width="672" /></p>
</div>
<p><br /></p>
</div>
<div id="residuals-versus-order-plot-checks-assumption-5"
class="section level5">
<h5>Residuals versus Order Plot: Checks Assumption #5</h5>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-25-1.png" width="144" />
</td>
<td width="75%">
<p>When the data is collected in a specific order, or has some other
important ordering to it, then the independence of the error terms can
be assessed. This is typically done by plotting the residuals against
their order of occurrance. If any dramatic trends are visible in the
plot, then the independence assumption is violated.</p>
<p>|
<a href="javascript:showhide('resorderplots')" style="font-size:.8em;color:steelblue2;">Show
Examples</a> |</p>
</td>
</tr>
</table>
<div id="resorderplots" style="display:none;">
<p><a href="javascript:showhide('resorderplotsread')" style="font-size:.8em;color:skyblue;">(Read
more…)</a></p>
<div id="resorderplotsread" style="display:none;">
<p>Plotting the residuals against the order in which the data was
collected provides insight as to whether or not the observations can be
considered independent. If the plot shows no trend, then the error terms
are considered independent and the regression assumption satisfied. If
there is a visible trend in the plot, then the regression assumption is
likely violated.</p>
</div>
<pre class="r"><code>par(mfrow=c(2,2),  mai=c(.5,.5,.25,.25), mgp=c(1,.75,0))

  tmp &lt;- lm(mpg ~ disp, data=mtcars)
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Good: No Trend&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;skyblue&quot;)

  tmp &lt;- lm(height ~ age, data=Loblolly)
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Questionable: General Trend&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;orangered&quot;)

  tmp &lt;- lm(hp ~ qsec, data=mtcars)
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Questionable: Interesting Patterns&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;orangered&quot;)
  
  tmp &lt;- lm(hp ~ drat, data=mtcars[order(mtcars$cyl),])
  plot(tmp$residuals, pch=20,
       xlab=&quot;Order&quot;, ylab=&quot;Residuals&quot;, 
       main=&quot;Bad: Obvious Trend&quot;, cex.main=0.95,
       xaxt=&#39;n&#39;, yaxt=&#39;n&#39;, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
</div>
</div>
<p><br /></p>
<div id="problems-from-failed-assumptions" class="section level5">
<h5>Problems from Failed Assumptions</h5>
<p>There are various problems that can arise when certain of the
regression assupmtions are not satisfied.</p>
<p><strong>Lack of Linearity</strong></p>
<p>When the linearity assumption is violated, pretty much everything we
obtain from the regression summary is no longer meaningful.</p>
<ul>
<li><p>The y-intercept estimate can be drastically off from its actual
true value.</p></li>
<li><p>Important model information is lost by trying to use a simple
slope term <span class="math inline">\(\beta_1\)</span> to describe the
model with respect to <span class="math inline">\(X\)</span>.</p></li>
<li><p>The residual standard error will be much higher than it otherwise
would be because of curvature patterns in the data that the line cannot
capture. Thus, R-squared will be lower than it otherwise should
be.</p></li>
<li><p>P-values can become non-significant, when in fact there is a
strong pattern in the data, but that pattern just cannot be captured by
a simple line.</p></li>
</ul>
<p>*Normality of the errors is often put into question as well when a
simplified line is used to try to capture a more complicated curved
model.</p>
<p>The plot below demonstrate these difficulties.</p>
<pre class="r"><code># Create Data from a True Model

n &lt;- 30                           #sample size

beta_0 &lt;- 14.2                    #True y-intercept

beta_1 &lt;- 7.5                     #True slope

beta_2 &lt;- -0.25                   #True bend

X_i &lt;- runif(n, 0, 20)            #Sample of X-values

sigma &lt;- 2.5                      #True standard deviation

epsilon_i &lt;- rnorm(n, 0, sigma)   #normally distributed errors

Y_i &lt;- beta_0 + beta_1*X_i + beta_2*X_i^2 + epsilon_i 
                                  #Sample of Y-values from model


# Plot the Data and Fitted Model

mylm &lt;- lm(Y_i ~ X_i)            #Fit Model to Data


layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), 
   widths=c(2,2,2), heights=c(4,2,2))
                                 #create plot panel


plot(Y_i ~ X_i,                  #Plot the data
     pch=16, 
     col=&quot;darkgray&quot;, 
     xlim=c(0,20), 
     ylim=c(0,100),
     main=&quot;Non-Linear Relation&quot;)

abline(mylm, col=&quot;gray&quot;)         #Add fitted line to plot

curve(beta_0 + beta_1*x + beta_2*x^2, col=&quot;gray&quot;, lty=2, add=TRUE) 
                                 #Add True line to plot
      

                                 #Add summary to plot
legend(&quot;topleft&quot;, legend=c(paste(&quot;Y-Intercept:&quot;, round(mylm$coef[[1]], 3), &quot;  (True value:&quot;, beta_0, &quot;)&quot;),
                           paste(&quot;Slope:&quot;, round(mylm$coef[[2]], 3), &quot;  (True value:&quot;, beta_1, &quot;)&quot;),
                           paste(&quot;Sigma:&quot;, round(summary(mylm)$sigma, 3), &quot;  (True value:&quot;, sigma, &quot;)&quot;)), bty=&#39;n&#39;)

                                 #Draw diagnostic plots
plot(mylm, which=1:2)
plot(mylm$residuals, ylab=&quot;Residuals&quot;)
mtext(&quot;Residuals vs Order&quot;, side=3)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p><br/></p>
<p><strong>Non-normal Error Terms</strong></p>
<p>When the normality assumption for the error terms is violated, not
all is lost. In fact, the estimate of the slope and intercept are still
often fairly meaningful. However, it is unwise to put too much trust in
the residual standard error as an estimate of the standard deviation
<span class="math inline">\(\sigma\)</span>. This is because the
standard deviation in skewed distributions does not carry the same
meaning it has in normal distributions.</p>
<pre class="r"><code># Create Data from a True Model

n &lt;- 30                           #sample size

beta_0 &lt;- 14.2                    #True y-intercept

beta_1 &lt;- 3.5                     #True slope

X_i &lt;- runif(n, 0, 20)            #Sample of X-values

sigma &lt;- 2.5                      #True standard deviation

epsilon_i &lt;- rchisq(n, 1)*3 - 1 #non-normally distributed errors

Y_i &lt;- beta_0 + beta_1*X_i + epsilon_i 
                                  #Sample of Y-values from model


# Plot the Data and Fitted Model

mylm &lt;- lm(Y_i ~ X_i)            #Fit Model to Data


layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), 
   widths=c(2,2,2), heights=c(4,2,2))
                                 #create plot panel


plot(Y_i ~ X_i,                  #Plot the data
     pch=16, 
     col=&quot;darkgray&quot;, 
     xlim=c(0,20), 
     ylim=c(0,100),
     main=&quot;Normality Assumption Violated&quot;)

abline(mylm, col=&quot;gray&quot;)         #Add fitted line to plot

abline(beta_0, beta_1,           #Add True line to plot
       col=&quot;gray&quot;, lty=2)

                                 #Add summary to plot
legend(&quot;topleft&quot;, legend=c(paste(&quot;Y-Intercept:&quot;, round(mylm$coef[[1]], 3), &quot;  (&quot;, beta_0, &quot;)&quot;),
                           paste(&quot;Slope:&quot;, round(mylm$coef[[2]], 3), &quot;  (&quot;, beta_1, &quot;)&quot;),
                           paste(&quot;Sigma:&quot;, round(summary(mylm)$sigma, 3), &quot;  (&quot;, sigma, &quot;)&quot;)), bty=&#39;n&#39;)

                                 #Draw diagnostic plots
plot(mylm, which=1:2)
plot(mylm$residuals, ylab=&quot;Residuals&quot;)
mtext(&quot;Residuals vs Order&quot;, side=3)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p><strong>Unconstant Variance</strong></p>
<p>When variance of the error term changes across the regression, the
regression approximates the “average variance” of the errors because the
regression is still assuming the variance is constant across the
regression. The estimates of the slope and intercept are still typically
quite good, and can be used for interpretation. The residual standard
error however should not be considered to be meaningful as it will be
too large on one end of the regression and too small on the other
end.</p>
<pre class="r"><code># Create Data from a True Model

n &lt;- 30                           #sample size

beta_0 &lt;- 14.2                    #True y-intercept

beta_1 &lt;- 3.5                     #True slope

X_i &lt;- runif(n, 0, 20)            #Sample of X-values

sigma &lt;- 2.5                      #True standard deviation

epsilon_i &lt;- rnorm(n, 0, sigma + X_i)   
                                  #normally distributed errors
                                  #with increasing variance

Y_i &lt;- beta_0 + beta_1*X_i + epsilon_i 
                                  #Sample of Y-values from model


# Plot the Data and Fitted Model

mylm &lt;- lm(Y_i ~ X_i)            #Fit Model to Data


layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), 
   widths=c(2,2,2), heights=c(4,2,2))
                                 #create plot panel


plot(Y_i ~ X_i,                  #Plot the data
     pch=16, 
     col=&quot;darkgray&quot;, 
     xlim=c(0,20), 
     ylim=c(0,100),
     main=&quot;Variance Varies (Non-Constant)&quot;)

abline(mylm, col=&quot;gray&quot;)         #Add fitted line to plot

abline(beta_0, beta_1,           #Add True line to plot
       col=&quot;gray&quot;, lty=2)

                                 #Summarize the Model Fit
pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]),
      Slope = c(True = beta_1, Estimated = mylm$coef[[2]]),
      Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))</code></pre>
<table style="width:51%;">
<colgroup>
<col width="25%" />
<col width="9%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">True</th>
<th align="center">Estimated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Y-Intercept</strong></td>
<td align="center">14.2</td>
<td align="center">13.82</td>
</tr>
<tr class="even">
<td align="center"><strong>Slope</strong></td>
<td align="center">3.5</td>
<td align="center">3.768</td>
</tr>
<tr class="odd">
<td align="center"><strong>Sigma</strong></td>
<td align="center">2.5</td>
<td align="center">13.02</td>
</tr>
</tbody>
</table>
<pre class="r"><code>                                 #Add summary to plot
legend(&quot;topleft&quot;, legend=c(paste(&quot;Y-Intercept:&quot;, round(mylm$coef[[1]], 3), &quot;  (&quot;, beta_0, &quot;)&quot;),
                           paste(&quot;Slope:&quot;, round(mylm$coef[[2]], 3), &quot;  (&quot;, beta_1, &quot;)&quot;),
                           paste(&quot;Sigma:&quot;, round(summary(mylm)$sigma, 3), &quot;  (&quot;, round(mean(sigma + X_i), 2), &quot;, mean)&quot;)), bty=&#39;n&#39;)

                                 #Draw diagnostic plots
plot(mylm, which=1:2)
plot(mylm$residuals, ylab=&quot;Residuals&quot;)
mtext(&quot;Residuals vs Order&quot;, side=3)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p><strong>Normality Violated</strong></p>
<p>As silly as it sounds, if the only problem with the regression is the
lack of normality of the error terms, it isn’t all that big of a
problem. Depending on how non-normal the residuals appear, there could
be some skewing to the residual standard error, but otherwise, the slope
and intercept are still interpretable and meaningful.</p>
<pre class="r"><code># Create Data from a True Model

n &lt;- 30                           #sample size

beta_0 &lt;- 14.2                    #True y-intercept

beta_1 &lt;- 3.5                     #True slope

X_i &lt;- runif(n, 0, 20)            #Sample of X-values

sigma &lt;- 2.5                      #True standard deviation

epsilon_i &lt;- runif(n, -sqrt(12*sigma^2)/2, sqrt(12*sigma^2)/2) #non-normally distributed errors

Y_i &lt;- beta_0 + beta_1*X_i + epsilon_i 
                                  #Sample of Y-values from model


# Plot the Data and Fitted Model

mylm &lt;- lm(Y_i ~ X_i)            #Fit Model to Data


layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), 
   widths=c(2,2,2), heights=c(4,2,2))
                                 #create plot panel


plot(Y_i ~ X_i,                  #Plot the data
     pch=16, 
     col=&quot;darkgray&quot;, 
     xlim=c(0,20), 
     ylim=c(0,100),
     main=&quot;Normality Assumption Violated&quot;)

abline(mylm, col=&quot;gray&quot;)         #Add fitted line to plot

abline(beta_0, beta_1,           #Add True line to plot
       col=&quot;gray&quot;, lty=2)

                                 #Summarize the Model Fit
pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]),
      Slope = c(True = beta_1, Estimated = mylm$coef[[2]]),
      Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))</code></pre>
<table style="width:51%;">
<colgroup>
<col width="25%" />
<col width="9%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">True</th>
<th align="center">Estimated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Y-Intercept</strong></td>
<td align="center">14.2</td>
<td align="center">14.7</td>
</tr>
<tr class="even">
<td align="center"><strong>Slope</strong></td>
<td align="center">3.5</td>
<td align="center">3.449</td>
</tr>
<tr class="odd">
<td align="center"><strong>Sigma</strong></td>
<td align="center">2.5</td>
<td align="center">2.317</td>
</tr>
</tbody>
</table>
<pre class="r"><code>                                 #Add summary to plot
legend(&quot;topleft&quot;, legend=c(paste(&quot;Y-Intercept:&quot;, round(mylm$coef[[1]], 3), &quot;  (&quot;, beta_0, &quot;)&quot;),
                           paste(&quot;Slope:&quot;, round(mylm$coef[[2]], 3), &quot;  (&quot;, beta_1, &quot;)&quot;),
                           paste(&quot;Sigma:&quot;, round(summary(mylm)$sigma, 3), &quot;  (&quot;, sigma, &quot;)&quot;)), bty=&#39;n&#39;)

                                 #Draw diagnostic plots
plot(mylm, which=1:2)
plot(mylm$residuals, ylab=&quot;Residuals&quot;)
mtext(&quot;Residuals vs Order&quot;, side=3)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p><strong>Independence Assumption Violated</strong></p>
<p>While the slope and intercept are often still meaningful when the
independence assumption is violated, the residual standard error is
unnecessarily large in this case.</p>
<pre class="r"><code># Create Data from a True Model

n &lt;- 30                           #sample size

beta_0 &lt;- 14.2                    #True y-intercept

beta_1 &lt;- 3.5                     #True slope

X_i &lt;- runif(n, 0, 20)            #Sample of X-values

sigma &lt;- 2.5                      #True standard deviation

epsilon_i &lt;- rnorm(n, 0, 2.5) + (1:n -n/2)*.5
                                #normal, but correlated errors

Y_i &lt;- beta_0 + beta_1*X_i + epsilon_i 
                                  #Sample of Y-values from model


# Plot the Data and Fitted Model

mylm &lt;- lm(Y_i ~ X_i)            #Fit Model to Data


layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), 
   widths=c(2,2,2), heights=c(4,2,2))
                                 #create plot panel


plot(Y_i ~ X_i,                  #Plot the data
     pch=16, 
     col=&quot;darkgray&quot;, 
     xlim=c(0,20), 
     ylim=c(0,100),
     main=&quot;Independence Assumption Violated&quot;)

abline(mylm, col=&quot;gray&quot;)         #Add fitted line to plot

abline(beta_0, beta_1,           #Add True line to plot
       col=&quot;gray&quot;, lty=2)

                                 #Summarize the Model Fit
pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]),
      Slope = c(True = beta_1, Estimated = mylm$coef[[2]]),
      Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))</code></pre>
<table style="width:51%;">
<colgroup>
<col width="25%" />
<col width="9%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">True</th>
<th align="center">Estimated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Y-Intercept</strong></td>
<td align="center">14.2</td>
<td align="center">16.47</td>
</tr>
<tr class="even">
<td align="center"><strong>Slope</strong></td>
<td align="center">3.5</td>
<td align="center">3.296</td>
</tr>
<tr class="odd">
<td align="center"><strong>Sigma</strong></td>
<td align="center">2.5</td>
<td align="center">4.819</td>
</tr>
</tbody>
</table>
<pre class="r"><code>                                 #Add summary to plot
legend(&quot;topleft&quot;, legend=c(paste(&quot;Y-Intercept:&quot;, round(mylm$coef[[1]], 3), &quot;  (&quot;, beta_0, &quot;)&quot;),
                           paste(&quot;Slope:&quot;, round(mylm$coef[[2]], 3), &quot;  (&quot;, beta_1, &quot;)&quot;),
                           paste(&quot;Sigma:&quot;, round(summary(mylm)$sigma, 3), &quot;  (&quot;, sigma, &quot;)&quot;)), bty=&#39;n&#39;)

                                 #Draw diagnostic plots
plot(mylm, which=1:2)
plot(mylm$residuals, ylab=&quot;Residuals&quot;)
mtext(&quot;Residuals vs Order&quot;, side=3)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p><strong>Outliers Present</strong></p>
<p>While outliers do not violate any of the regression assumptions, they
do pose substantial difficulties for the least squares regression
estimates of the slope and intercept.</p>
<pre class="r"><code># Create Data from a True Model

n &lt;- 30                           #sample size

beta_0 &lt;- 14.2                    #True y-intercept

beta_1 &lt;- 3.5                     #True slope

X_i &lt;- runif(n, 0, 20)            #Sample of X-values

sigma &lt;- 2.5                      #True standard deviation

epsilon_i &lt;- rnorm(n, 0, sigma)   #normally distributed errors


epsilon_i[3] &lt;- ifelse(X_i[3] &lt; 10, runif(1,25,35), -runif(1,25,35))
                                  #create outlier

Y_i &lt;- beta_0 + beta_1*X_i + epsilon_i 
                                  #Sample of Y-values from model



# Plot the Data and Fitted Model

mylm &lt;- lm(Y_i ~ X_i)            #Fit Model to Data


layout(matrix(c(1,1,1,2,3,4), 2, 3, byrow = TRUE), 
   widths=c(2,2,2), heights=c(4,2,2))
                                 #create plot panel


plot(Y_i ~ X_i,                  #Plot the data
     pch=16, 
     col=&quot;darkgray&quot;, 
     xlim=c(0,20), 
     ylim=c(0,100),
     main=&quot;An Outlier Present&quot;)

abline(mylm, col=&quot;gray&quot;)         #Add fitted line to plot

abline(beta_0, beta_1,           #Add True line to plot
       col=&quot;gray&quot;, lty=2)

                                 #Summarize the Model Fit
pander(rbind(`Y-Intercept` = c(True = beta_0, Estimated = mylm$coef[[1]]),
      Slope = c(True = beta_1, Estimated = mylm$coef[[2]]),
      Sigma = c(True = sigma, Estimated = summary(mylm)$sigma)))</code></pre>
<table style="width:51%;">
<colgroup>
<col width="25%" />
<col width="9%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">True</th>
<th align="center">Estimated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Y-Intercept</strong></td>
<td align="center">14.2</td>
<td align="center">13.18</td>
</tr>
<tr class="even">
<td align="center"><strong>Slope</strong></td>
<td align="center">3.5</td>
<td align="center">3.447</td>
</tr>
<tr class="odd">
<td align="center"><strong>Sigma</strong></td>
<td align="center">2.5</td>
<td align="center">6.389</td>
</tr>
</tbody>
</table>
<pre class="r"><code>                                 #Add summary to plot
legend(&quot;topleft&quot;, legend=c(paste(&quot;Y-Intercept:&quot;, round(mylm$coef[[1]], 3), &quot;  (&quot;, beta_0, &quot;)&quot;),
                           paste(&quot;Slope:&quot;, round(mylm$coef[[2]], 3), &quot;  (&quot;, beta_1, &quot;)&quot;),
                           paste(&quot;Sigma:&quot;, round(summary(mylm)$sigma, 3), &quot;  (&quot;, sigma, &quot;)&quot;)), bty=&#39;n&#39;)

                                 #Draw diagnostic plots
plot(mylm, which=1:2)
plot(mylm$residuals, ylab=&quot;Residuals&quot;)
mtext(&quot;Residuals vs Order&quot;, side=3)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
</div>
</div>
<p><br /></p>
<hr />
<p><br/></p>
<p><em>The material below this section is meant for Math 425 students
only.</em></p>
<p><br/></p>
</div>
<div id="estimating-the-model-parameters-expand" class="section level4">
<h4>Estimating the Model Parameters
<a href="javascript:showhide('estimatingparameters')" style="font-size:.6em;color:skyblue;" id="estMod">(Expand)</a></h4>
<p><span class="expand-caption">How to get <span
class="math inline">\(b_0\)</span> and <span
class="math inline">\(b_1\)</span>: least squares &amp; maximum
likelihood…</span></p>
<div id="estimatingparameters" style="display:none;">
<p>There are two approaches to estimating the parameters <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> in the regression model. The
oldest and most tradiational approach is using the idea of least
squares. A more general approach uses the idea of maximum likelihood
(see below). Fortunately, for simple linear regression, the estimates
for <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> obtained from either method are
identical. The estimates for the true parameter values <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> are typically denoted by <span
class="math inline">\(b_0\)</span> and <span
class="math inline">\(b_1\)</span>, respectively, and are given by the
following formulas.</p>
<table>
<colgroup>
<col width="40%" />
<col width="44%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter Estimate</th>
<th>Mathematical Formula</th>
<th>R Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Slope</td>
<td><span class="math inline">\(b_1 = \frac{\sum
X_i(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}\)</span></td>
<td><code>b_1 &lt;- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 )</code></td>
</tr>
<tr class="even">
<td>Intercept</td>
<td><span class="math inline">\(b_0 = \bar{Y} - b_1\bar{X}\)</span></td>
<td><code>b_0 &lt;- mean(Y) - b_1*mean(X)</code></td>
</tr>
</tbody>
</table>
<p>It is important to note that these estimates are entirely determined
from the observed data <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>. When the regression equation is
written using the estimates instead of the parameters, we use the
notation <span class="math inline">\(\hat{Y}\)</span>, which is the
estimator of <span class="math inline">\(E\{Y\}\)</span>. Thus, we write
<span class="math display">\[\begin{equation}
  \hat{Y}_i = b_0 + b_1 X_i
\end{equation}\]</span> which is directly comparable to the true, but
unknown values <span class="math display">\[\begin{equation}
  E\{Y_i\} = \beta_0 + \beta_1 X_i.
  \label{exp}
\end{equation}\]</span></p>
<div id="leastSquares" class="section level5">
<h5>Least Squares</h5>
<p>To estimate the model parameters <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> using least squares, we start by
defining the function <span class="math inline">\(Q\)</span> as the sum
of the squared errors, <span class="math inline">\(\epsilon_i\)</span>.
<span class="math display">\[
  Q = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (Y_i - (\beta_0 + \beta_1
X_i))^2
\]</span> Then we use the function Q as if it were a function of <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. Ironically, the values of <span
class="math inline">\(Y\)</span> and <span
class="math inline">\(X\)</span> are considered fixed. However, this
makes sense because once a particular data set has been observed, these
values are all known for that data set. What we don’t know are the
values of <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>.</p>
<p>This <a
href="https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html">least
squares applet</a> is a good way to explore how various choices of the
slope and intercept yield different values of the “sum of squared
residuals”. But it turns out that there is one “best” choice of the
slope and intercept that yields a “smallest” value of the “sum of
squared residuals.” This best choice can actually be found using
calculus by taking the partial derivatives of <span
class="math inline">\(Q\)</span> with respect to both <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. <span class="math display">\[
  \frac{\partial Q}{\partial \beta_0} = -2\sum (Y_i - \beta_0 -
\beta_1X_i)
\]</span> <span class="math display">\[
  \frac{\partial Q}{\partial \beta_1} = -2\sum
X_i(Y_i-\beta_0-\beta_1X_i)
\]</span> Setting these partial derivatives to zero, and solving the
resulting system of equations provides the values of the parameters
which minimize <span class="math inline">\(Q\)</span> for a given set of
data. After all the calculations are completed we find the values of the
parameter estimators <span class="math inline">\(b_0\)</span> and <span
class="math inline">\(b_1\)</span> (of <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>, respectively) are as stated
previously.</p>
</div>
<div id="mle" class="section level5">
<h5>Maximum Likelihood</h5>
<p>The idea of maximum likelihood estimation is opposite that of least
squares. Instead of choosing those values of <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> which minime the least squares
<span class="math inline">\(Q\)</span> function, we choose the values of
<span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> which maximize the likelihood
function. The likelihood function is created by first determining the
joint distribution of the <span class="math inline">\(Y_i\)</span> for
all observations <span class="math inline">\(i=1,\ldots,n\)</span>. We
can do this rather simply by using the assumption that the errors, <span
class="math inline">\(\epsilon_i\)</span> are independently normally
distributed. When events are independent, their joint probability is
simply the product of their individual probabilities. Thus, if <span
class="math inline">\(f(Y_i)\)</span> denotes the probability density
function for <span class="math inline">\(Y_i\)</span>, then the joint
probability density for all <span class="math inline">\(Y_i\)</span>,
<span class="math inline">\(f(Y_1,\ldots,Y_n)\)</span> is given by <span
class="math display">\[
  f(Y_1,\ldots,Y_n) = \prod_{i=1}^n f(Y_i)
\]</span> Since each <span class="math inline">\(Y_i\)</span> is assumed
to be normally distributed with mean <span class="math inline">\(\beta_0
+ \beta_1 X_i\)</span> and variance <span
class="math inline">\(\sigma^2\)</span> (see model (<span
class="math inline">\(\ref{model}\)</span>)) we have that <span
class="math display">\[
  f(Y_i) =
\frac{1}{\sqrt{2\pi}\sigma}\exp{\left[-\frac{1}{2}\left(\frac{Y_i-\beta_0-\beta_1X_i}{\sigma}\right)^2\right]}
\]</span> which provides the joint probability as <span
class="math display">\[
  f(Y_1,\ldots,Y_n) = \prod_{i=1}^n f(Y_i) =
\frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2\right]}
\]</span> The likelihood function <span class="math inline">\(L\)</span>
is then given by consider the <span class="math inline">\(Y_i\)</span>
and <span class="math inline">\(X_i\)</span> fixed and the parameters
<span class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span> and <span
class="math inline">\(\sigma^2\)</span> as the variables in the
function. <span class="math display">\[
  L(\beta_0,\beta_1,\sigma^2) =
\frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2\right]}
\]</span> Instead of taking partial derivatives of <span
class="math inline">\(L\)</span> directly (with respect to all
parameters) we take the partial derivatives of the <span
class="math inline">\(\log\)</span> of <span
class="math inline">\(L\)</span>, which is easier to work with. In a
similar, but more difficult calculation, to that of minimizing <span
class="math inline">\(Q\)</span>, we obtain the values of <span
class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span>, and <span
class="math inline">\(\sigma^2\)</span> which maximize the log of <span
class="math inline">\(L\)</span>, and which therefore maximize <span
class="math inline">\(L\)</span>. (This is not an obvious result, but
can be verified after some intense calculations.) The additional result
that maximimum likelihood estimation provides that the least squares
estimates did not give us is the estimate <span
class="math inline">\(\hat{\sigma}^2\)</span> of <span
class="math inline">\(\sigma^2\)</span>. <span class="math display">\[
  \hat{\sigma}^2 = \frac{\sum(Y_i-\hat{Y}_i)^2}{n}
\]</span></p>
</div>
</div>
<p><br /></p>
</div>
<div id="estimating-the-model-variance-expand" class="section level4">
<h4>Estimating the Model Variance
<a href="javascript:showhide('estimatingvariance')" style="font-size:.6em;color:skyblue;" id="varEst">(Expand)</a></h4>
<p><span class="expand-caption">Estimating <span
class="math inline">\(\sigma^2\)</span> with MSE…</span></p>
<div id="estimatingvariance" style="display:none;">
<p>As shown previously in the “Estimating Model Parameters” section of
this page, we can obtain estimates for the model parameters <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> by using either least squares
estimation or maximum likelihood estimation. Those estimates were given
by the formulas</p>
<p><span class="math display">\[
b_1 = \frac{\sum X_i(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2} \quad
\text{(Unbiased Estimate of $\beta_1$)}
\]</span></p>
<p><span class="math display">\[
b_0 = \bar{Y} - b_1\bar{X} \quad \text{(Unbiased Estimate of $\beta_0$)}
\]</span></p>
<p>It turns out that these estimates for <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> are nice in the sense that on
average they provide the correct estimate of the true parameter, i.e.,
they are unbiased estimators. Unfortunately, this is not the case for
the maximum likelihood estimate <span
class="math inline">\(\widehat{\sigma}^2\)</span> of the model variance
<span class="math inline">\(\sigma^2\)</span>. This estimate turns out
to be a biased estimator. This means that it is consistently wrong in
its estimates of <span class="math inline">\(\sigma^2\)</span>. If we
left the estimator alone, our estimates for <span
class="math inline">\(\sigma^2\)</span> would always be wrong. This is
bad. Fortunately, there is a way to fix it, and this corrected version
of the estimator is what we will actually use in practice to estimate
<span class="math inline">\(\sigma^2\)</span>.</p>
<p>Without going into all the details, to “fix” the biased estimator of
<span class="math inline">\(\sigma^2\)</span> that is given to us
through maximum likelihood estimation, we need to correct its
denominator so that it properly represent the degrees of freedom
associated with the numerator, which it does not currently. To find the
correct degrees of freedom, we have to notice that the <span
class="math inline">\(\hat{Y}_i\)</span> in the numerator of <span
class="math inline">\(\widehat{\sigma}^2\)</span> is defined by <span
class="math display">\[\begin{equation}
  \widehat{Y}_i = b_0 + b_1X_i
  \label{hatY}
\end{equation}\]</span> From this equation, we notice that two means,
<span class="math inline">\(\bar{X}\)</span> and <span
class="math inline">\(\bar{Y}\)</span>, were estimated from the data in
order to obtain <span class="math inline">\(\hat{Y}_i\)</span>. (See the
formulas for <span class="math inline">\(b_0\)</span> and <span
class="math inline">\(b_1\)</span> above, and note how they use both
<span class="math inline">\(\bar{X}\)</span> and <span
class="math inline">\(\bar{Y}\)</span> in their calculation.) Anytime a
mean is estimated from the data we lose a degree of freedom. Hence, the
denominator for <span class="math inline">\(\hat{\sigma}^2\)</span>
should be <span class="math inline">\(n-2\)</span> instead of <span
class="math inline">\(n\)</span>. Some incredibly long calculations will
show that the “fixed” estimator <span
class="math display">\[\begin{equation}
  s^2 = MSE = \frac{\sum(Y_i-\hat{Y}_i)^2}{n-2} \quad \text{(Unbiased
Estimator of $\sigma^2$)}
\end{equation}\]</span> is an unbiased estimator of <span
class="math inline">\(\sigma^2\)</span>. Here <span
class="math inline">\(MSE\)</span> stands for <strong>m</strong>ean
<strong>s</strong>quared <strong>e</strong>rror, which is the most
obvious name for a formula that squares the errors <span
class="math inline">\(Y_i-\hat{Y}_i\)</span> then adds them up and
divides by their degrees of freedom. Similarly, we call the numerator
<span class="math inline">\(\sum(Y_i-\hat{Y}_i)^2\)</span> the sum of
the squared errors, denoted by <span class="math inline">\(SSE\)</span>.
It is also important to note that the errors are often denoted by <span
class="math inline">\(r_i = Y_i-\hat{Y}_i\)</span>, the residuals.
Putting this all together we get the following equivalent statements for
<span class="math inline">\(MSE\)</span>. <span
class="math display">\[\begin{equation}
  s^2 = MSE = \frac{SSE}{n-2} = \frac{\sum(Y_i-\widehat{Y}_i)^2}{n-2} =
\frac{\sum r_i^2}{n-2}
\end{equation}\]</span> As a final note, even though the expected value
<span class="math inline">\(E\{MSE\} = \sigma^2\)</span>, which shows
<span class="math inline">\(MSE\)</span> is an unbiased estimator of
<span class="math inline">\(\sigma^2\)</span>, it unfortunately isn’t
true that <span class="math inline">\(\sqrt{MSE}\)</span> is an unbiased
estimator of <span class="math inline">\(\sigma\)</span>. This presents
a few problems later on, but these are minimal enough that we can
overlook the issue and move forward.</p>
</div>
<p><br /></p>
</div>
<div id="transformations-expand" class="section level4">
<h4>Transformations
<a href="javascript:showhide('transformations')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span
class="math inline">\(Y&#39;\)</span>, <span
class="math inline">\(X&#39;\)</span>, and returning to the original
space…</span></p>
<div id="transformations" style="display:none;">
<p>Y transformations are denoted by y-prime, written <span
class="math inline">\(Y&#39;\)</span>, and consist of raising <span
class="math inline">\(Y\)</span> to some power called <span
class="math inline">\(\lambda\)</span>.</p>
<p><span class="math display">\[
  Y&#39; = Y^\lambda \quad \text{(Y Transformation)}
\]</span></p>
<table>
<colgroup>
<col width="35%" />
<col width="42%" />
<col width="21%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Value of <span
class="math inline">\(\lambda\)</span></th>
<th>Transformation to Use</th>
<th>R Code</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-2</td>
<td><span class="math inline">\(Y&#39; = Y^{-2} = 1/Y^2\)</span></td>
<td><code>lm(Y^-2 ~ X)</code></td>
</tr>
<tr class="even">
<td align="center">-1</td>
<td><span class="math inline">\(Y&#39; = Y^{-1} = 1/Y\)</span></td>
<td><code>lm(Y^-1 ~ X)</code></td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td><span class="math inline">\(Y&#39; = \log(Y)\)</span></td>
<td><code>lm(log(Y) ~ X)</code></td>
</tr>
<tr class="even">
<td align="center">0.25</td>
<td><span class="math inline">\(Y&#39; = \sqrt(\sqrt(Y))\)</span></td>
<td><code>lm(sqrt(sqrt(Y)) ~ X)</code></td>
</tr>
<tr class="odd">
<td align="center">0.5</td>
<td><span class="math inline">\(Y&#39; = \sqrt(Y)\)</span></td>
<td><code>lm(sqrt(Y) ~ X)</code></td>
</tr>
<tr class="even">
<td align="center">1</td>
<td><span class="math inline">\(Y&#39; = Y\)</span></td>
<td><code>lm(Y ~ X)</code></td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td><span class="math inline">\(Y&#39; = Y^2\)</span></td>
<td><code>lm(Y^2 ~ X)</code></td>
</tr>
</tbody>
</table>
<p>Using “maximum-likelihood” estimation, the Box-Cox procedure can
actually automatically detect the “optimal” value of <span
class="math inline">\(\lambda\)</span> to consider for a
Y-transformation. Keep in mind however, that simply accepting a
suggested Y-transformation without considering the scatterplot and
diagnostic plots first, is unwise.</p>
<div class="tab">
<p><button class="tablinks" onclick="openTab(event, 'ScatterplotView')">Scatterplot
Recognition</button>
<button class="tablinks" onclick="openTab(event, 'BoxCoxView')">Box-Cox
Suggestion</button>
<button class="tablinks" onclick="openTab(event, 'YTransExample')">An
Example</button></p>
</div>
<div id="ScatterplotView" class="tabcontent" style="display:block;">
<p>
<div id="scatterplot-recognition" class="section level6">
<h6>Scatterplot Recognition</h6>
<p>The following panel of scatterplots can give you a good feel for when
to try different values of <span
class="math inline">\(\lambda\)</span>.</p>
<pre class="r"><code>set.seed(15)
N &lt;- 300
X &lt;- runif(N, 5, 50)
Y &lt;- 25 + 3.5*X + rnorm(N, 0, 20)

Ya &lt;- 1/sqrt(Y)   #1/Y^2   Lam = -2
Yb &lt;- 1/Y         #1/Y     Lam = -1
Yc &lt;- exp(.02*Y)  #log(Y)  Lam =  0
Yd &lt;- Y^2         #sqrt(Y) Lam =  0.5
Ye &lt;- Y           #Y       Lam =  1
Yf &lt;- sqrt(Y)     #Y^2     Lam =  2


par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0))

plot(Ya ~ X, main=expression(paste(&quot;Use &quot;, lambda == -2)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Ya^-2 ~ X))
curve(1/sqrt(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Yb ~ X, main=expression(paste(&quot;Use &quot;, lambda == -1)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Yb^-1 ~ X))
curve(1/(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Yc ~ X, main=expression(paste(&quot;Use &quot;, lambda == 0, &quot; i.e., log(...)&quot;)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(log(Yc) ~ X))
curve(exp(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Yd ~ X, main=expression(paste(&quot;Use &quot;, lambda == 0.5)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(sqrt(Yd) ~ X))
curve((b[1] + b[2]*x)^2, add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Ye ~ X, main=expression(paste(&quot;Use &quot;, lambda == 1, &quot; (No Transformation)&quot;)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Ye ~ X))
curve((b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Yf ~ X, main=expression(paste(&quot;Use &quot;, lambda == 2)), 
ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Yf^2 ~ X))
curve(sqrt(b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</p>
</div>
</div>
<div id="BoxCoxView" class="tabcontent">
<p>
<div id="box-cox-suggestion" class="section level6">
<h6>Box-Cox Suggestion</h6>
<p>The <code>boxCox(...)</code> function in <code>library(car)</code>
can also be helpful on finding values of <span
class="math inline">\(\lambda\)</span> to try.</p>
<pre class="r"><code>par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(2,0.5,0))

boxCox(lm(Ya ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == -2)), line=.5)

boxCox(lm(Yb ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == -1)), line=.5)

boxCox(lm(Yc ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 0, &quot; i.e., log(...)&quot;)), line=.5)

boxCox(lm(Yd ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 0.5)), line=.5)

boxCox(lm(Ye ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 1, &quot; (No Transformation)&quot;)), line=.5)

boxCox(lm(Yf ~ X))
mtext(side=3, text=expression(paste(&quot;Use &quot;, lambda == 2)), line=.5)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
</p>
</div>
</div>
<div id="YTransExample" class="tabcontent">
<p>
<div id="an-example" class="section level6">
<h6>An Example</h6>
<p>Suppose we were running a simple linear regression on the
<code>cars</code> dataset.</p>
<p>This would be done with the code</p>
<p><code>cars.lm &lt;- lm(dist ~ speed, data=cars)</code></p>
<p><code>summary(cars.lm)</code></p>
<p>Notice the line doesn’t quite fit the data as well as we would hope.
Instead, the data looks a little curved.</p>
<pre class="r"><code>cars.lm &lt;-lm(dist ~ speed,data=cars)
plot(dist ~ speed, data=cars, pch=20, col=&quot;firebrick&quot;, cex=1.2, las=1,
     xlab=&quot;Speed of the Vehicle (mph) \n the Moment the Brakes were Applied&quot;, ylab=&quot;Distance (ft) it took the Vehicle to Stop&quot;,
     main=&quot;Don&#39;t Step in front of a Moving 1920&#39;s Vehicle...&quot;)
mtext(side=3, text=&quot;...they take a few feet to stop.&quot;, cex=0.7, line=.5)
legend(&quot;topleft&quot;, legend=&quot;Stopping Distance Experiment&quot;, bty=&quot;n&quot;)

abline(cars.lm, col=&quot;gray&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>Using the <code>boxCox(...)</code> function from
<code>library(car)</code> we would compute the following to determine
which Y-transformation would be most meaningful.</p>
<p><code>library(car)</code></p>
<p><code>boxCox(cars.lm)</code></p>
<p>The output from the <code>boxCox(...)</code> function looks as
follows.</p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>This plot tells use to use the <span class="math inline">\(\lambda =
0.5\)</span> transformation, so that <span class="math inline">\(Y&#39;
= Y^0.5 = \sqrt{Y}\)</span>. (To see this yourself, click on the
“Box-Cox Suggestion” tab above, as well as on the “Scatterplot
Recognition” tab.)</p>
<p>Now, a transformation regression is performed using
<code>sqrt(Y)</code> in place of <code>Y</code> as follows:</p>
<p><code>cars.lm.t &lt;- lm(sqrt(dist) ~ speed, data=cars)</code></p>
<p><code>summary(cars.lm.t)</code></p>
<table>
<thead>
<tr class="header">
<th> </th>
<th>Estimate</th>
<th>Std. Error</th>
<th>t value</th>
<th>Pr(&gt;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>(Intercept)</strong></td>
<td>1.277</td>
<td>0.4844</td>
<td>2.636</td>
<td>0.01126</td>
</tr>
<tr class="even">
<td><strong>speed</strong></td>
<td>0.3224</td>
<td>0.02978</td>
<td>10.83</td>
<td>1.773e-14</td>
</tr>
</tbody>
</table>
<p>Then,</p>
<p><span class="math display">\[
  \widehat{Y}_i&#39; = 1.277 + 0.3224 X_i
\]</span></p>
<p>And replacing <span class="math inline">\(\hat{Y}_i&#39; =
\sqrt{\hat{Y}_i}\)</span> we have</p>
<p><span class="math display">\[
  \sqrt{\widehat{Y}_i} = 1.277 + 0.3224 X_i
\]</span></p>
<p>Solving for <span class="math inline">\(\hat{Y}_i\)</span> gives</p>
<p><span class="math display">\[
  \widehat{Y}_i = (1.277 + 0.3224 X_i)^2
\]</span></p>
<p>Which, using <code>curve((1.277 + 0.3224*x)^2, add=TRUE)</code> (see
code for details) looks like this:</p>
<pre class="r"><code>plot(dist ~ speed, data=cars, pch=20, col=&quot;firebrick&quot;, cex=1.2, las=1,
     xlab=&quot;Speed of the Vehicle (mph) \n the Moment the Brakes were Applied&quot;, ylab=&quot;Distance (ft) it took the Vehicle to Stop&quot;,
     main=&quot;Don&#39;t Step in front of a Moving 1920&#39;s Vehicle...&quot;)
mtext(side=3, text=&quot;...they take a few feet to stop.&quot;, cex=0.7, line=.5)
legend(&quot;topleft&quot;, legend=&quot;Stopping Distance Experiment&quot;, bty=&quot;n&quot;)

curve( (1.277 + 0.3224*x)^2, add=TRUE, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
</p>
</div>
</div>
<p><br /></p>
<div id="x-transformations" class="section level5">
<h5>X-Transformations</h5>
<p>X-transformations are more difficult to recognize than
y-transformations. This is partially because there is no Box-Cox method
to automatically search for them.</p>
<p>The best indicator that you should consider an x-transformation is
when the variance of the residuals is constant across all fitted-values,
but linearity is clearly violated.</p>
<p>The following panel of scatterplots can give you a good feel for when
to try different values of an x-transformation.</p>
<pre class="r"><code>set.seed(15)
N &lt;- 300
X &lt;- runif(N, 5, 50)
Y &lt;- 25 + 3.5*X + rnorm(N, 0, 20)

Xa &lt;- 1/sqrt(X)   #1/X^2   Lam = -2
Xb &lt;- 1/X         #1/X     Lam = -1
Xc &lt;- exp(.02*X)  #log(X)  Lam =  0
Xd &lt;- X^2         #sqrt(X) Lam =  0.5
Xe &lt;- X           #X       Lam =  1
Xf &lt;- sqrt(X)     #X^2     Lam =  2


par(mfrow=c(2,3), mai=c(.4,.4,.3,.2), mgp=c(0.5,0.5,0))

plot(Y ~ Xa, main=expression(paste(&quot;Use &quot;, X*minute == X^-2)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ I(Xa^-2)))
curve(b[1] + b[2]*x^-2, add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Y ~ Xb, main=expression(paste(&quot;Use &quot;, X*minute == X^-1)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ I(Xb^-1)))
curve(b[1] + b[2]*x^-1, add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Y ~ Xc, main=expression(paste(&quot;Use &quot;, X*minute == log(X))), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ log(Xc)))
curve(b[1] + b[2]*log(x), add=TRUE, col=&quot;green&quot;, lwd=2)


plot(Y ~ Xd, main=expression(paste(&quot;Use &quot;, X*minute == sqrt(X))), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ sqrt(Xd)))
curve(b[1] + b[2]*sqrt(x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Y ~ Xe, main=expression(paste(&quot;Use &quot;, X*minute == X, &quot; (No Transformation)&quot;)), ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ Xe))
curve((b[1] + b[2]*x), add=TRUE, col=&quot;green&quot;, lwd=2)

plot(Y ~ Xf, main=expression(paste(&quot;Use &quot;, X*minute == X^2)), 
ylab=&quot;Y in Original Units&quot;, pch=16, col=&quot;gray45&quot;, cex=0.9, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;X in Original Units&quot;)
b &lt;- coef(lm(Y ~ I(Xf^2)))
curve(b[1] + b[2]*x^2, add=TRUE, col=&quot;green&quot;, lwd=2)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
</div>
</div>
<p><br /></p>
</div>
<div id="inference-for-the-model-parameters-expand"
class="section level4">
<h4>Inference for the Model Parameters
<a href="javascript:showhide('inference1')" style="font-size:.6em;color:skyblue;" id="infModelParam">(Expand)</a></h4>
<p><span class="expand-caption">t test formulas, sampling distributions,
confidence intervals, and F tests…</span></p>
<div id="inference1" style="display:none;">
<p>When fitting the regression model given by the equation</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \quad \text{where} \
\epsilon_i \sim N(0, \sigma^2)
\]</span> to a sample of data, we typically test hypotheses about the
parameters <span class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span>, or both.</p>
<table class="fancytable">
<tr>
<th>
<strong>Hypotheses</strong>
</th>
<th>
<strong>Test Statistic</strong>
</th>
<th>
<strong>P-value</strong>
</th>
</tr>
<tr>
<td style="text-align:center;width:25%;">
<p><span class="math inline">\(H_0: \beta_0 =\)</span> <span
class="tooltiprbold"> <span class="math inline">\(\underbrace{0}_\text{a
number}\)</span> <span class="tooltiprtext">This could be any number,
not just 0. However, the default summar(mylm) output in R only shows the
test statistic and p-value for the test that uses 0. To test a different
value, you would need to compute the test statistic and p-value by hand
using the formula shown.</span> </span></p>
<p><span class="math inline">\(H_a: \beta_0\)</span><span
class="tooltiprbold"> <span class="math inline">\(\,\neq\,\)</span>
<span class="tooltiprtext">You could use <span
class="math inline">\(&gt;\)</span> or <span
class="math inline">\(&lt;\)</span> instead of <span
class="math inline">\(\neq\)</span> for the alternative hypothesis. By
default, the p-value from summary(mylm) in R uses <span
class="math inline">\(\neq\)</span>.</span> </span><span
class="tooltiprbold"> <span class="math inline">\(\underbrace{0}_\text{a
number}\)</span> <span class="tooltiprtext">This could be any number,
not just 0. However, the default summar(mylm) output in R only shows the
test statistic and p-value for the test that uses 0. To test a different
value, you would need to compute the test statistic and p-value by hand
using the formula shown.</span> </span></p>
</td>
<td style="text-align:center;width:25%;">
<p><span class="tooltiprbold"> <span class="math display">\[t =
\frac{b_0 - \overbrace{0}^\text{a number}}{s_{b_0}}\]</span> <span
class="tooltiprtext">This is the formula for the test statistic. It
measures how far the estimated y-intercept <span
class="math inline">\(b_0\)</span> is from the null hypothesis for <span
class="math inline">\(\beta_0\)</span> in units of “standard errors of
<span class="math inline">\(b_0\)</span>”. Thus the division by <span
class="math inline">\(s_{b_0}\)</span>. Though the hypothesized value of
<span class="math inline">\(\beta_0\)</span> is typically 0, it could be
any number.</span> </span></p>
</td>
<td style="text-align:center;width:50%;">
<p><a href="https://byuimath.com/apps/normprobwitht.html" target="_blank" title="Go to t applet"></p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</a>
</td>
</tr>
<tr>
<td style="text-align:center;width:25%;">
<p><span class="math inline">\(H_0: \beta_1 =\)</span> <span
class="tooltiprbold"> <span class="math inline">\(\underbrace{0}_\text{a
number}\)</span> <span class="tooltiprtext">This could be any number,
not just 0. However, the default summar(mylm) output in R only shows the
test statistic and p-value for the test that uses 0. To test a different
value, you would need to compute the test statistic and p-value by hand
using the formula shown.</span> </span></p>
<p><span class="math inline">\(H_a: \beta_1\)</span><span
class="tooltiprbold"> <span class="math inline">\(\,\neq\,\)</span>
<span class="tooltiprtext">You could use <span
class="math inline">\(&gt;\)</span> or <span
class="math inline">\(&lt;\)</span> instead of <span
class="math inline">\(\neq\)</span> for the alternative hypothesis. By
default, the p-value from summary(mylm) in R uses <span
class="math inline">\(\neq\)</span>.</span> </span><span
class="tooltiprbold"> <span class="math inline">\(\underbrace{0}_\text{a
number}\)</span> <span class="tooltiprtext">This could be any number,
not just 0. However, the default summar(mylm) output in R only shows the
test statistic and p-value for the test that uses 0. To test a different
value, you would need to compute the test statistic and p-value by hand
using the formula shown.</span> </span></p>
</td>
<td style="text-align:center;width:25%;">
<p><span class="tooltiprbold"> <span class="math display">\[t =
\frac{b_1 - \overbrace{0}^\text{a number}}{s_{b_1}}\]</span> <span
class="tooltiprtext">This is the formula for the test statistic. It
measures how far the estimated slope <span
class="math inline">\(b_1\)</span> is from the null hypothesis for <span
class="math inline">\(\beta_1\)</span> in units of “standard errors of
<span class="math inline">\(b_1\)</span>”. Thus the division by <span
class="math inline">\(s_{b_1}\)</span>. Though the hypothesized value of
<span class="math inline">\(\beta_1\)</span> is typically 0, it could be
any number.</span> </span></p>
</td>
<td>
<p>Left-tailed p-value =
<code>pt(-abs(tvalue), degrees of freedom)</code>.</p>
Double it to get the two-sided p-value.
</td>
</tr>
</table>
<p><br></p>
<p>In R, these values correspond to the output summary of an lm as
follows.</p>
<p><br></p>
<p><img src="Images/summaryOutputLabeled.png"></p>
<p><a href="javascript:showhide('ttestexample')" style="font-size:.9em;color:skyblue;">(Show
Example)</a></p>
<div id="ttestexample" style="display:none;">
<p>Consider the <code>cars</code> data in R. Suppose we used the
regression model given by</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{Feet to Stop} = \beta_0 + \beta_1
\underbrace{X_{i}}_\text{mph} + \epsilon_i \quad \text{where} \
\epsilon_i \quad \sim N(0,\sigma^2)
\]</span> to model the feet a vehicle (from the 1920’s) takes to stop
when traveling at a certain speed (in miles per hour, mph) prior to
stopping. When the regression is performed and summarized in R, it is
always testing the following two hypotheses:</p>
<p><span class="math display">\[
H_0: \beta_0 = 0 \quad\quad H_0: \beta_1 = 0 \\
H_a: \beta_0 \neq 0 \quad\quad H_a: \beta_1 \neq 0
\]</span></p>
<p>To perform the test of these hypotheses for the regression stated
above, we would run the following codes in R.</p>
<p><code>cars.lm &lt;- lm(dist ~ speed, data=cars)</code></p>
<p><code>pander(summary(cars.lm)$coefficients)</code></p>
<p>These would produce summary output like the following, but the
following output has been labeled with the math notation corresponding
to each value.</p>
<p><img src="Images/summaryOutputLabeled.png"></p>
<p>Let’s emphasize what is happening in this summary output table.</p>
<p>First, here is how the “t value” is calculated for the “(Intercept)”
in the summary table above.</p>
<p><span class="math display">\[
t = \frac{b_0-0}{s_{b_0}} = \frac{-17.58 - 0}{6.758} = -2.601
\]</span> Second, here is a visual representation of how the P-value,
the “Pr(&gt;|t|)” as it is called in the summary table above, is
calculated for this test statistic. (Click the graph to view an
interactive applet showing this calculation.) Notice both ends of the
t-distribution are being shaded to compute the P-value because the
alternative hypothesis was <span class="math inline">\(H_a: \beta_0 \neq
0\)</span>.</p>
<p><a href="https://byuimath.com/apps/normprobwitht.html?z=-2.601&df=48" target="_blank" title="Click to View in t Applet"></p>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p></a></p>
<p>To compute the P-value in R, we use the “percentile function for the
t-distribution” called <code>pt( )</code>. This function requires two
things, the t-value and the degrees of freedom, in our case
<code>pt(-2.601, 48)</code>. Note the degrees of freedom (df) are 48
because the sample size is <span class="math inline">\(n=50\)</span> and
there are two parameters (<span class="math inline">\(\beta_0\)</span>
and <span class="math inline">\(\beta_1\)</span>) in our regression
model.</p>
<p>Running this code in R gives: <code>pt(-2.601, 48)</code> =
0.00616</p>
<p>However, note that this value is only half of the actual P-value of
0.0123. To get the “two-sided” P-value (note that our alternative
hypothesis used a <span class="math inline">\(\neq\)</span> symbol) we
need to double this left-tailed P-value.</p>
<p><code>2*pt(-2.601, 48))</code> = 0.0123</p>
<p>Finally, note that the same procedure can be used to test hypotheses
that use a value other than 0 in the null and alternative. For example,
to test the hypotheses:</p>
<p><span class="math display">\[
H_0: \beta_1 = 3 \\
H_a: \beta_1 \neq 3
\]</span> Use the t-formula</p>
<p><span class="math display">\[
  t = \frac{b_1 - 3}{s_{b_1}} = \frac{\overbrace{3.932}^{b_1} -
\overbrace{3}^{H_0}}{\underbrace{0.4155}_{s_{b_1}}} = 2.243
\]</span> then the P-value is calculated in R by</p>
<p><code>2*pt(-abs(2.243), 48)</code> = 0.0295495</p>
<hr>
</div>
<p><br/></p>
<p>To obtain confidence intervals in R use
<code>confint(mylm)</code>.</p>
<table class="fancytable">
<tr>
<th>
<strong>Confidence Interval</strong>
</th>
<th>
<strong>Formula</strong>
</th>
<th>
<strong>Standard Error</strong>
</th>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(\beta_0\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(b_0 \pm\)</span><span class="tooltiprbold">
<span class="math inline">\(t^*\)</span> <span class="tooltiprtext">This
is called the “critical value” and denotes the number of standard
deviations that are needed to obtain a 95% confidence interval from a t
distribution with degrees of freedom <span
class="math inline">\(n-p\)</span>. Use <code>qt(0.975, df)</code> to
get <span class="math inline">\(t*\)</span> in R.</span> </span><span
class="tooltiprbold"> <span class="math inline">\(\cdot\)</span> <span
class="tooltiprtext">The critical value is multiplied by the standard
error of <span class="math inline">\(b_0\)</span>.</span> </span><span
class="tooltiprbold"> <span class="math inline">\(s_{b_0}\)</span> <span
class="tooltiprtext">The standard error of <span
class="math inline">\(b_0\)</span>, denoted by <span
class="math inline">\(s_{b_0}\)</span> is provided in the regression
summary output under the column header called “Std. Error” for the
“(Intercept)” row of the output. It is calculated using the formula
shown below.</span> </span>
</td>
<td style="text-align:center;">
<span class="tooltiprbold"> <span class="math display">\[s^2_{b_0} =
MSE\left[\frac{1}{n} +
\frac{\bar{X}^2}{\sum(X_i-\bar{X})^2}\right]\]</span> <span
class="tooltiprtext">This is called the “estimated variance of <span
class="math inline">\(b_0\)</span>”. Taking the square root of this
number gives the “standard error of <span
class="math inline">\(b_0\)</span>”.</span> </span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(\beta_1\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(b_1 \pm\)</span><span class="tooltiprbold">
<span class="math inline">\(t^*\)</span> <span class="tooltiprtext">This
is called the “critical value” and denotes the number of standard
deviations that are needed to obtain a 95% confidence interval from a t
distribution with degrees of freedom <span
class="math inline">\(n-p\)</span> (sample size - number of parameters
in the regression model). Use <code>qt(0.975, df)</code> to get <span
class="math inline">\(t*\)</span> in R</span> </span><span
class="tooltiprbold"> <span class="math inline">\(\cdot\)</span> <span
class="tooltiprtext">The critical value is multiplied by the standard
error of <span class="math inline">\(b_1\)</span>.</span> </span><span
class="tooltiprbold"> <span class="math inline">\(s_{b_1}\)</span> <span
class="tooltiprtext">The standard error of <span
class="math inline">\(b_1\)</span>, denoted by <span
class="math inline">\(s_{b_1}\)</span> is provided in the regression
summary output under the column header called “Std. Error”. It is
calculated using the formula shown below.</span> </span>
</td>
<td style="text-align:center;">
<span class="tooltiprbold"> <span class="math display">\[s^2_{b_1} =
\frac{MSE}{\sum(X_i-\bar{X})^2}\]</span> <span class="tooltiprtext">This
is called the “estimated variance of <span
class="math inline">\(b_1\)</span>”. Taking the square root of this
number gives the “standard error of <span
class="math inline">\(b_1\)</span>”.</span> </span>
</td>
</tr>
</table>
<p>To be more exact, the types of inference we are interested in are the
following.</p>
<ol style="list-style-type: decimal">
<li><p>Determine if there is evidence of a meaningful linear
relationship in the data. If <span class="math inline">\(\beta_1 =
0\)</span>, then there is no relation between <span
class="math inline">\(X\)</span> and <span
class="math inline">\(E\{Y\}\)</span>. Hence we might be interested in
testing the hypotheses <span class="math display">\[
  H_0: \beta_1 = 0
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq 0
\]</span></p></li>
<li><p>Determine if the slope is greater, less than, or different from
some other hypothesized value. In this case, we would be interested in
using hypotheses of the form <span class="math display">\[
  H_0: \beta_1 = \beta_{10}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10}
\]</span> where <span class="math inline">\(\beta_{10}\)</span> is some
hypothesized number.</p></li>
<li><p>To provide a confidence interval for the true value of <span
class="math inline">\(\beta_1\)</span>.</p></li>
</ol>
<p><br /></p>
<p>Before we discuss how to test the hypotheses listed above or
construct a confidence interval, we must understand the <strong>sampling
distribution</strong> of the estimate <span
class="math inline">\(b_1\)</span> of the parameter <span
class="math inline">\(\beta_1\)</span>. And, while we are at it, we may
as well come to understand the sampling distribution of the estimate
<span class="math inline">\(b_0\)</span> of the parameter <span
class="math inline">\(\beta_0\)</span>.</p>
<div style="padding-left:30px;color:darkgray;font-size:.8em;">
<p>Review <a
href="http://statistics.byuimath.com/index.php?title=Lesson_6:_Distribution_of_Sample_Means_%26_The_Central_Limit_Theorem#Introduction_to_Sampling_Distributions">sampling
distributions</a> from Math 221.</p>
</div>
<p>Since <span class="math inline">\(b_1\)</span> is an estimate, it
will vary from sample to sample, even though the truth, <span
class="math inline">\(\beta_1\)</span>, remains fixed. (The same holds
for <span class="math inline">\(b_0\)</span> and <span
class="math inline">\(\beta_0\)</span>.) It turns out that the sampling
distribution of <span class="math inline">\(b_1\)</span> (where the
<span class="math inline">\(X\)</span> values remain fixed from study to
study) is normal with mean and variance: <span class="math display">\[
  \mu_{b_1} = \beta_1
\]</span> <span class="math display">\[
  \sigma^2_{b_1} = \frac{\sigma^2}{\sum(X_i-\bar{X})^2}
\]</span></p>
<pre class="r"><code>## Simulation to Show relationship between Standard Errors

##-----------------------------------------------
## Edit anything in this area... 

n &lt;- 100 #sample size
Xstart &lt;- 30 #lower-bound for x-axis
Xstop &lt;- 100 #upper-bound for x-axis

beta_0 &lt;- 2 #choice of true y-intercept
beta_1 &lt;- 3.5 #choice of true slope
sigma &lt;- 13.8 #choice of st. deviation of error terms

## End of Editable area.
##-----------------------------------------------


# Create X, which will be used in the next R-chunk.
X &lt;- rep(seq(Xstart,Xstop, length.out=n/2), each=2) 

## After playing this chunk, play the next chunk as well.</code></pre>
<p>To see that this is true, consider the regression model with values
specified for each parameter as follows.</p>
<p><span class="math display">\[
  Y_i = \overbrace{\beta_0}^{2} + \overbrace{\beta_1}^{3.5} X_i +
\epsilon_i \quad \text{where} \ \epsilon_i \sim N(0,
\overbrace{\sigma^2}^{\sigma=13.8})
\]</span></p>
<p>Using the equations above for <span
class="math inline">\(\mu_{b_1}\)</span> and <span
class="math inline">\(\sigma^2_{b_1}\)</span> we obtain that the mean of
the sampling distribution of <span class="math inline">\(b_1\)</span>
will be</p>
<p><span class="math inline">\(\mu_{b_1} = \beta_1 = 3.5\)</span></p>
<p>Further, we see that the variance of the sampling distribution of
<span class="math inline">\(b_1\)</span> will be</p>
<p><span class="math inline">\(\sigma^2_{b_1} =
\frac{\sigma^2}{\sum(X_i-\bar{X})^2} = \frac{13.8^2}{4.25\times
10^{4}}\)</span></p>
<p>Taking the square root of the variance, the standard deviation of the
sampling distribution of <span class="math inline">\(b_1\)</span> will
be</p>
<p><span class="math inline">\(\sigma_{b_1} = 0.067\)</span>.</p>
<p>That’s very nice. But to really believe it, let’s run a simulation
ourselves. The “Code” below is worth studying. It runs a simulation that
(1) takes a sample of data from the true regression relation, (2) fits
the sampled data with an estimated regression equation (gray lines in
the plot), and (3) computes the estimated values of <span
class="math inline">\(b_1\)</span> and <span
class="math inline">\(b_0\)</span> for that regression.</p>
<p>After doing this many, many times, the results of every single
regression are plotted (in gray lines, which creates a gray shaded
region because there are so many lines) in the scatterplot below.
Further, each obtained estimate of <span
class="math inline">\(b_0\)</span> is plotted in the histogram on the
left (below the scatterplot) and each obtained estimate of <span
class="math inline">\(b_1\)</span> is plotted in the histogram on the
right. Looking at the histograms carefully, it can be seen that the mean
of each histogram is very close to the true parameter value of <span
class="math inline">\(\beta_0\)</span> or <span
class="math inline">\(\beta_1\)</span>, respectively. Also, the “Std.
Error” of each histogram is incredibly close (if not exact to 3 decimal
places) to the computed value of <span
class="math inline">\(\sigma_{b_0}\)</span> and <span
class="math inline">\(\sigma_{b_1}\)</span>, respectively. Amazing!</p>
<pre class="r"><code>N &lt;- 5000 #number of times to pull a random sample
storage_b0 &lt;- storage_b1 &lt;- storage_rmse &lt;- rep(NA, N)
for (i in 1:N){
  Y &lt;- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model
  mylm &lt;- lm(Y ~ X)
  storage_b0[i] &lt;- coef(mylm)[1]
  storage_b1[i] &lt;- coef(mylm)[2]
  storage_rmse[i] &lt;- summary(mylm)$sigma
}


layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3))

Ystart &lt;- 0 #min(0,min(Y)) 
Ystop &lt;- 500 #max(max(Y), 0)
Yrange &lt;- Ystop - Ystart

plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), 
     ylim=c(Ystart, Ystop), pch=16, col=&quot;gray&quot;,
     main=&quot;Regression Lines from many Samples (gray lines) \n Plus Residual Standard Deviation Lines (green lines)&quot;)
text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1)
text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1)
text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1)


for (i in 1:N){
  abline(storage_b0[i], storage_b1[i], col=&quot;darkgray&quot;)  
}
abline(beta_0, beta_1, col=&quot;green&quot;, lwd=3)
abline(beta_0+sigma, beta_1, col=&quot;green&quot;, lwd=2)
abline(beta_0-sigma, beta_1, col=&quot;green&quot;, lwd=2)
abline(beta_0+2*sigma, beta_1, col=&quot;green&quot;, lwd=1)
abline(beta_0-2*sigma, beta_1, col=&quot;green&quot;, lwd=1)
abline(beta_0+3*sigma, beta_1, col=&quot;green&quot;, lwd=.5)
abline(beta_0-3*sigma, beta_1, col=&quot;green&quot;, lwd=.5)

par(mai=c(1,.6,.5,.01))

  addnorm &lt;- function(m,s, col=&quot;firebrick&quot;){
    curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2)
    lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col)
    lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col)
    lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col)
    lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col)
    lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col)
    lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col)
    lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col)
    legend(&quot;topleft&quot;, legend=paste(&quot;Std. Error = &quot;, round(s,3)), cex=0.7, bty=&quot;n&quot;)
  }

  h0 &lt;- hist(storage_b0, 
             col=&quot;skyblue3&quot;, 
             main=&quot;Sampling Distribution\n Y-intercept&quot;,
             xlab=expression(paste(&quot;Estimates of &quot;, beta[0], &quot; from each Sample&quot;)),
             freq=FALSE, yaxt=&#39;n&#39;, ylab=&quot;&quot;)
  m0 &lt;- mean(storage_b0)
  s0 &lt;- sd(storage_b0)
  addnorm(m0,s0, col=&quot;green&quot;)
  
  h1 &lt;- hist(storage_b1, 
             col=&quot;skyblue3&quot;, 
             main=&quot;Sampling Distribution\n Slope&quot;,
             xlab=expression(paste(&quot;Estimates of &quot;, beta[1], &quot; from each Sample&quot;)),
             freq=FALSE, yaxt=&#39;n&#39;, ylab=&quot;&quot;)
  m1 &lt;- mean(storage_b1)
  s1 &lt;- sd(storage_b1)
  addnorm(m1,s1, col=&quot;green&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-42-1.png" width="768" /></p>
<div style="padding-left:15px;">
<div id="tTests" class="section level5">
<h5>t Tests</h5>
<p>Using the information above about the sampling distributions of <span
class="math inline">\(b_1\)</span> and <span
class="math inline">\(b_0\)</span>, an immediate choice of statistical
test to test the hypotheses <span class="math display">\[
  H_0: \beta_1 = \beta_{10}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10}
\]</span> where <span class="math inline">\(\beta_{10}\)</span> can be
zero, or any other value, is a t test given by <span
class="math display">\[
  t = \frac{b_1 - \beta_{10}}{s_{b_1}}
\]</span> where <span class="math inline">\(s^2_{b_1} =
\frac{MSE}{\sum(X_i-\bar{X})^2}\)</span>. (You may want to review the
section “Estimating the Model Variance” of this file to know where MSE
came from.) With quite a bit of work it has been shown that <span
class="math inline">\(t\)</span> is distributed as a <span
class="math inline">\(t\)</span> distribution with <span
class="math inline">\(n-2\)</span> degrees of freedom. The nearly
identical test statistic for testing <span class="math display">\[
  H_0: \beta_0 = \beta_{00}
\]</span> <span class="math display">\[
  H_a: \beta_0 \neq \beta_{00}
\]</span> is given by <span class="math display">\[
  t = \frac{b_0 - \beta_{00}}{s_{b_0}}
\]</span> where <span class="math inline">\(s^2_{b_0} =
MSE\left[\frac{1}{n}+\frac{\bar{X}^2}{\sum(X_i-\bar{X})^2}\right]\)</span>.
This version of <span class="math inline">\(t\)</span> has also been
shown to be distributed as a <span class="math inline">\(t\)</span>
distribution with <span class="math inline">\(n-2\)</span> degrees of
freedom.</p>
</div>
<div id="confidence-intervals" class="section level5">
<h5>Confidence Intervals</h5>
<p>Creating a confidence interval for either <span
class="math inline">\(\beta_1\)</span> or <span
class="math inline">\(\beta_0\)</span> follows immediately from these
results using the formulas <span class="math display">\[
  b_1 \pm t^*_{n-2}\cdot s_{b_1}
\]</span> <span class="math display">\[
  b_0 \pm t^*_{n-2}\cdot s_{b_0}
\]</span> where <span class="math inline">\(t^*_{n-2}\)</span> is the
critical value from a t distribution with <span
class="math inline">\(n-2\)</span> degrees of freedom corresponding to
the chosen confidence level.</p>
<p><br /></p>
</div>
<div id="Ftests" class="section level5">
<h5>F tests</h5>
<p>Another way to test the hypotheses <span class="math display">\[
  H_0: \beta_1 = \beta_{10}  \quad\quad \text{or} \quad\quad H_0:
\beta_0 = \beta_{00}
\]</span> <span class="math display">\[
  H_a: \beta_1 \neq \beta_{10} \quad\quad \ \ \quad \quad H_a: \beta_0
\neq \beta_{00}
\]</span> is with an <span class="math inline">\(F\)</span> Test. One
downside of the F test is that we cannot construct confidence intervals.
Another is that we can only perform two-sided tests, we cannot use
one-sided alternatives with an F test. The upside is that an <span
class="math inline">\(F\)</span> test is very general and can be used in
many places that a t test cannot.</p>
<p>In its most general form, the <span class="math inline">\(F\)</span>
test partitions the sums of squared errors into different pieces and
compares the pieces to see what is accounting for the most variation in
the data. To test the hypothesis that <span
class="math inline">\(H_0:\beta_1=0\)</span> against the alternative
that <span class="math inline">\(H_a: \beta_1\neq 0\)</span>, we are
essentially comparing two models against each other. If <span
class="math inline">\(\beta_1=0\)</span>, then the corresponding model
would be <span class="math inline">\(E\{Y_i\} = \beta_0\)</span>. If
<span class="math inline">\(\beta_1\neq0\)</span>, then the model
remains <span
class="math inline">\(E\{Y_i\}=\beta_0+\beta_1X_i\)</span>. We call the
model corresponding to the null hypothesis the reduced model because it
will always have fewer parameters than the model corresponding to the
alternative hypothesis (which we call the full model). This is the first
requirement of the <span class="math inline">\(F\)</span> Test, that the
null model (reduced model) have fewer “free” parameters than the
alternative model (full model). To demonstrate what we mean by “free”
parameters, consider the following example.</p>
<p>Say we wanted to test the hypothesis that <span
class="math inline">\(H_0:\beta_1 = 2.5\)</span> against the alternative
that <span class="math inline">\(\beta_1\neq2.5\)</span>. Then the null,
or reduced model, would be <span
class="math inline">\(E\{Y_i\}=\beta_0+2.5X_i\)</span>. The alternative,
or full model, would be <span
class="math inline">\(E\{Y_i\}=\beta_0+\beta_1X_i\)</span>. Thus, the
null (reduced) model contains only one “free” parameter because <span
class="math inline">\(\beta_1\)</span> has been fixed to be 2.5 and is
no longer free to be estimated from the data. The alternative (full)
model contains two “free” parameters, both are to be estimated from the
data. The null (reduced) model must contain fewer free parameters than
the alternative (full) model.</p>
<p>Once the null and alternative models have been specified, the General
Linear Test is performed by appropriately partitioning the squared
errors into pieces corresponding to each model. In the first example
where we were testing <span class="math inline">\(H_0:
\beta_1=0\)</span> against <span
class="math inline">\(H_a:\beta_1\neq0\)</span> we have the partition
<span class="math display">\[
  \underbrace{Y_i-\bar{Y}}_{Total} = \underbrace{\hat{Y}_i -
\bar{Y}}_{Regression} + \underbrace{Y_i-\hat{Y}_i}_{Error}
\]</span> The reason we use <span class="math inline">\(\bar{Y}\)</span>
for the null model is that <span class="math inline">\(\bar{Y}\)</span>
is the unbiased estimator of <span
class="math inline">\(\beta_0\)</span> for the null model, <span
class="math inline">\(E\{Y_i\} = \beta_0\)</span>. Thus we would compute
the following sums of squares: <span class="math display">\[
  SSTO = \sum(Y_i-\bar{Y})^2
\]</span> <span class="math display">\[
  SSR = \sum(\hat{Y}_i-\bar{Y})^2
\]</span> <span class="math display">\[
  SSE = \sum(Y_i-\hat{Y}_i)^2
\]</span> and note that <span class="math inline">\(SSTO = SSR +
SSE\)</span>. Important to note is that <span
class="math inline">\(SSTO\)</span> uses the difference between the
observations <span class="math inline">\(Y_i\)</span> and the null
(reduced) model. The <span class="math inline">\(SSR\)</span> uses the
diffences between the alternative (full) and null (reduced) model. The
<span class="math inline">\(SSE\)</span> uses the differences between
the observations <span class="math inline">\(Y_i\)</span> and the
alternative (full) model. From these we could set up a General <span
class="math inline">\(F\)</span> table of the form</p>
<table style="width:100%;">
<colgroup>
<col width="21%" />
<col width="21%" />
<col width="10%" />
<col width="23%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Sum Sq</th>
<th>Df</th>
<th>Mean Sq</th>
<th>F Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Model Error</td>
<td><span class="math inline">\(SSR\)</span></td>
<td><span class="math inline">\(df_R-df_F\)</span></td>
<td><span class="math inline">\(\frac{SSR}{df_R-df_F}\)</span></td>
<td><span
class="math inline">\(\frac{SSR}{df_R-df_F}\cdot\frac{df_F}{SSE}\)</span></td>
</tr>
<tr class="even">
<td>Residual Error</td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(df_F\)</span></td>
<td><span class="math inline">\(\frac{SSE}{df_F}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Total Error</td>
<td><span class="math inline">\(SSTO\)</span></td>
<td><span class="math inline">\(df_R\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p><br /></p>
<div id="prediction-and-confidence-intervals-for-haty_h-expand"
class="section level4">
<h4>Prediction and Confidence Intervals for <span
class="math inline">\(\hat{Y}_h\)</span>
<a href="javascript:showhide('predictionintervals')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">predict(…, interval=“prediction”)…
</span></p>
<div id="predictionintervals" style="display:none;">
<p>It is a common mistake to assume that averages (means) describe
individuals. They do not. So, when providing predictions on individuals,
it is crucial to capture the variability of individuals around the
line.</p>
<table>
<colgroup>
<col width="21%" />
<col width="17%" />
<col width="32%" />
<col width="28%" />
</colgroup>
<thead>
<tr class="header">
<th>Interval</th>
<th>R Code</th>
<th>Math Equation</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prediction</td>
<td><span
style="font-size:.8em;"><code>predict(..., interval="prediction")</code></span></td>
<td><span class="math inline">\(\hat{Y}_i \pm t^* \cdot s_{\text{Pred}\
Y}\)</span></td>
<td>Predict an individual’s value.</td>
</tr>
<tr class="even">
<td>Confidence</td>
<td><span
style="font-size:.8em;"><code>predict(..., interval="confidence")</code></span></td>
<td><span class="math inline">\(\hat{Y}_i \pm t^* \cdot
s_{\hat{Y}}\)</span></td>
<td>Estimate location of the mean y-value.</td>
</tr>
</tbody>
</table>
<p><code>predict(mylm, data.frame(XvarName = number), interval=...)</code></p>
<p><br /> <br /></p>
<p>For example, consider this graph. Then
<a href="javascript:showhide('predictionintervalsgraph')" style="color:skyblue;">click
here</a> to read about the graph.</p>
<div id="predictionintervalsgraph"
style="padding-left:30px;padding-right:30px;font-size:.9em;display:none;">
<p>Notice the three dots above 15 mph in the graph. Each of these dots
show a car that was going 15 mph when it applied the brakes. However,
stopping distances of the three individual cars differ with one at 20
feet, one at 26 feet and one at 54 feet.</p>
<p>The regression line represents the average stopping distance of cars.
In this case, cars going 15 mph are estimated to have an average
stopping distance of about 40 feet, as shown by the line. But individual
vehicles, all going the same speed of 15 mph, varied from stopping
distances of 20 feet up to 54 feet!</p>
<p>So, to predict that a car going 15 mph will take 41.4 feet to stop,
doesn’t tell the whole story. Far more revealing is the complete
statement, “Cars going 15 mph are predicted to take anywhere from 10.2
to 72.6 feet to stop, with an average stopping distance of 41.4 feet.”
This is called the “prediction interval” and is shown in the graph in
blue. It is obtained in R with the codes:</p>
<p><code>cars.lm &lt;- lm(dist ~ speed, data=cars)</code></p>
<p><code>predict(cars.lm, data.frame(speed=15), interval="prediction")</code></p>
<pre class="r"><code>cars.lm &lt;- lm(dist ~ speed, data=cars)
pander(predict(cars.lm, data.frame(speed=15), interval=&quot;prediction&quot;))</code></pre>
<table style="width:33%;">
<colgroup>
<col width="11%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">fit</th>
<th align="center">lwr</th>
<th align="center">upr</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">41.41</td>
<td align="center">10.17</td>
<td align="center">72.64</td>
</tr>
</tbody>
</table>
</div>
<pre class="r"><code>plot(dist ~ speed, data=cars, pch=20, col=&quot;firebrick&quot;, cex=1.2, las=1,
     xlab=&quot;Speed of the Vehicle (mph) \n the Moment the Brakes were Applied&quot;, ylab=&quot;Distance (ft) it took the Vehicle to Stop&quot;,
     main=&quot;Don&#39;t Step in front of a Moving 1920&#39;s Vehicle...&quot;)
mtext(side=3, text=&quot;...they take a few feet to stop.&quot;, cex=0.7, line=.5)
legend(&quot;topleft&quot;, legend=&quot;Stopping Distance Experiment&quot;, bty=&quot;n&quot;)
points(dist ~ speed, data=subset(cars, speed==15), pch=20, col=&quot;firebrick2&quot;, cex=1.5)

cars.lm &lt;- lm(dist ~ speed, data=cars)
abline(cars.lm, lwd=2, col=rgb(.689,.133,.133, .3))
abline(h=seq(0,120,20), v=seq(5,25,5), lty=2, col=rgb(.2,.2,.2,.2))
abline(v=15, lty=2, col=&quot;firebrick&quot;)

preds &lt;- predict(cars.lm, data.frame(speed=15), interval=&quot;prediction&quot;)
lines(c(15,15), preds[2:3] - c(-.5,.5), col=rgb(.529,.8078,.9216,.4), lwd=12)
lines(c(0,15), preds[c(2,2)], col=rgb(.529,.8078,.9216,.8))
lines(c(0,15), preds[c(3,3)], col=rgb(.529,.8078,.9216,.8))</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>Now, for the details behind prediction intervals and confidence
intervals.</p>
<p>Let’s begin by recalling some details (from the section “Inference
for the Model Parameters”) about the standard error of the y-intercept,
<span class="math inline">\(b_0\)</span>. Recall that the y-intercept is
the average y-value for the given x-value of <span
class="math inline">\(x=0\)</span>. Recall further that the formula for
the standard error of <span class="math inline">\(b_0\)</span> is given
by the formula</p>
<p><span class="math display">\[
  s^2_{b_0} = MSE\left[\frac{1}{n} +
\frac{\bar{X}^2}{\sum(X_i-\bar{X})^2}\right]
\]</span></p>
<p>If we wanted to be more exact with this formula, we would write it
as</p>
<p><span class="math display">\[
  s^2_{b_0} = MSE\left[\frac{1}{n} +
\frac{(0-\bar{X})^2}{\sum(X_i-\bar{X})^2}\right]
\]</span></p>
<p>Did you notice the addition of <span class="math inline">\((0 -
\bar{X})^2\)</span> instead of just <span
class="math inline">\(\bar{X}^2\)</span> in the numerator of the
right-most part of the equation? This more complete statement obviously
would reduce to just <span class="math inline">\(\bar{X}^2\)</span>, but
that is only because <span class="math inline">\(X=0\)</span> when we
are working with the y-intercept, <span
class="math inline">\(b_0\)</span>. We could be working with other
values of <span class="math inline">\(X\)</span> than just zero.</p>
<div class="note">
<p>Let’s take a quick detour and talk notation for a second. Typically,
<span class="math inline">\(X_i\)</span> and <span
class="math inline">\(Y_i\)</span> are used to denote the x-value and
y-value of points that are contained in our data set. When we want to
reference a point that wasn’t within our original data set, we use the
notation <span class="math inline">\(X_h\)</span> and <span
class="math inline">\(Y_h\)</span>. (The letter h is close to i, but
different from i, so why not. There is really no other reason to use h.)
Thus, <span class="math inline">\(Y_h\)</span> is the y-value for the
<span class="math inline">\(X_h\)</span> x-value, neither of which were
included in our original regression of <span
class="math inline">\(X_i\)</span>’s and <span
class="math inline">\(Y_i\)</span>’s.</p>
</div>
<p>Now, back to the previous discussion. If <span
class="math inline">\(X_h = 0\)</span>, then <span
class="math inline">\(\hat{Y}_h\)</span> is the y-intercept, so <span
class="math inline">\(\hat{Y}_h = b_0\)</span> when <span
class="math inline">\(X_h=0\)</span>. So, we could write,</p>
<p><span class="math display">\[
  s^2_{\hat{Y}_h} = MSE\left[\frac{1}{n} +
\frac{(X_h-\bar{X})^2}{\sum(X_i-\bar{X})^2}\right]
\]</span></p>
<p>Did you notice how the <span class="math inline">\(b_0\)</span> in
<span class="math inline">\(s_{b_0}\)</span> was replaced with <span
class="math inline">\(\hat{Y}_h\)</span> to get <span
class="math inline">\(s_{\hat{Y}_h}\)</span> and the 0 in <span
class="math inline">\((0 - \bar{X})^2\)</span> was replaced with <span
class="math inline">\(X_h\)</span> to get <span
class="math inline">\((X_h - \bar{X})^2\)</span>? Interesting. We now
have a formula that would give us the standard error of <span
class="math inline">\(\hat{Y}_h\)</span> for any <span
class="math inline">\(X_h\)</span> value, not just <span
class="math inline">\(X_h = 0\)</span>, or the y-intercept, <span
class="math inline">\(b_0\)</span>. That is fantastic. It would look
like this if plotted. Notice how the gray region is showing the standard
error for each <span class="math inline">\(\hat{Y}_h\)</span> value? (It
is technically showing the confidence interval for <span
class="math inline">\(E\{Y_h\}\)</span> at every possible <span
class="math inline">\(X_h\)</span> value, but that is just <span
class="math inline">\(\hat{Y}_h \pm t^* \cdot
s_{\hat{Y}_h}\)</span>.)</p>
<pre class="r"><code>ggplot(cars, aes(x=speed, y=dist)) + 
  geom_point() +
  geom_smooth(method=&quot;lm&quot;, color=&quot;skyblue&quot;) +
  theme_bw()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p><strong>Confidence Interval for <span
class="math inline">\(\hat{Y}_h\)</span></strong></p>
<p><span class="math display">\[
  \hat{Y}_h \pm t^* s_{\hat{Y}_h} \quad \text{where} \ s_{\hat{Y}_h}^2 =
MSE\left[\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum(X_i -
\bar{X})^2}\right]
\]</span></p>
<p>The confidence interval is a wonderful tool for estimating <span
class="math inline">\(E\{Y_h\}\)</span>, the “true” average y-value for
a given x-value of <span class="math inline">\(X_h\)</span>. However, it
is not valuable for predicting an individual dot, or <span
class="math inline">\(Y_h\)</span> value. Notice how few of the dots of
the regression are actually contained within the confidence interval
band in the plot? The confidence interval does not really predict where
the dots will land, just where the average y-value is located for each
x-value.</p>
<p>Remember the 68-95-99.7 Rule of the normal distribution? If not, here
is a link back to that concept in the <a
href="https://byuistats.github.io/BYUI_M221_Book/Lesson05.html#normal-probability-computations">Math
221</a> textbook. This rule states that roughly 95% of data, when
normally distributed, will be between <span
class="math inline">\(z=-2\)</span> and <span
class="math inline">\(z=2\)</span> standard deviations from the mean.
So, is going two “residual standard errors” to both sides of the
regression line enough to capture 95% of the data? The answer is, not
quite. The reason for this is because our knowledge of where the true
mean lies is uncertain. (Notice the confidence interval band shown in
the plot.) However, adding two standard errors to the edges of the
confidence band would get us in the right place. In other words, there
are two sources of variability at play here, (1) our uncertaintity in
where the regression line is sitting, and (2) the natural variability of
the data points around the line. Thus, the “prediction interval”
requires accounting for both of these sources of variability to produce
the following equation.</p>
<p><strong>Prediction Interval for <span
class="math inline">\(Y_h\)</span></strong></p>
<p><span class="math display">\[
  \hat{Y}_h \pm t^* s_{Pred \hat{Y}_h} \quad \text{where} \ s_{Pred
\hat{Y}_h}^2 = MSE\left[\frac{1}{n} + 1 + \frac{(X_h -
\bar{X})^2}{\sum(X_i - \bar{X})^2}\right]
\]</span></p>
<p>This formula provides a useful band for identifying a region where we
are 95% confident that a new observation for <span
class="math inline">\(Y_h\)</span> will land, given the value of <span
class="math inline">\(X_h\)</span>.</p>
<p>It looks as follows. Notice the prediction interval is much wider
than the confidence interval. This is because data varies far more than
do means. Prediction is for where the individual data points will land,
confidence is for where the mean will land.</p>
<pre class="r"><code>cars.lm &lt;- lm(dist ~ speed, data=cars)
predy &lt;- predict(cars.lm, data.frame(speed=15), interval=&quot;prediction&quot;)

ggplot(cars, aes(x=speed, y=dist)) + 
  geom_point() +
  geom_smooth(method=&quot;lm&quot;, color=&quot;skyblue&quot;) +
  geom_segment(aes(x=15, xend=15, y=predy[2], yend=predy[3]), lwd=4, color=rgb(.5,.7,.5,.01)) + 
  geom_point(aes(x=15, y=predy[1]), cex=2, color=&quot;skyblue&quot;, pch=15) +
  theme_bw()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
</div>
<p><br/></p>
</div>
<div id="lowess-and-loess-curves-expand" class="section level4">
<h4>Lowess (and Loess) Curves
<a href="javascript:showhide('lowesscurves')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">A non-parametric approach to estimating
<span class="math inline">\(E\{Y_i\}\)</span>… </span></p>
<div id="lowesscurves" style="display:none;">
<p>Robust <strong>lo</strong>cally <strong>wei</strong>ghted regression
and <strong>s</strong>moothing <strong>s</strong>catterplots (LOWESS),
is an effective way to visually model the average y-value.</p>
<hr />
<table>
<tr>
<td>
<p><strong>Using Base R</strong></p>
<pre class="r"><code>air2 &lt;- na.omit(select(airquality, Temp, Ozone))

# Just quickly draw the lowess curve:
plot(Temp ~ Ozone, data=air2, pch=16, col=&quot;darkgray&quot;)
lines(lowess(air2$Ozone, air2$Temp), col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<pre class="r"><code>## OR optionally, 
## allow for predictions as well as the graph:
# plot(Temp ~ Ozone, data=air2, pch=16, col=&quot;darkgray&quot;)
# air2 &lt;- arrange(air2, desc(Ozone))
# mylo &lt;- loess(Temp ~ Ozone, data=air2, degree=1)
# lines(mylo$fit ~ Ozone, data=air2)</code></pre>
</td>
<td>
<p><strong>Using ggplot2</strong></p>
<pre class="r"><code>air2 &lt;- na.omit(select(airquality, Temp, Ozone))

# Just quickly draw the lowess curve:
ggplot(air2, aes(x=Ozone, y=Temp)) +
  geom_point(color=&quot;darkgray&quot;) + 
  geom_smooth(se=F, method=&quot;loess&quot;, method.args = list(degree=1)) + #Note, degree=2 by default.
  theme_bw()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<pre class="r"><code>## OR optionally, 
## allow for predictions as well as the graph:
# air2 &lt;- arrange(air2, desc(Ozone))
# mylo &lt;- loess(Temp ~ Ozone, data=air2, degree=1)
# ggplot(air2, aes(x=Ozone, y=Temp)) +
#   geom_point() +
#   geom_line(data=air2, aes(y=mylo$fit, x=Ozone))</code></pre>
</td>
</tr>
</table>
<hr />
<p><br /></p>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Advantages</strong></th>
<th><strong>Disadvantages</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Quick. Good at ignoring outliers. Good at capturing the general
pattern in the data. Good for making predictions within the scope of the
data.</td>
<td>No mathematical model. Not interpretable. No p-values. No adjusted
R-squared.</td>
</tr>
</tbody>
</table>
<p><strong>How it Works</strong></p>
<p>The Lowess curve localizes the regression model to a “neighborhood”
of points, and then joins these localized regressions together into a
smooth line. It minimizes the effect of outliers, and let’s the data
“speak for itself”.</p>
<p>As a downside, it is not interpretable, and has no final way to write
the model mathematically. All the same, it is a very powerful tool for
identifying an appropriate model, or verifying the fit of a model, or
making predictions when no reasonable model does an adequate job.</p>
<p>Study this graphic and the explanations below to learn how it
works.</p>
<p><em>Recommendation: run the code in this “Code” chunk to the right in
your Console, and flip through the resulting graphics.</em></p>
<pre class="r"><code>X &lt;- cars$speed
Y &lt;- cars$dist
X &lt;- X[!is.na(X) &amp; !is.na(Y)]
Y &lt;- Y[!is.na(X) &amp; !is.na(Y)]
f &lt;- 1/2
n &lt;- length(X)

lfit &lt;- rep(NA,n)
for (xh in 1:n){
 xdists &lt;- X - X[xh]
 nn &lt;- floor(n*f)
 r &lt;- sort(abs(xdists))[nn]
 xdists.nbrhd &lt;- which(abs(xdists) &lt; r)
 w &lt;- rep(0, length(xdists))
 w[xdists.nbrhd] &lt;- (1 - abs(xdists[xdists.nbrhd]/r)^3)^3
 plot(Y ~ X, pch=21, bg=rgb(.53,.81,.92, w),   
      col=rgb(.2,.2,.2,.3), cex=1.5, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, xlab=&quot;&quot;, ylab=&quot;&quot;)
 points(Y[xh] ~ X[xh], pch=16, col=&quot;orange&quot;)
 lmc &lt;- lm(Y ~ X, weights=w)
 curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;orange&quot;, add=TRUE)
 lines(lfit[1:xh] ~ X[1:xh], col=&quot;gray&quot;)
 
 #lines(lowess(X,Y), col=rgb(0.698,0.133,0.133,.2))
 cat(&quot;\n\n&quot;)
 readline(prompt=paste0(&quot;Center point is point #&quot;, xh, &quot;... Press [enter] to continue...&quot;))
 

 MADnotThereYet &lt;- TRUE
 count &lt;- 0
 while(MADnotThereYet){
   
      readline(prompt=paste0(&quot;\n   Adjusting line to account for outliers in the y-direction... Press [enter] to continue...&quot;))   
   
   curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;wheat&quot;, add=TRUE)

   MAD &lt;- median(abs(lmc$res))
   resm &lt;- lmc$res/(6*MAD)
   resm[resm&gt;1] &lt;- 1
   bisq &lt;- (1-resm^2)^2
   w &lt;- w*bisq
   obs &lt;- coef(lmc)
   lmc &lt;- lm(Y ~ X, weights=w)
 
   curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;orange&quot;, add=TRUE)

   count &lt;- count + 1
   if ( (sum(abs(obs-lmc$coef))&lt;.1) | (count &gt; 3))
     MADnotThereYet &lt;- FALSE
       
 }

   curve(lmc$coef[1] + lmc$coef[2]*x, from=min(X[xdists.nbrhd]), to=max(X[xdists.nbrhd]), col=&quot;green&quot;, add=TRUE)
   points(lmc$coef[1] + lmc$coef[2]*X[xh] ~ X[xh], pch=16, col=&quot;green&quot;)
   

  readline(prompt=paste0(&quot;\n   Use final line to get fitted value for this point... Press [enter] to continue to next point...&quot;))
 
 lfit[xh] &lt;- predict(lmc, data.frame(X=X[xh]))
 lines(lfit[1:xh] ~ X[1:xh], col=&quot;gray&quot;)
 

 if (xh == n){
     readline(prompt=paste0(&quot;\n  Press [enter] to see actual Lowess curve...&quot;))
    lines(lowess(X,Y, f=f), col=&quot;firebrick&quot;)
    legend(&quot;topleft&quot;, bty=&quot;n&quot;, legend=&quot;Actual lowess Curve using lowess(...)&quot;, col=&quot;firebrick&quot;, lty=1)
 }
  
  
}</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
<ol style="list-style-type: decimal">
<li><p>Select a fraction of the data to use for the “neighborhood” of
points (shown in blue in the graph above). The <code>lowess</code>
function in R uses “f=2/3” and the <code>loess</code> function uses
“span=0.75” for this value, which selects the nearest two-thirds or 75%
of the data, respectively, depending on which function you use. For this
example, we set the fraction of points at 50%. Both functions can be set
to whatever you want.</p></li>
<li><p>Pick any point in the regression, eventually selecting all points
one at a time. The selected point becomes the “center” of a
“neighborhood” of points surrounding it. In this example, the center
point is in orange, and the neighboring points are in blue.</p></li>
<li><p>Use the points within the neighborhood to fit a regression line.
However, make the regression depend most on points closest to “center”
and least on points furthest from “center.” This is called a weighted
regression. Weights are decided according to what is called the tricubic
weight function, so that the weight <span
class="math inline">\(w\)</span> given to point <span
class="math inline">\(j\)</span> of the neighborhood of points is
defined by <span class="math display">\[
  w_j = \left(1- \left( \frac{|X_c - X_j|}{\max_k |X_c -
X_k|}\right)^3\right)^3
\]</span> where <span class="math inline">\(X_c\)</span> is the x-value
of the “center” dot and <span class="math inline">\(X_j\)</span> is the
x-value of any other dot in the neighborhood.</p></li>
<li><p>The fitted-value of <span
class="math inline">\(\hat{Y}_c\)</span> is obtained for the center
point <span class="math inline">\(X_c\)</span> of the current
regression. This point is used as the Lowess (or Loess) curve’s value at
that particular x-value. Well, almost. It’s a first guess at where this
value will end up, but there’s a little more to the algorithm before we
are done. Initial guesses for each of these fitted values are obtained
for each point in the regression.</p></li>
<li><p>Now each local regression for each neighborhood is re-run a few
times in such a way the the effect of outliers is minimized. The final
line for each neighborhood is obtained by the following steps.</p>
<ul>
<li>Compute all residuals for points in the neighborhood of the current
regression, denoted by <span class="math inline">\(r_i\)</span>.</li>
<li>Then compute the MAD, median absolute deviation, of the residuals
<span class="math inline">\(MAD = \text{median} (|r_1|, |r_2|,
\ldots)\)</span>.</li>
<li>Divide all residuals by 6 times the MAD: <span
class="math inline">\(u_i = r_i/(6\cdot MAD)\)</span> (If <span
class="math inline">\(r_i &gt; 6\cdot MAD\)</span> then set <span
class="math inline">\(u_i = 0\)</span>.)</li>
<li>Compute what are called bisquare weights using the formula: <span
class="math inline">\(b_i = (1 - u_i^2)^2\)</span></li>
<li>Perform a regression using the weights <span
class="math inline">\(w_i = w_i b_i\)</span></li>
<li>Repeat the above process with the new weights <span
class="math inline">\(w_i\)</span> until the weights stop changing very
much.</li>
</ul></li>
<li><p>The final fitted values for each <span
class="math inline">\(X\)</span>-value in the regression are obtained
from the final regression line for each neighborhood. These fitted
values make up the Lowess (or loess) curve.</p></li>
</ol>
<p>Note that the default of the <code>loess</code> function in R is to
use quadratic regressions in each neighborhood instead of linear
regressions. This can be controlled with the <code>loess</code> option
of “degree=2” (quadratic fits) or “degree = 1”. In the
<code>lowess</code> function only a linear regression in each
neighborhood is allowed.</p>
</div>
<p><br /></p>
<hr />
</div>
</div>
<div id="section" class="section level2">
<h2></h2>
<div style="padding-left:125px;">
<p><strong>Examples:</strong> <a
href="./Analyses/Linear%20Regression/Examples/BodyWeightSLR.html">bodyweight</a>,
<a
href="./Analyses/Linear%20Regression/Examples/carsSLR.html">cars</a></p>
</div>
<hr />
</div>
<div id="multiple-linear-regression"
class="section level2 tabset tabset-fade tabset-pills">
<h2 class="tabset tabset-fade tabset-pills">Multiple Linear
Regression</h2>
<div style="float:left;width:125px;" align="center">
<p><img src="Images/QuantYMultX.png" width=108px;></p>
</div>
<p>Multiple regression allows for more than one explanatory variable to
be included in the modeling of the expected value of the quantitative
response variable <span class="math inline">\(Y_i\)</span>. There are
infinitely many possible multiple regression models to choose from. Here
are a few “basic” models that work as building blocks to more
complicated models.</p>
<div id="overview-1" class="section level3">
<h3>Overview</h3>
<div style="padding-left:125px;">
<p>Select a model to see interpretation details, an example, and R Code
help.</p>
<div class="tab">
<p><button class="tablinks" onclick="openTab(event, 'LearnMoresimpleLinearModel')">Simple</button>
<button class="tablinks" onclick="openTab(event, 'LearnMoreQuadraticModel')">Quadratic</button>
<button class="tablinks" onclick="openTab(event, 'LearnMoreCubicModel')">Cubic</button>
<button class="tablinks" onclick="openTab(event, 'LearnMoreTwoLinesModel')">Two-Lines</button>
<button class="tablinks" onclick="openTab(event, 'LearnMorethreeDModel')">3D</button>
<button class="tablinks" onclick="openTab(event, 'LearnMoreHDModel')">HD</button></p>
</div>
<div id="LearnMoresimpleLinearModel" class="tabcontent"
style="display:block;">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-51-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
Y_i = \overbrace{\underbrace{\beta_0 + \beta_1
X_i}_{E\{Y_i\}}}^\text{Simple Model} + \epsilon_i
\]</span></p>
</td>
</tr>
</table>
<p><br/></p>
<p>The Simple Linear Regression model uses a single x-variable once:
<span class="math inline">\(X_i\)</span>.</p>
<table>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>Slope of the line</td>
</tr>
</tbody>
</table>
</p>
</div>
<div id="LearnMoreQuadraticModel" class="tabcontent">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-52-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_i + \beta_2
X_i^2}_{E\{Y_i\}}}^\text{Quadratic Model} + \epsilon_i
\]</span></p>
</td>
</tr>
</table>
<p><br/></p>
<p>The Quadratic model uses the same <span
class="math inline">\(X\)</span>-variable twice, once with a <span
class="math inline">\(\beta_1 X_i\)</span> term and once with a <span
class="math inline">\(\beta_2 X_i^2\)</span> term. The <span
class="math inline">\(X_i^2\)</span> term is called the “quadratic”
term.</p>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>Controls the x-position of the vertex of the parabola by <span
class="math inline">\(\frac{-\beta_1}{2\cdot\beta_2}\)</span>.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td>Controls the concavity and “steepness” of the Model: negative values
face down, positive values face up; large values imply “steeper”
parabolas and low values imply “flatter” parabolas. Also involved in the
position of the vertex, see <span
class="math inline">\(\beta_1\)</span>’s explanation.</td>
</tr>
</tbody>
</table>
<p><strong>An Example</strong></p>
<p>Using the <code>airquality</code> data set, we run the following
“quadratic” regression. Pay careful attention to how the mathematical
model for <span class="math inline">\(Y_i = \ldots\)</span> is
translated to R-Code inside of <code>lm(...)</code>.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{Temp} \underbrace{=}_{\sim}
\overbrace{\beta_0}^{\text{y-int}} +
\overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{term}}}
\underbrace{X_{i}}_\text{Month} \underbrace{+}_{+}
\overbrace{\beta_2}^{\stackrel{\text{quadratic}}{\text{term}}}  \underbrace{X_{i}^2}_\text{I(Month^2)}
+ \epsilon_i
\]</span></p>
<a href="javascript:showhide('quadraticregressionexamplecode')">
<div class="hoverchunk">
<p><span class="tooltipr"> lm.quad &lt;- <span class="tooltiprtext">A
name we made up for our “quadratic” regression.</span> </span><span
class="tooltipr"> lm( <span class="tooltiprtext">R function lm used to
perform linear regressions in R. The lm stands for “linear
model”.</span> </span><span class="tooltipr"> Temp <span
class="tooltiprtext">Y-variable, should be quantitative.</span>
</span><span class="tooltipr">  ~  <span class="tooltiprtext">The tilde
<code>~</code> is what lm(…) uses to state the regression equation <span
class="math inline">\(Y_i = ...\)</span>. Notice that the <code>~</code>
is not followed by <span class="math inline">\(\beta_0 +
\beta_1\)</span> like <span class="math inline">\(Y_i = ...\)</span>.
Instead, <span class="math inline">\(X_{i}\)</span> (Month in this case)
is the first term following <code>~</code>. This is because the <span
class="math inline">\(\beta\)</span>’s are going to be estimated by the
lm(…). These “Estimates” can be found using summary(lmObject) and
looking at the <strong>Estimates</strong> column in the output.</span>
</span><span class="tooltipr"> Month <span class="tooltiprtext"><span
class="math inline">\(X_{i}\)</span>, should be quantitative.</span>
</span><span class="tooltipr">  +  <span class="tooltiprtext">The plus
<code>+</code> is used between each term in the model. Note that only
the x-variables are included in the lm(…) from the <span
class="math inline">\(Y_i = ...\)</span> model. No beta’s are
included.</span> </span><span class="tooltipr"> I(Month^2) <span
class="tooltiprtext"><span class="math inline">\(X_{i}^2\)</span>, where
the function I(…) protects the squaring of Month from how lm(…) would
otherwise interpret that statement. The I(…) function must be used
anytime you raise an x-variable to a power in the lm(…)
statement.</span> </span><span class="tooltipr"> , data=airquality <span
class="tooltiprtext">This is the data set we are using for the
regression.</span> </span><span class="tooltipr"> )<br />
<span class="tooltiprtext">Closing parenthsis for the lm(…)
function.</span> </span><span class="tooltipr">     <br />
<span class="tooltiprtext">Press Enter to run the code.</span>
</span><span class="tooltipr" style="float:right;">  …  <span
class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
</a>
<div id="quadraticregressionexamplecode" style="display:none;">
<p>Pay special attention to how the lm(…) code uses <span
class="math inline">\(Y_i \sim X_{i} + X_{i}^2\)</span> and drops all
<span class="math inline">\(\beta\)</span>’s and <span
class="math inline">\(\epsilon\)</span> from the model statement. This
is because the estimates for the <span
class="math inline">\(\beta\)</span>’s and <span
class="math inline">\(\epsilon\)</span> are given by the output of the
lm(…) funtion in the “Estimates” column of summary(….) and in
<code>lmObject$residuals</code>.</p>
</div>
<pre class="r"><code>lm.quad &lt;- lm(Temp ~ Month + I(Month^2), data=airquality)
emphasize.strong.cols(1)
pander(summary(lm.quad)$coefficients, )</code></pre>
<table style="width:92%;">
<colgroup>
<col width="25%" />
<col width="18%" />
<col width="18%" />
<col width="13%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center"><strong>-95.73</strong></td>
<td align="center">15.24</td>
<td align="center">-6.281</td>
<td align="center">3.458e-09</td>
</tr>
<tr class="even">
<td align="center"><strong>Month</strong></td>
<td align="center"><strong>48.72</strong></td>
<td align="center">4.489</td>
<td align="center">10.85</td>
<td align="center">1.29e-20</td>
</tr>
<tr class="odd">
<td align="center"><strong>I(Month^2)</strong></td>
<td align="center"><strong>-3.283</strong></td>
<td align="center">0.3199</td>
<td align="center">-10.26</td>
<td align="center">4.737e-19</td>
</tr>
</tbody>
</table>
<p>The <strong>estimates</strong> shown in the summary output table
above approximate the <span class="math inline">\(\beta\)</span>’s in
the regression model:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is estimated by the
(Intercept) value of -95.73,</li>
<li><span class="math inline">\(\beta_1\)</span> is estimated by the
<code>Month</code> value of 48.72, and</li>
<li><span class="math inline">\(\beta_2\)</span> is estimated by the
<code>I(Month^2)</code> value of -3.283.</li>
</ul>
<p>Because the estimate of the <span
class="math inline">\(\beta_2\)</span> term is negative (-3.283), this
parabola will “open down” (concave). This tells us that average
temperatures will increase to a point, then decrease again. The vertex
of this parabola will be at <span class="math inline">\(-b_1/(2b_2) =
-(48.72)/(2\cdot (-3.283)) = 7.420043\)</span> months, which tells us
that the highest average temperature will occur around mid July (7.42
months to be exact). The y-intercept is -95.73, which would be awfully
cold if it were possible for the month to be “month zero.” Since this is
not possible, the y-intercept is not meaningful for this model.</p>
<p>Note that interpreting either <span
class="math inline">\(\beta_1\)</span> or <span
class="math inline">\(\beta_2\)</span> by themselves is quite difficult
because they both work with together with <span
class="math inline">\(X_{i}\)</span>.</p>
<p><span class="math display">\[
\hat{Y}_i = \overbrace{-95.73}^\text{y-int} +
\overbrace{48.72}^{\stackrel{\text{slope}}{\text{term}}} X_{i} +
\overbrace{-3.283}^{\stackrel{\text{quadratic}}{\text{term}}} X_{i}^2
\]</span></p>
<p>The regression function is drawn as follows. Be sure to look at the
“Code” to understand how this graph was created using the ideas in the
equation above.</p>
<table>
<tr>
<td>
<p><strong>Using Base R</strong></p>
<pre class="r"><code>plot(Temp ~ Month, data=airquality, col=&quot;skyblue&quot;, pch=21, bg=&quot;gray83&quot;, main=&quot;Quadratic Model using airquality data set&quot;, cex.main=1)

#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.quad)
# Then b will have 3 numbers stored inside:
# b[1] is the estimate of beta_0: -95.73
# b[2] is the estimate of beta_1: 48.72
# b[3] is the estimate of beta_2: -3.28
curve(b[1] + b[2]*x + b[3]*x^2, col=&quot;skyblue&quot;, lwd=2, add=TRUE)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
</td>
<td>
<p><strong>Using ggplot2</strong></p>
<pre class="r"><code>#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.quad)
# Then b will have 3 estimates:
# b[1] is the estimate of beta_0: 35.38
# b[2] is the estimate of beta_1: -7.099
# b[3] is the estimate of beta_2: 0.4759

ggplot(airquality, aes(y=Temp, x=Month)) +
  geom_point(pch=21, bg=&quot;gray83&quot;, color=&quot;skyblue&quot;) +
  #geom_smooth(method=&quot;lm&quot;, se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic.
  stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, color=&quot;skyblue&quot;) +
  labs(title=&quot;Quadratic Model using airquality data set&quot;) </code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
</td>
</tr>
</table>
</p>
</div>
<div id="LearnMoreCubicModel" class="tabcontent" style="display:none;">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-56-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_i + \beta_2 X_i^2 +
\beta_3 X_i^3}_{E\{Y_i\}}}^\text{Cubic Model} + \epsilon_i
\]</span></p>
</td>
</tr>
</table>
<p><br/></p>
<p>The Cubic model uses the same <span
class="math inline">\(X\)</span>-variable thrice, once with a <span
class="math inline">\(\beta_1 X_i\)</span> term, once with a <span
class="math inline">\(\beta_2 X_i^2\)</span> term, and once with a <span
class="math inline">\(\beta_3 X_i^3\)</span> term. The <span
class="math inline">\(X_i^3\)</span> term is called the “cubic”
term.</p>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>No clear interpretation, but could be called the “base slope
coefficient” and contributes to the position of the inflection points of
the cubic function.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td>No clear interpretation, but it also contributes to the location of
the inflection points.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_3\)</span></td>
<td>This is the coefficient of the cubic term. No clear interpretation,
but it determines the concavity of the model by its sign.</td>
</tr>
</tbody>
</table>
<p><strong>An Example</strong></p>
<p>Using the <code>CO2</code> data set, we run the following “cubic”
regression.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{uptake} \underbrace{=}_{\sim}
\overbrace{\beta_0}^{\text{y-int}} +
\overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{term}}}
\underbrace{X_{i}}_\text{conc} +
\overbrace{\beta_2}^{\stackrel{\text{quadratic}}{\text{term}}}  \underbrace{X_{i}^2}_\text{I(conc^2)}
+  \overbrace{\beta_3}^{\stackrel{\text{cubic}}{\text{term}}}  \underbrace{X_{i}^3}_\text{I(conc^3)}
+ \epsilon_i
\]</span></p>
<a href="javascript:showhide('cubicregressionexamplecode')">
<div class="hoverchunk">
<p><span class="tooltipr"> lm.cubic &lt;- <span class="tooltiprtext">A
name we made up for our “cubic” regression.</span> </span><span
class="tooltipr"> lm( <span class="tooltiprtext">R function lm used to
perform linear regressions in R. The lm stands for “linear
model”.</span> </span><span class="tooltipr"> uptake <span
class="tooltiprtext">Y-variable, should be quantitative.</span>
</span><span class="tooltipr">  ~  <span class="tooltiprtext">The tilde
<code>~</code> is what lm(…) uses to state the regression equation <span
class="math inline">\(Y_i = ...\)</span>. Notice that the <code>~</code>
is not followed by <span class="math inline">\(\beta_0 +
\beta_1\)</span> like <span class="math inline">\(Y_i = ...\)</span>.
Instead, <span class="math inline">\(X_i\)</span> is the first term
following <code>~</code>. This is because the <span
class="math inline">\(\beta\)</span>’s are going to be estimated by the
lm(…). These estimates can be found using summary(lmObject).</span>
</span><span class="tooltipr"> conc <span class="tooltiprtext"><span
class="math inline">\(X_{i}\)</span>, should be quantitative.</span>
</span><span class="tooltipr">  +  <span class="tooltiprtext">The plus
<code>+</code> is used between each term in the model. Note that only
the x-variables are included in the lm(…) from the <span
class="math inline">\(Y_i = ...\)</span> model. No beta’s are
included.</span> </span><span class="tooltipr"> I(conc^2) <span
class="tooltiprtext"><span class="math inline">\(X_{i}^2\)</span>, where
the function I(…) protects the squaring of conc from how lm(…) would
otherwise interpret that statement. The I(…) function must be used
anytime you raise an x-variable to a power in the lm(…)
statement.</span> </span><span class="tooltipr">  +  <span
class="tooltiprtext">The plus <code>+</code> is used between each term
in the model. Note that only the x-variables are included in the lm(…)
from the <span class="math inline">\(Y_i = ...\)</span> model. No beta’s
are included.</span> </span><span class="tooltipr"> I(conc^3) <span
class="tooltiprtext"><span class="math inline">\(X_{i}^3\)</span>, where
the function I(…) protects the cubing of conc from how lm(…) would
otherwise interpret that statement. The I(…) function must be used
anytime you raise an x-variable to a power in the lm(…)
statement.</span> </span><span class="tooltipr"> , data=CO2 <span
class="tooltiprtext">This is the data set we are using for the
regression.</span> </span><span class="tooltipr"> )<br />
<span class="tooltiprtext">Closing parenthsis for the lm(…)
function.</span> </span><span class="tooltipr">     <br />
<span class="tooltiprtext">Press Enter to run the code.</span>
</span><span class="tooltipr" style="float:right;">  …  <span
class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
</a>
<div id="cubicregressionexamplecode" style="display:none;">
<p>Pay special attention to how the lm(…) code uses <span
class="math inline">\(Y_i \sim X_{i} + X_{i}^2\)</span> and drops all
<span class="math inline">\(\beta\)</span>’s and <span
class="math inline">\(\epsilon\)</span> from the model statement. This
is because the estimates for the <span
class="math inline">\(\beta\)</span>’s and <span
class="math inline">\(\epsilon\)</span> are given by the output of the
lm(…) funtion in the “Estimates” column of summary(….) and in
<code>lmObject$residuals</code>.</p>
</div>
<pre class="r"><code>lm.cubic &lt;- lm(uptake ~ conc + I(conc^2) + I(conc^3), data=CO2)
pander(summary(lm.cubic)$coefficients)</code></pre>
<table style="width:90%;">
<colgroup>
<col width="25%" />
<col width="18%" />
<col width="18%" />
<col width="13%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">-1.483</td>
<td align="center">5.043</td>
<td align="center">-0.2941</td>
<td align="center">0.7694</td>
</tr>
<tr class="even">
<td align="center"><strong>conc</strong></td>
<td align="center">0.1814</td>
<td align="center">0.0416</td>
<td align="center">4.36</td>
<td align="center">3.83e-05</td>
</tr>
<tr class="odd">
<td align="center"><strong>I(conc^2)</strong></td>
<td align="center">-0.0003063</td>
<td align="center">9.067e-05</td>
<td align="center">-3.378</td>
<td align="center">0.00113</td>
</tr>
<tr class="even">
<td align="center"><strong>I(conc^3)</strong></td>
<td align="center">1.601e-07</td>
<td align="center">5.512e-08</td>
<td align="center">2.905</td>
<td align="center">0.004745</td>
</tr>
</tbody>
</table>
<p>The <strong>estimates</strong> shown above approximate the <span
class="math inline">\(\beta\)</span>’s in the regression model: <span
class="math inline">\(\beta_0\)</span> is estimated by the (Intercept)
value of -1.483, <span class="math inline">\(\beta_1\)</span> is
estimated by the <code>conc</code> value of 0.1814, <span
class="math inline">\(\beta_2\)</span> is estimated by the
<code>I(conc^2)</code> value of -0.0003063, and <span
class="math inline">\(\beta_3\)</span> is estimated by the
<code>I(conc^3)</code> value of 1.601e-07, which translates to
0.0000001601.</p>
<p>Because the estimate of the <span
class="math inline">\(\beta_3\)</span> term is positive, this cubic
model will “open up”. In other words, as the function moves from left to
right, it will go off to positive infinity (up). If the term would have
been negative, then the function would head to negative infinity (down)
instead.</p>
<p><span class="math display">\[
\hat{Y}_i = \overbrace{-1.483}^\text{y-int} +
\overbrace{0.1814}^{\stackrel{\text{slope}}{\text{term}}} X_{i} +
\overbrace{-0.0003063}^{\stackrel{\text{quadratic}}{\text{term}}}
X_{i}^2 + \overbrace{1.601e-07}^{\stackrel{\text{cubic}}{\text{term}}}
X_{i}^3
\]</span></p>
<p>The regression function is drawn as follows. Be sure to look at the
“Code” to understand how this graph was created using the ideas in the
equation above.</p>
<table>
<tr>
<td>
<p><strong>Using Base R</strong></p>
<pre class="r"><code>plot(uptake ~ conc, data=CO2, col=&quot;skyblue&quot;, pch=21, bg=&quot;gray83&quot;, main=&quot;Cubic Model using CO2 data set&quot;, cex.main=1)

#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.cubic)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -1.483
# b[2] is the estimate of beta_1: 0.1814
# b[3] is the estimate of beta_2: -0.0003063
# b[4] is the estimate of beta_3: 1.601e-07
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=&quot;skyblue&quot;, lwd=2, add=TRUE)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
</td>
<td>
<p><strong>Using ggplot2</strong></p>
<pre class="r"><code>#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.cubic)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -1.483
# b[2] is the estimate of beta_1: 0.1814
# b[3] is the estimate of beta_2: -0.0003063
# b[4] is the estimate of beta_3: 1.601e-07

ggplot(CO2, aes(y=uptake, x=conc)) +
  geom_point(pch=21, bg=&quot;gray83&quot;, color=&quot;skyblue&quot;) +
  #geom_smooth(method=&quot;lm&quot;, se=F, formula = y ~ poly(x, 3)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic.
  stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, color=&quot;skyblue&quot;) +
  labs(title=&quot;Cubic Model using CO2 data set&quot;) </code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
</td>
</tr>
</table>
<p>It should be stated, that the cubic function is not the best fit for
this data. However, it is a lot better than just a simple line, or a
quadratic model, as shown below.</p>
<pre class="r"><code>plot(uptake ~ conc, data=CO2, col=&quot;skyblue&quot;, pch=21, bg=&quot;gray83&quot;, main=&quot;Cubic Model using CO2 data set&quot;, cex.main=1)

#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.cubic)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -1.483
# b[2] is the estimate of beta_1: 0.1814
# b[3] is the estimate of beta_2: -0.0003063
# b[4] is the estimate of beta_3: 1.601e-07
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, col=&quot;skyblue&quot;, lwd=2, add=TRUE)
b &lt;- coef(lm(uptake ~ conc + I(conc^2), data=CO2))
curve(b[1] + b[2]*x + b[3]*x^2, col=&quot;firebrick&quot;, lwd=2, add=TRUE)
b &lt;- coef(lm(uptake ~ conc, data=CO2))
curve(b[1] + b[2]*x, col=&quot;orange&quot;, lwd=2, add=TRUE)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
</p>
</div>
<div id="LearnMoreTwoLinesModel" class="tabcontent">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-61-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} +
\beta_3 X_{1i} X_{2i}}_{E\{Y_i\}}}^\text{Two-lines Model} + \epsilon_i
\]</span></p>
<p><span class="math display">\[
X_{2i} = \left\{\begin{array}{ll} 1, &amp; \text{Group B} \\ 0, &amp;
\text{Group A} \end{array}\right.
\]</span></p>
</td>
</tr>
</table>
<p>The so called “two-lines” model uses a quantitative <span
class="math inline">\(X_{1i}\)</span> variable and a 0,1 indicator
variable <span class="math inline">\(X_{2i}\)</span>. It is a basic
example of how a “dummy variable” or “indicator variable” can be used to
turn qualitative variables into quantitative terms. In this case, the
indicator variable <span class="math inline">\(X_{2i}\)</span>, which is
either 0 or 1, produces two separate lines: one line for Group A, and
one line for Group B.</p>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>Controls the slope of the “base-line” of the model, the “Group 0”
line.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td>Controls the <strong>change in y-intercept</strong> for the second
line in the model as compared to the y-intercept of the “base-line”
line.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_3\)</span></td>
<td>Called the “interaction” term. Controls the <strong>change in the
slope</strong> for the second line in the model as compared to the slope
of the “base-line” line.</td>
</tr>
</tbody>
</table>
<p><strong>An Example</strong></p>
<p>Using the <code>mtcars</code> data set, we run the following
“two-lines” regression. Note that <code>am</code> has only 0 or 1
values: <code>View(mtcars)</code>.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{mpg} \underbrace{=}_{\sim}
\overbrace{\beta_0}^{\stackrel{\text{y-int}}{\text{baseline}}} +
\overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{baseline}}}
\underbrace{X_{1i}}_\text{qsec} +
\overbrace{\beta_2}^{\stackrel{\text{change
in}}{\text{y-int}}}  \underbrace{X_{2i}}_\text{am} +
\overbrace{\beta_3}^{\stackrel{\text{change in}}{\text{slope}}}
\underbrace{X_{1i}X_{2i}}_\text{qsec:am} + \epsilon_i
\]</span></p>
<a href="javascript:showhide('twolinesregressionexamplecode')">
<div class="hoverchunk">
<p><span class="tooltipr"> lm.2lines &lt;- <span class="tooltiprtext">A
name we made up for our “two-lines” regression.</span> </span><span
class="tooltipr"> lm( <span class="tooltiprtext">R function lm used to
perform linear regressions in R. The lm stands for “linear
model”.</span> </span><span class="tooltipr"> mpg <span
class="tooltiprtext">Y-variable, should be quantitative.</span>
</span><span class="tooltipr">  ~  <span class="tooltiprtext">The tilde
<code>~</code> is what lm(…) uses to state the regression equation <span
class="math inline">\(Y_i = ...\)</span>. Notice that the <code>~</code>
is not followed by <span class="math inline">\(\beta_0 +
\beta_1\)</span> like <span class="math inline">\(Y_i = ...\)</span>.
Instead, <span class="math inline">\(X_{1i}\)</span> is the first term
following <code>~</code>. This is because <span
class="math inline">\(\beta\)</span>’s are going to be estimated by the
lm(…). These estimates can be found using summary(lmObject).</span>
</span><span class="tooltipr"> qsec <span class="tooltiprtext"><span
class="math inline">\(X_{1i}\)</span>, should be quantitative.</span>
</span><span class="tooltipr">  +  <span class="tooltiprtext">The plus
<code>+</code> is used between each term in the model. Note that only
the x-variables are included in the lm(…) from the <span
class="math inline">\(Y_i = ...\)</span> model. No beta’s are
included.</span> </span><span class="tooltipr"> am <span
class="tooltiprtext"><span class="math inline">\(X_{2i}\)</span>, an
indicator or 0,1 variable. This term allows the y-intercept of the two
lines to differ.</span> </span><span class="tooltipr">  +  <span
class="tooltiprtext">The plus <code>+</code> is used between each term
in the model. Note that only the x-variables are included in the lm(…)
from the <span class="math inline">\(Y_i = ...\)</span> model. No beta’s
are included.</span> </span><span class="tooltipr"> qsec:am <span
class="tooltiprtext"><span class="math inline">\(X_{1i}X_{2i}\)</span>
the interaction term. This allows the slopes of the two lines to
differ.</span> </span><span class="tooltipr"> , data=mtcars <span
class="tooltiprtext">This is the data set we are using for the
regression.</span> </span><span class="tooltipr"> )<br />
<span class="tooltiprtext">Closing parenthsis for the lm(…)
function.</span> </span><span class="tooltipr">     <br />
<span class="tooltiprtext">Press Enter to run the code.</span>
</span><span class="tooltipr" style="float:right;">  …  <span
class="tooltiprtext">Click to View Output.</span> </span></p>
</div>
</a>
<div id="twolinesregressionexamplecode" style="display:none;">
<p>Pay special attention to how the lm(…) code uses <span
class="math inline">\(Y_i \sim X_{1i} + X_{2i} + X_{1i}X_{2i}\)</span>
and drops all <span class="math inline">\(\beta\)</span>’s and <span
class="math inline">\(\epsilon\)</span> from the model statement. This
is because the estimates for the <span
class="math inline">\(\beta\)</span>’s and <span
class="math inline">\(\epsilon\)</span> are given by the output of the
lm(…) funtion in the “Estimates” column of summary(….) and in
<code>lm.2lines$residuals</code>.</p>
</div>
<pre class="r"><code>lm.2lines &lt;- lm(mpg ~ qsec + am + qsec:am, data=mtcars)
pander(summary(lm.2lines)$coefficients)</code></pre>
<table style="width:88%;">
<colgroup>
<col width="25%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">-9.01</td>
<td align="center">8.218</td>
<td align="center">-1.096</td>
<td align="center">0.2823</td>
</tr>
<tr class="even">
<td align="center"><strong>qsec</strong></td>
<td align="center">1.439</td>
<td align="center">0.45</td>
<td align="center">3.197</td>
<td align="center">0.003432</td>
</tr>
<tr class="odd">
<td align="center"><strong>am</strong></td>
<td align="center">-14.51</td>
<td align="center">12.48</td>
<td align="center">-1.163</td>
<td align="center">0.2548</td>
</tr>
<tr class="even">
<td align="center"><strong>qsec:am</strong></td>
<td align="center">1.321</td>
<td align="center">0.7017</td>
<td align="center">1.883</td>
<td align="center">0.07012</td>
</tr>
</tbody>
</table>
<p>The <strong>estimates</strong> shown above approximate the <span
class="math inline">\(\beta\)</span>’s in the regression model: <span
class="math inline">\(\beta_0\)</span> is estimated by the (Intercept),
<span class="math inline">\(\beta_1\)</span> is estimated by the
<code>qsec</code> value of 1.439, <span
class="math inline">\(\beta_2\)</span> is estimated by the
<code>am</code> value of -14.51, and <span
class="math inline">\(\beta_3\)</span> is estimated by the
<code>qsec:am</code> value of 1.321.</p>
<p>This gives two separate equations of lines.</p>
<p><strong>Automatic Transmission (am==0, <span
class="math inline">\(X_{2i} = 0\)</span>) Line</strong></p>
<p><span class="math display">\[
\hat{Y}_i = \overbrace{-9.01}^{\stackrel{\text{y-int}}{\text{baseline}}}
+ \overbrace{1.439}^{\stackrel{\text{slope}}{\text{baseline}}} X_{1i}
\]</span></p>
<p><strong>Manual Transmission (am==1 , <span
class="math inline">\(X_{2i} = 1\)</span>) Line</strong></p>
<p><span class="math display">\[
\hat{Y}_i =
\underbrace{(\overbrace{-9.01}^{\stackrel{\text{y-int}}{\text{baseline}}}
+ \overbrace{-14.51}^{\stackrel{\text{change
in}}{\text{y-int}}})}_{\stackrel{\text{y-intercept}}{-23.52}} +
\underbrace{(\overbrace{1.439}^{\stackrel{\text{slope}}{\text{baseline}}}
+\overbrace{1.321}^{\stackrel{\text{change
in}}{\text{slope}}})}_{\stackrel{\text{slope}}{2.76}} X_{1i}
\]</span></p>
<p>These lines are drawn as follows. Be sure to look at the “Code” to
understand how this graph was created using the ideas in the two
equations above.</p>
<table>
<tr>
<td>
<p><strong>Using Base R</strong></p>
<pre class="r"><code>plot(mpg ~ qsec, data=mtcars, col=c(&quot;skyblue&quot;,&quot;orange&quot;)[as.factor(am)], pch=21, bg=&quot;gray83&quot;, main=&quot;Two-lines Model using mtcars data set&quot;, cex.main=1)

legend(&quot;topleft&quot;, legend=c(&quot;Baseline (am==0)&quot;, &quot;Changed-line (am==1)&quot;), bty=&quot;n&quot;, lty=1, col=c(&quot;skyblue&quot;,&quot;orange&quot;), cex=0.8)

#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.2lines)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -9.0099
# b[2] is the estimate of beta_1:  1.4385
# b[3] is the estimate of beta_2: -14.5107
# b[4] is the estimate of beta_3: 1.3214
curve(b[1] + b[2]*x, col=&quot;skyblue&quot;, lwd=2, add=TRUE)  #baseline (in blue)
curve((b[1] + b[3]) + (b[2] + b[4])*x, col=&quot;orange&quot;, lwd=2, add=TRUE) #changed line (in orange)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
</td>
<td>
<p><strong>Using ggplot2</strong></p>
<pre class="r"><code>#get the &quot;Estimates&quot; automatically:
b &lt;- coef(lm.2lines)
# Then b will have 4 estimates:
# b[1] is the estimate of beta_0: -9.0099
# b[2] is the estimate of beta_1:  1.4385
# b[3] is the estimate of beta_2: -14.5107
# b[4] is the estimate of beta_3: 1.3214

ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) +
  geom_point(pch=21, bg=&quot;gray83&quot;) +
  #geom_smooth(method=&quot;lm&quot;, se=F) + #easy way, but only draws the full interaction model. The manual way using stat_function (see below) is more involved, but more dynamic.
  stat_function(fun = function(x) b[1] + b[2]*x, color=&quot;skyblue&quot;) + #am==0 line
  stat_function(fun = function(x) (b[1]+b[3]) + (b[2]+b[4])*x,color=&quot;orange&quot;) + #am==1 line 
  scale_color_manual(name=&quot;Transmission (am)&quot;, values=c(&quot;skyblue&quot;,&quot;orange&quot;)) +
  labs(title=&quot;Two-lines Model using mtcars data set&quot;) </code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-64-1.png" width="672" /></p>
</td>
</tr>
</table>
</p>
</div>
<div id="LearnMorethreeDModel" class="tabcontent">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/volcano-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} +
\beta_3 X_{1i}X_{2i}}_{E\{Y_i\}}}^\text{3D Model} + \epsilon_i
\]</span></p>
</td>
</tr>
</table>
<p>The so called “3D” regression model uses two different quantitative
x-variables, an <span class="math inline">\(X_{1i}\)</span> and an <span
class="math inline">\(X_{2i}\)</span>. Unlike the two-lines model where
<span class="math inline">\(X_{2i}\)</span> could only be a 0 or a 1,
this <span class="math inline">\(X_{2i}\)</span> variable is
quantitative, and can take on any quantitative value.</p>
<table>
<colgroup>
<col width="12%" />
<col width="87%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>Slope of the line in the <span class="math inline">\(X_1\)</span>
direction.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td>Slope of the line in the <span class="math inline">\(X_2\)</span>
direction.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_3\)</span></td>
<td>Interaction term that allows the model, which is a plane in
three-dimensional space, to “bend”. If this term is zero, then the
regression surface is just a flat plane.</td>
</tr>
</tbody>
</table>
<p><strong>An Example</strong></p>
<p>Here is what a 3D regression looks like when there is no interaction
term. The two x-variables of <code>Month</code> and <code>Temp</code>
are being used to predict the y-variable of <code>Ozone</code>.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{Ozone} \underbrace{=}_{\sim}
\overbrace{\beta_0}^{\stackrel{\text{y-int}}{\text{baseline}}} +
\overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{baseline}}}
\underbrace{X_{1i}}_\text{Temp} +
\overbrace{\beta_2}^{\stackrel{\text{change
in}}{\text{y-int}}}  \underbrace{X_{2i}}_\text{Month} + \epsilon_i
\]</span></p>
<pre class="r"><code>air_lm &lt;- lm(Ozone ~ Temp + Month, data= airquality)
pander(air_lm$coefficients)</code></pre>
<table style="width:43%;">
<colgroup>
<col width="19%" />
<col width="11%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">(Intercept)</th>
<th align="center">Temp</th>
<th align="center">Month</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-139.6</td>
<td align="center">2.659</td>
<td align="center">-3.522</td>
</tr>
</tbody>
</table>
<p>Notice how the slope, <span class="math inline">\(\beta_1\)</span>,
in the “Temp” direction is estimated to be 2.659 and the slope in the
“Month” direction, <span class="math inline">\(\beta_2\)</span>, is
estimated to be -3.522. Also, the y-intercept, <span
class="math inline">\(\beta_0\)</span>, is estimated to be -139.6.</p>
<pre class="r"><code>## Hint: library(car) has a scatterplot 3d function which is simple to use
#  but the code should only be run in your console, not knit.

## library(car)
## scatter3d(Y ~ X1 + X2, data=yourdata)



## To embed the 3d-scatterplot inside of your html document is harder.
#library(plotly)
#library(reshape2)

#Perform the multiple regression
air_lm &lt;- lm(Ozone ~ Temp + Month, data= airquality)

#Graph Resolution (more important for more complex shapes)
graph_reso &lt;- 0.5

#Setup Axis
axis_x &lt;- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso)
axis_y &lt;- seq(min(airquality$Month), max(airquality$Month), by = graph_reso)

#Sample points
air_surface &lt;- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F)
air_surface$Z &lt;- predict.lm(air_lm, newdata = air_surface)
air_surface &lt;- acast(air_surface, Month ~ Temp, value.var = &quot;Z&quot;) #y ~ x

#Create scatterplot
plot_ly(airquality, 
        x = ~Temp, 
        y = ~Month, 
        z = ~Ozone,
        text = rownames(airquality), 
        type = &quot;scatter3d&quot;, 
        mode = &quot;markers&quot;) %&gt;%
  add_trace(z = air_surface,
            x = axis_x,
            y = axis_y,
            type = &quot;surface&quot;)</code></pre>
<div class="plotly html-widget html-fill-item" id="htmlwidget-fa4c6b8013297568dd7a" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-fa4c6b8013297568dd7a">{"x":{"visdat":{"1e9875aa283e":["function () ","plotlyVisDat"]},"cur_data":"1e9875aa283e","attrs":{"1e9875aa283e":{"x":{},"y":{},"z":{},"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d"},"1e9875aa283e.1":{"x":[56,56.5,57,57.5,58,58.5,59,59.5,60,60.5,61,61.5,62,62.5,63,63.5,64,64.5,65,65.5,66,66.5,67,67.5,68,68.5,69,69.5,70,70.5,71,71.5,72,72.5,73,73.5,74,74.5,75,75.5,76,76.5,77,77.5,78,78.5,79,79.5,80,80.5,81,81.5,82,82.5,83,83.5,84,84.5,85,85.5,86,86.5,87,87.5,88,88.5,89,89.5,90,90.5,91,91.5,92,92.5,93,93.5,94,94.5,95,95.5,96,96.5,97],"y":[5,5.5,6,6.5,7,7.5,8,8.5,9],"z":[[-8.2930156346033321,-6.9632802100885804,-5.6335447855738288,-4.3038093610590771,-2.9740739365443254,-1.6443385120295737,-0.31460308751482202,1.0151323369999297,2.3448677615146813,3.674603186029433,5.0043386105441847,6.3340740350589364,7.6638094595736881,8.9935448840884398,10.323280308603191,11.653015733117943,12.982751157632695,14.312486582147447,15.642222006662198,16.97195743117695,18.301692855691702,19.631428280206453,20.961163704721205,22.290899129235957,23.620634553750708,24.95036997826546,26.280105402780212,27.609840827294963,28.939576251809715,30.269311676324467,31.599047100839218,32.928782525353967,34.258517949868718,35.588253374383441,36.917988798898193,38.247724223412945,39.577459647927697,40.907195072442448,42.2369304969572,43.566665921471952,44.896401345986703,46.226136770501455,47.555872195016207,48.885607619530958,50.21534304404571,51.545078468560462,52.874813893075213,54.204549317589965,55.534284742104717,56.864020166619468,58.19375559113422,59.523491015648972,60.853226440163724,62.182961864678475,63.512697289193227,64.842432713707979,66.17216813822273,67.501903562737482,68.831638987252234,70.161374411766985,71.491109836281737,72.820845260796489,74.15058068531124,75.480316109825992,76.810051534340715,78.139786958855467,79.469522383370219,80.79925780788497,82.128993232399722,83.458728656914474,84.788464081429225,86.118199505943977,87.447934930458729,88.77767035497348,90.107405779488232,91.437141204002984,92.766876628517736,94.096612053032487,95.426347477547239,96.756082902061991,98.085818326576742,99.415553751091494,100.74528917560625],[-10.053971968669721,-8.7242365441549694,-7.3945011196402177,-6.064765695125466,-4.7350302706107144,-3.4052948460959627,-2.075559421581211,-0.7458239970664593,0.58391142744829239,1.9136468519630441,3.2433822764777958,4.5731177009925474,5.9028531255072991,7.2325885500220508,8.5623239745368025,9.8920593990515542,11.221794823566306,12.551530248081058,13.881265672595809,15.211001097110561,16.540736521625313,17.870471946140064,19.200207370654816,20.529942795169568,21.859678219684319,23.189413644199071,24.519149068713823,25.848884493228574,27.178619917743326,28.508355342258078,29.838090766772829,31.167826191287581,32.497561615802333,33.827297040317056,35.157032464831808,36.486767889346559,37.816503313861311,39.146238738376063,40.475974162890815,41.805709587405566,43.135445011920318,44.46518043643507,45.794915860949821,47.124651285464573,48.454386709979325,49.784122134494076,51.113857559008828,52.44359298352358,53.773328408038331,55.103063832553083,56.432799257067835,57.762534681582586,59.092270106097338,60.42200553061209,61.751740955126841,63.081476379641593,64.411211804156352,65.740947228671104,67.070682653185855,68.400418077700607,69.730153502215359,71.05988892673011,72.389624351244862,73.719359775759614,75.049095200274337,76.378830624789089,77.70856604930384,79.038301473818592,80.368036898333344,81.697772322848095,83.027507747362847,84.357243171877599,85.68697859639235,87.016714020907102,88.346449445421854,89.676184869936606,91.005920294451357,92.335655718966109,93.665391143480861,94.995126567995612,96.324861992510364,97.654597417025116,98.984332841539867],[-11.81492830273611,-10.485192878221358,-9.1554574537066067,-7.825722029191855,-6.4959866046771033,-5.1662511801623516,-3.8365157556475999,-2.5067803311328483,-1.1770449066180966,0.15269051789665511,1.4824259424114068,2.8121613669261585,4.1418967914409102,5.4716322159556618,6.8013676404704135,8.1311030649851652,9.4608384894999169,10.790573914014669,12.12030933852942,13.450044763044172,14.779780187558924,16.109515612073675,17.439251036588427,18.768986461103179,20.09872188561793,21.428457310132682,22.758192734647434,24.087928159162185,25.417663583676937,26.747399008191689,28.07713443270644,29.406869857221192,30.736605281735944,32.066340706250671,33.396076130765422,34.725811555280174,36.055546979794926,37.385282404309677,38.715017828824429,40.044753253339181,41.374488677853932,42.704224102368684,44.033959526883436,45.363694951398188,46.693430375912939,48.023165800427691,49.352901224942443,50.682636649457194,52.012372073971946,53.342107498486698,54.671842923001449,56.001578347516201,57.331313772030953,58.661049196545704,59.990784621060456,61.320520045575208,62.650255470089959,63.979990894604711,65.309726319119463,66.639461743634214,67.969197168148966,69.298932592663718,70.62866801717847,71.958403441693221,73.288138866207944,74.617874290722696,75.947609715237448,77.2773451397522,78.607080564266951,79.936815988781703,81.266551413296455,82.596286837811206,83.926022262325958,85.25575768684071,86.585493111355461,87.915228535870213,89.244963960384965,90.574699384899716,91.904434809414468,93.23417023392922,94.563905658443971,95.893641082958723,97.223376507473475],[-13.575884636802499,-12.246149212287747,-10.916413787772996,-9.586678363258244,-8.2569429387434923,-6.9272075142287406,-5.5974720897139889,-4.2677366651992372,-2.9380012406844855,-1.6082658161697339,-0.27853039165498217,1.0512050328597695,2.3809404573745212,3.7106758818892729,5.0404113064040246,6.3701467309187763,7.6998821554335279,9.0296175799482796,10.359353004463031,11.689088428977783,13.018823853492535,14.348559278007286,15.678294702522038,17.00803012703679,18.337765551551541,19.667500976066293,20.997236400581045,22.326971825095796,23.656707249610548,24.9864426741253,26.316178098640052,27.645913523154803,28.975648947669555,30.305384372184278,31.63511979669903,32.964855221213782,34.294590645728533,35.624326070243285,36.954061494758037,38.283796919272788,39.61353234378754,40.943267768302292,42.273003192817043,43.602738617331795,44.932474041846547,46.262209466361298,47.59194489087605,48.921680315390802,50.251415739905553,51.581151164420305,52.910886588935057,54.240622013449808,55.57035743796456,56.900092862479312,58.229828286994064,59.559563711508815,60.889299136023567,62.219034560538319,63.54876998505307,64.878505409567822,66.208240834082574,67.537976258597325,68.867711683112077,70.197447107626829,71.527182532141552,72.856917956656304,74.186653381171055,75.516388805685807,76.846124230200559,78.17585965471531,79.505595079230062,80.835330503744814,82.165065928259565,83.494801352774317,84.824536777289069,86.154272201803821,87.484007626318572,88.813743050833324,90.143478475348076,91.473213899862827,92.802949324377579,94.132684748892331,95.462420173407082],[-15.336840970868888,-14.007105546354136,-12.677370121839385,-11.347634697324633,-10.017899272809881,-8.6881638482951296,-7.3584284237803779,-6.0286929992656262,-4.6989575747508745,-3.3692221502361228,-2.0394867257213711,-0.70975130120661944,0.61998412330813224,1.9497195478228839,3.2794549723376356,4.6091903968523873,5.938925821367139,7.2686612458818907,8.5983966703966423,9.928132094911394,11.257867519426146,12.587602943940897,13.917338368455649,15.247073792970401,16.576809217485152,17.906544641999904,19.236280066514656,20.566015491029408,21.895750915544159,23.225486340058911,24.555221764573663,25.884957189088414,27.214692613603166,28.544428038117889,29.874163462632641,31.203898887147393,32.533634311662141,33.863369736176892,35.193105160691644,36.522840585206396,37.852576009721147,39.182311434235899,40.512046858750651,41.841782283265402,43.171517707780154,44.501253132294906,45.830988556809658,47.160723981324409,48.490459405839161,49.820194830353913,51.149930254868664,52.479665679383416,53.809401103898168,55.139136528412919,56.468871952927671,57.798607377442423,59.128342801957174,60.458078226471926,61.787813650986678,63.117549075501429,64.447284500016181,65.777019924530933,67.106755349045685,68.436490773560436,69.766226198075159,71.095961622589911,72.425697047104663,73.755432471619415,75.085167896134166,76.414903320648918,77.74463874516367,79.074374169678421,80.404109594193173,81.733845018707925,83.063580443222676,84.393315867737428,85.72305129225218,87.052786716766931,88.382522141281683,89.712257565796435,91.041992990311186,92.371728414825938,93.70146383934069],[-17.097797304935277,-15.768061880420525,-14.438326455905774,-13.108591031391022,-11.77885560687627,-10.449120182361519,-9.1193847578467668,-7.7896493333320151,-6.4599139088172635,-5.1301784843025118,-3.8004430597877601,-2.4707076352730084,-1.1409722107582567,0.18876321375649496,1.5184986382712466,2.8482340627859983,4.17796948730075,5.5077049118155017,6.8374403363302534,8.1671757608450051,9.4969111853597568,10.826646609874508,12.15638203438926,13.486117458904012,14.815852883418763,16.145588307933515,17.475323732448267,18.805059156963019,20.13479458147777,21.464530005992522,22.794265430507274,24.124000855022025,25.453736279536777,26.7834717040515,28.113207128566252,29.442942553081004,30.772677977595755,32.102413402110507,33.432148826625259,34.76188425114001,36.091619675654762,37.421355100169514,38.751090524684265,40.080825949199017,41.410561373713769,42.74029679822852,44.070032222743272,45.399767647258024,46.729503071772776,48.059238496287527,49.388973920802279,50.718709345317031,52.048444769831782,53.378180194346534,54.707915618861286,56.037651043376037,57.367386467890789,58.697121892405541,60.026857316920292,61.356592741435044,62.686328165949796,64.01606359046454,65.345799014979292,66.675534439494044,68.005269864008767,69.335005288523519,70.66474071303827,71.994476137553022,73.324211562067774,74.653946986582525,75.983682411097277,77.313417835612029,78.64315326012678,79.972888684641532,81.302624109156284,82.632359533671035,83.962094958185787,85.291830382700539,86.621565807215291,87.951301231730042,89.281036656244794,90.610772080759546,91.940507505274297],[-18.858753639001666,-17.529018214486914,-16.199282789972163,-14.869547365457411,-13.539811940942659,-12.210076516427907,-10.880341091913156,-9.5506056673984041,-8.2208702428836524,-6.8911348183689007,-5.5613993938541491,-4.2316639693393974,-2.9019285448246457,-1.572193120309894,-0.24245769579514231,1.0872777287196094,2.4170131532343611,3.7467485777491127,5.0764840022638644,6.4062194267786161,7.7359548512933678,9.0656902758081195,10.395425700322871,11.725161124837623,13.054896549352375,14.384631973867126,15.714367398381878,17.04410282289663,18.373838247411381,19.703573671926133,21.033309096440885,22.363044520955636,23.692779945470388,25.022515369985111,26.352250794499863,27.681986219014615,29.011721643529366,30.341457068044118,31.67119249255887,33.000927917073625,34.330663341588377,35.660398766103128,36.99013419061788,38.319869615132632,39.649605039647383,40.979340464162135,42.309075888676887,43.638811313191638,44.96854673770639,46.298282162221142,47.628017586735893,48.957753011250645,50.287488435765397,51.617223860280149,52.9469592847949,54.276694709309652,55.606430133824404,56.936165558339155,58.265900982853907,59.595636407368659,60.92537183188341,62.255107256398162,63.584842680912914,64.914578105427665,66.244313529942389,67.57404895445714,68.903784378971892,70.233519803486644,71.563255228001395,72.892990652516147,74.222726077030899,75.55246150154565,76.882196926060402,78.211932350575154,79.541667775089905,80.871403199604657,82.201138624119409,83.530874048634161,84.860609473148912,86.190344897663664,87.520080322178416,88.849815746693167,90.179551171207919],[-20.619709973068055,-19.289974548553303,-17.960239124038551,-16.6305036995238,-15.300768275009048,-13.971032850494296,-12.641297425979545,-11.311562001464793,-9.9818265769500414,-8.6520911524352897,-7.322355727920538,-5.9926203034057863,-4.6628848788910346,-3.333149454376283,-2.0034140298615313,-0.67367860534677959,0.65605681916797209,1.9857922436827238,3.3155276681974755,4.6452630927122271,5.9749985172269788,7.3047339417417305,8.6344693662564822,9.9642047907712339,11.293940215285986,12.623675639800737,13.953411064315489,15.283146488830241,16.612881913344992,17.942617337859744,19.272352762374496,20.602088186889247,21.931823611403999,23.261559035918722,24.591294460433474,25.921029884948226,27.250765309462977,28.580500733977729,29.910236158492481,31.239971583007232,32.569707007521984,33.899442432036736,35.229177856551487,36.558913281066239,37.888648705580991,39.218384130095743,40.548119554610494,41.877854979125246,43.207590403639998,44.537325828154749,45.867061252669501,47.196796677184253,48.526532101699004,49.856267526213756,51.186002950728508,52.515738375243259,53.845473799758011,55.175209224272763,56.504944648787514,57.834680073302266,59.164415497817018,60.494150922331769,61.823886346846521,63.153621771361273,64.483357195875996,65.813092620390748,67.142828044905499,68.472563469420251,69.802298893935003,71.132034318449755,72.461769742964506,73.791505167479258,75.12124059199401,76.450976016508761,77.780711441023513,79.110446865538265,80.440182290053016,81.769917714567768,83.09965313908252,84.429388563597271,85.759123988112023,87.088859412626775,88.418594837141526],[-22.380666307134444,-21.050930882619692,-19.72119545810494,-18.391460033590189,-17.061724609075437,-15.731989184560685,-14.402253760045934,-13.072518335531182,-11.74278291101643,-10.413047486501679,-9.083312061986927,-7.7535766374721753,-6.4238412129574236,-5.0941057884426719,-3.7643703639279202,-2.4346349394131686,-1.1048995148984169,0.22483590961633482,1.5545713341310865,2.8843067586458382,4.2140421831605899,5.5437776076753416,6.8735130321900932,8.2032484567048449,9.5329838812195966,10.862719305734348,12.1924547302491,13.522190154763852,14.851925579278603,16.181661003793355,17.511396428308107,18.841131852822858,20.17086727733761,21.500602701852333,22.830338126367085,24.160073550881837,25.489808975396588,26.81954439991134,28.149279824426092,29.479015248940843,30.808750673455595,32.138486097970343,33.468221522485095,34.797956946999847,36.127692371514598,37.45742779602935,38.787163220544102,40.116898645058853,41.446634069573605,42.776369494088357,44.106104918603108,45.43584034311786,46.765575767632612,48.095311192147363,49.425046616662115,50.754782041176867,52.084517465691619,53.41425289020637,54.743988314721122,56.073723739235874,57.403459163750625,58.733194588265377,60.062930012780129,61.39266543729488,62.722400861809604,64.052136286324355,65.381871710839107,66.711607135353859,68.04134255986861,69.371077984383362,70.700813408898114,72.030548833412865,73.360284257927617,74.690019682442369,76.01975510695712,77.349490531471872,78.679225955986624,80.008961380501376,81.338696805016127,82.668432229530879,83.998167654045631,85.327903078560382,86.657638503075134]],"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"surface","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Temp"},"yaxis":{"title":"Month"},"zaxis":{"title":"Ozone"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[67,72,74,62,66,65,59,61,74,69,66,68,58,64,66,57,68,62,59,73,61,61,67,81,79,76,82,90,87,82,77,72,65,73,76,84,85,81,83,83,88,92,92,89,73,81,80,81,82,84,87,85,74,86,85,82,86,88,86,83,81,81,81,82,86,85,87,89,90,90,86,82,80,77,79,76,78,78,77,72,79,81,86,97,94,96,94,91,92,93,93,87,84,80,78,75,73,81,76,77,71,71,78,67,76,68,82,64,71,81,69,63,70,75,76,68],"y":[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9],"z":[41,36,12,18,28,23,19,8,7,16,11,14,18,14,34,6,30,11,1,11,4,32,23,45,115,37,29,71,39,23,21,37,20,12,13,135,49,32,64,40,77,97,97,85,10,27,7,48,35,61,79,63,16,80,108,20,52,82,50,64,59,39,9,16,78,35,66,122,89,110,44,28,65,22,59,23,31,44,21,9,45,168,73,76,118,84,85,96,78,73,91,47,32,20,23,21,24,44,21,28,9,13,46,18,13,24,16,13,23,36,7,14,30,14,18,20],"text":["1","2","3","4","6","7","8","9","11","12","13","14","15","16","17","18","19","20","21","22","23","24","28","29","30","31","38","40","41","44","47","48","49","50","51","62","63","64","66","67","68","69","70","71","73","74","76","77","78","79","80","81","82","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","104","105","106","108","109","110","111","112","113","114","116","117","118","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","151","152","153"],"mode":"markers","type":"scatter3d","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"Ozone","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[56,56.5,57,57.5,58,58.5,59,59.5,60,60.5,61,61.5,62,62.5,63,63.5,64,64.5,65,65.5,66,66.5,67,67.5,68,68.5,69,69.5,70,70.5,71,71.5,72,72.5,73,73.5,74,74.5,75,75.5,76,76.5,77,77.5,78,78.5,79,79.5,80,80.5,81,81.5,82,82.5,83,83.5,84,84.5,85,85.5,86,86.5,87,87.5,88,88.5,89,89.5,90,90.5,91,91.5,92,92.5,93,93.5,94,94.5,95,95.5,96,96.5,97],"y":[5,5.5,6,6.5,7,7.5,8,8.5,9],"z":[[-8.2930156346033321,-6.9632802100885804,-5.6335447855738288,-4.3038093610590771,-2.9740739365443254,-1.6443385120295737,-0.31460308751482202,1.0151323369999297,2.3448677615146813,3.674603186029433,5.0043386105441847,6.3340740350589364,7.6638094595736881,8.9935448840884398,10.323280308603191,11.653015733117943,12.982751157632695,14.312486582147447,15.642222006662198,16.97195743117695,18.301692855691702,19.631428280206453,20.961163704721205,22.290899129235957,23.620634553750708,24.95036997826546,26.280105402780212,27.609840827294963,28.939576251809715,30.269311676324467,31.599047100839218,32.928782525353967,34.258517949868718,35.588253374383441,36.917988798898193,38.247724223412945,39.577459647927697,40.907195072442448,42.2369304969572,43.566665921471952,44.896401345986703,46.226136770501455,47.555872195016207,48.885607619530958,50.21534304404571,51.545078468560462,52.874813893075213,54.204549317589965,55.534284742104717,56.864020166619468,58.19375559113422,59.523491015648972,60.853226440163724,62.182961864678475,63.512697289193227,64.842432713707979,66.17216813822273,67.501903562737482,68.831638987252234,70.161374411766985,71.491109836281737,72.820845260796489,74.15058068531124,75.480316109825992,76.810051534340715,78.139786958855467,79.469522383370219,80.79925780788497,82.128993232399722,83.458728656914474,84.788464081429225,86.118199505943977,87.447934930458729,88.77767035497348,90.107405779488232,91.437141204002984,92.766876628517736,94.096612053032487,95.426347477547239,96.756082902061991,98.085818326576742,99.415553751091494,100.74528917560625],[-10.053971968669721,-8.7242365441549694,-7.3945011196402177,-6.064765695125466,-4.7350302706107144,-3.4052948460959627,-2.075559421581211,-0.7458239970664593,0.58391142744829239,1.9136468519630441,3.2433822764777958,4.5731177009925474,5.9028531255072991,7.2325885500220508,8.5623239745368025,9.8920593990515542,11.221794823566306,12.551530248081058,13.881265672595809,15.211001097110561,16.540736521625313,17.870471946140064,19.200207370654816,20.529942795169568,21.859678219684319,23.189413644199071,24.519149068713823,25.848884493228574,27.178619917743326,28.508355342258078,29.838090766772829,31.167826191287581,32.497561615802333,33.827297040317056,35.157032464831808,36.486767889346559,37.816503313861311,39.146238738376063,40.475974162890815,41.805709587405566,43.135445011920318,44.46518043643507,45.794915860949821,47.124651285464573,48.454386709979325,49.784122134494076,51.113857559008828,52.44359298352358,53.773328408038331,55.103063832553083,56.432799257067835,57.762534681582586,59.092270106097338,60.42200553061209,61.751740955126841,63.081476379641593,64.411211804156352,65.740947228671104,67.070682653185855,68.400418077700607,69.730153502215359,71.05988892673011,72.389624351244862,73.719359775759614,75.049095200274337,76.378830624789089,77.70856604930384,79.038301473818592,80.368036898333344,81.697772322848095,83.027507747362847,84.357243171877599,85.68697859639235,87.016714020907102,88.346449445421854,89.676184869936606,91.005920294451357,92.335655718966109,93.665391143480861,94.995126567995612,96.324861992510364,97.654597417025116,98.984332841539867],[-11.81492830273611,-10.485192878221358,-9.1554574537066067,-7.825722029191855,-6.4959866046771033,-5.1662511801623516,-3.8365157556475999,-2.5067803311328483,-1.1770449066180966,0.15269051789665511,1.4824259424114068,2.8121613669261585,4.1418967914409102,5.4716322159556618,6.8013676404704135,8.1311030649851652,9.4608384894999169,10.790573914014669,12.12030933852942,13.450044763044172,14.779780187558924,16.109515612073675,17.439251036588427,18.768986461103179,20.09872188561793,21.428457310132682,22.758192734647434,24.087928159162185,25.417663583676937,26.747399008191689,28.07713443270644,29.406869857221192,30.736605281735944,32.066340706250671,33.396076130765422,34.725811555280174,36.055546979794926,37.385282404309677,38.715017828824429,40.044753253339181,41.374488677853932,42.704224102368684,44.033959526883436,45.363694951398188,46.693430375912939,48.023165800427691,49.352901224942443,50.682636649457194,52.012372073971946,53.342107498486698,54.671842923001449,56.001578347516201,57.331313772030953,58.661049196545704,59.990784621060456,61.320520045575208,62.650255470089959,63.979990894604711,65.309726319119463,66.639461743634214,67.969197168148966,69.298932592663718,70.62866801717847,71.958403441693221,73.288138866207944,74.617874290722696,75.947609715237448,77.2773451397522,78.607080564266951,79.936815988781703,81.266551413296455,82.596286837811206,83.926022262325958,85.25575768684071,86.585493111355461,87.915228535870213,89.244963960384965,90.574699384899716,91.904434809414468,93.23417023392922,94.563905658443971,95.893641082958723,97.223376507473475],[-13.575884636802499,-12.246149212287747,-10.916413787772996,-9.586678363258244,-8.2569429387434923,-6.9272075142287406,-5.5974720897139889,-4.2677366651992372,-2.9380012406844855,-1.6082658161697339,-0.27853039165498217,1.0512050328597695,2.3809404573745212,3.7106758818892729,5.0404113064040246,6.3701467309187763,7.6998821554335279,9.0296175799482796,10.359353004463031,11.689088428977783,13.018823853492535,14.348559278007286,15.678294702522038,17.00803012703679,18.337765551551541,19.667500976066293,20.997236400581045,22.326971825095796,23.656707249610548,24.9864426741253,26.316178098640052,27.645913523154803,28.975648947669555,30.305384372184278,31.63511979669903,32.964855221213782,34.294590645728533,35.624326070243285,36.954061494758037,38.283796919272788,39.61353234378754,40.943267768302292,42.273003192817043,43.602738617331795,44.932474041846547,46.262209466361298,47.59194489087605,48.921680315390802,50.251415739905553,51.581151164420305,52.910886588935057,54.240622013449808,55.57035743796456,56.900092862479312,58.229828286994064,59.559563711508815,60.889299136023567,62.219034560538319,63.54876998505307,64.878505409567822,66.208240834082574,67.537976258597325,68.867711683112077,70.197447107626829,71.527182532141552,72.856917956656304,74.186653381171055,75.516388805685807,76.846124230200559,78.17585965471531,79.505595079230062,80.835330503744814,82.165065928259565,83.494801352774317,84.824536777289069,86.154272201803821,87.484007626318572,88.813743050833324,90.143478475348076,91.473213899862827,92.802949324377579,94.132684748892331,95.462420173407082],[-15.336840970868888,-14.007105546354136,-12.677370121839385,-11.347634697324633,-10.017899272809881,-8.6881638482951296,-7.3584284237803779,-6.0286929992656262,-4.6989575747508745,-3.3692221502361228,-2.0394867257213711,-0.70975130120661944,0.61998412330813224,1.9497195478228839,3.2794549723376356,4.6091903968523873,5.938925821367139,7.2686612458818907,8.5983966703966423,9.928132094911394,11.257867519426146,12.587602943940897,13.917338368455649,15.247073792970401,16.576809217485152,17.906544641999904,19.236280066514656,20.566015491029408,21.895750915544159,23.225486340058911,24.555221764573663,25.884957189088414,27.214692613603166,28.544428038117889,29.874163462632641,31.203898887147393,32.533634311662141,33.863369736176892,35.193105160691644,36.522840585206396,37.852576009721147,39.182311434235899,40.512046858750651,41.841782283265402,43.171517707780154,44.501253132294906,45.830988556809658,47.160723981324409,48.490459405839161,49.820194830353913,51.149930254868664,52.479665679383416,53.809401103898168,55.139136528412919,56.468871952927671,57.798607377442423,59.128342801957174,60.458078226471926,61.787813650986678,63.117549075501429,64.447284500016181,65.777019924530933,67.106755349045685,68.436490773560436,69.766226198075159,71.095961622589911,72.425697047104663,73.755432471619415,75.085167896134166,76.414903320648918,77.74463874516367,79.074374169678421,80.404109594193173,81.733845018707925,83.063580443222676,84.393315867737428,85.72305129225218,87.052786716766931,88.382522141281683,89.712257565796435,91.041992990311186,92.371728414825938,93.70146383934069],[-17.097797304935277,-15.768061880420525,-14.438326455905774,-13.108591031391022,-11.77885560687627,-10.449120182361519,-9.1193847578467668,-7.7896493333320151,-6.4599139088172635,-5.1301784843025118,-3.8004430597877601,-2.4707076352730084,-1.1409722107582567,0.18876321375649496,1.5184986382712466,2.8482340627859983,4.17796948730075,5.5077049118155017,6.8374403363302534,8.1671757608450051,9.4969111853597568,10.826646609874508,12.15638203438926,13.486117458904012,14.815852883418763,16.145588307933515,17.475323732448267,18.805059156963019,20.13479458147777,21.464530005992522,22.794265430507274,24.124000855022025,25.453736279536777,26.7834717040515,28.113207128566252,29.442942553081004,30.772677977595755,32.102413402110507,33.432148826625259,34.76188425114001,36.091619675654762,37.421355100169514,38.751090524684265,40.080825949199017,41.410561373713769,42.74029679822852,44.070032222743272,45.399767647258024,46.729503071772776,48.059238496287527,49.388973920802279,50.718709345317031,52.048444769831782,53.378180194346534,54.707915618861286,56.037651043376037,57.367386467890789,58.697121892405541,60.026857316920292,61.356592741435044,62.686328165949796,64.01606359046454,65.345799014979292,66.675534439494044,68.005269864008767,69.335005288523519,70.66474071303827,71.994476137553022,73.324211562067774,74.653946986582525,75.983682411097277,77.313417835612029,78.64315326012678,79.972888684641532,81.302624109156284,82.632359533671035,83.962094958185787,85.291830382700539,86.621565807215291,87.951301231730042,89.281036656244794,90.610772080759546,91.940507505274297],[-18.858753639001666,-17.529018214486914,-16.199282789972163,-14.869547365457411,-13.539811940942659,-12.210076516427907,-10.880341091913156,-9.5506056673984041,-8.2208702428836524,-6.8911348183689007,-5.5613993938541491,-4.2316639693393974,-2.9019285448246457,-1.572193120309894,-0.24245769579514231,1.0872777287196094,2.4170131532343611,3.7467485777491127,5.0764840022638644,6.4062194267786161,7.7359548512933678,9.0656902758081195,10.395425700322871,11.725161124837623,13.054896549352375,14.384631973867126,15.714367398381878,17.04410282289663,18.373838247411381,19.703573671926133,21.033309096440885,22.363044520955636,23.692779945470388,25.022515369985111,26.352250794499863,27.681986219014615,29.011721643529366,30.341457068044118,31.67119249255887,33.000927917073625,34.330663341588377,35.660398766103128,36.99013419061788,38.319869615132632,39.649605039647383,40.979340464162135,42.309075888676887,43.638811313191638,44.96854673770639,46.298282162221142,47.628017586735893,48.957753011250645,50.287488435765397,51.617223860280149,52.9469592847949,54.276694709309652,55.606430133824404,56.936165558339155,58.265900982853907,59.595636407368659,60.92537183188341,62.255107256398162,63.584842680912914,64.914578105427665,66.244313529942389,67.57404895445714,68.903784378971892,70.233519803486644,71.563255228001395,72.892990652516147,74.222726077030899,75.55246150154565,76.882196926060402,78.211932350575154,79.541667775089905,80.871403199604657,82.201138624119409,83.530874048634161,84.860609473148912,86.190344897663664,87.520080322178416,88.849815746693167,90.179551171207919],[-20.619709973068055,-19.289974548553303,-17.960239124038551,-16.6305036995238,-15.300768275009048,-13.971032850494296,-12.641297425979545,-11.311562001464793,-9.9818265769500414,-8.6520911524352897,-7.322355727920538,-5.9926203034057863,-4.6628848788910346,-3.333149454376283,-2.0034140298615313,-0.67367860534677959,0.65605681916797209,1.9857922436827238,3.3155276681974755,4.6452630927122271,5.9749985172269788,7.3047339417417305,8.6344693662564822,9.9642047907712339,11.293940215285986,12.623675639800737,13.953411064315489,15.283146488830241,16.612881913344992,17.942617337859744,19.272352762374496,20.602088186889247,21.931823611403999,23.261559035918722,24.591294460433474,25.921029884948226,27.250765309462977,28.580500733977729,29.910236158492481,31.239971583007232,32.569707007521984,33.899442432036736,35.229177856551487,36.558913281066239,37.888648705580991,39.218384130095743,40.548119554610494,41.877854979125246,43.207590403639998,44.537325828154749,45.867061252669501,47.196796677184253,48.526532101699004,49.856267526213756,51.186002950728508,52.515738375243259,53.845473799758011,55.175209224272763,56.504944648787514,57.834680073302266,59.164415497817018,60.494150922331769,61.823886346846521,63.153621771361273,64.483357195875996,65.813092620390748,67.142828044905499,68.472563469420251,69.802298893935003,71.132034318449755,72.461769742964506,73.791505167479258,75.12124059199401,76.450976016508761,77.780711441023513,79.110446865538265,80.440182290053016,81.769917714567768,83.09965313908252,84.429388563597271,85.759123988112023,87.088859412626775,88.418594837141526],[-22.380666307134444,-21.050930882619692,-19.72119545810494,-18.391460033590189,-17.061724609075437,-15.731989184560685,-14.402253760045934,-13.072518335531182,-11.74278291101643,-10.413047486501679,-9.083312061986927,-7.7535766374721753,-6.4238412129574236,-5.0941057884426719,-3.7643703639279202,-2.4346349394131686,-1.1048995148984169,0.22483590961633482,1.5545713341310865,2.8843067586458382,4.2140421831605899,5.5437776076753416,6.8735130321900932,8.2032484567048449,9.5329838812195966,10.862719305734348,12.1924547302491,13.522190154763852,14.851925579278603,16.181661003793355,17.511396428308107,18.841131852822858,20.17086727733761,21.500602701852333,22.830338126367085,24.160073550881837,25.489808975396588,26.81954439991134,28.149279824426092,29.479015248940843,30.808750673455595,32.138486097970343,33.468221522485095,34.797956946999847,36.127692371514598,37.45742779602935,38.787163220544102,40.116898645058853,41.446634069573605,42.776369494088357,44.106104918603108,45.43584034311786,46.765575767632612,48.095311192147363,49.425046616662115,50.754782041176867,52.084517465691619,53.41425289020637,54.743988314721122,56.073723739235874,57.403459163750625,58.733194588265377,60.062930012780129,61.39266543729488,62.722400861809604,64.052136286324355,65.381871710839107,66.711607135353859,68.04134255986861,69.371077984383362,70.700813408898114,72.030548833412865,73.360284257927617,74.690019682442369,76.01975510695712,77.349490531471872,78.679225955986624,80.008961380501376,81.338696805016127,82.668432229530879,83.998167654045631,85.327903078560382,86.657638503075134]],"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","type":"surface","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Here is a second view of this same regression with what is called a
contour plot, contour map, or density plot.</p>
<pre class="r"><code>mycolorpalette &lt;- colorRampPalette(c(&quot;skyblue2&quot;, &quot;orange&quot;))
filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(26))</code></pre>
<p><strong>Including the Interaction Term</strong></p>
<p>Here is what a 3D regression looks like when the interaction term is
present. The two x-variables of <code>Month</code> and <code>Temp</code>
are being used to predict the y-variable of <code>Ozone</code>.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{Ozone} \underbrace{=}_{\sim}
\overbrace{\beta_0}^{\stackrel{\text{y-int}}{\text{baseline}}} +
\overbrace{\beta_1}^{\stackrel{\text{slope}}{\text{baseline}}}
\underbrace{X_{1i}}_\text{Temp} +
\overbrace{\beta_2}^{\stackrel{\text{change
in}}{\text{y-int}}}  \underbrace{X_{2i}}_\text{Month} +
\overbrace{\beta_3}^{\stackrel{\text{change in}}{\text{slope}}}
\underbrace{X_{1i}X_{2i}}_\text{Temp:Month} + \epsilon_i
\]</span></p>
<pre class="r"><code>air_lm &lt;- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality)
pander(air_lm$coefficients)</code></pre>
<table style="width:60%;">
<colgroup>
<col width="19%" />
<col width="9%" />
<col width="12%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">(Intercept)</th>
<th align="center">Temp</th>
<th align="center">Month</th>
<th align="center">Temp:Month</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-3.915</td>
<td align="center">0.77</td>
<td align="center">-23.01</td>
<td align="center">0.2678</td>
</tr>
</tbody>
</table>
<p>Notice how all coefficient estimates have changed. The y-intercept,
<span class="math inline">\(\beta_0\)</span> is now estimated to be
<span class="math inline">\(-3.915\)</span>. The slope term, <span
class="math inline">\(\beta_1\)</span>, in the Temp-direction is
estimated as <span class="math inline">\(0.77\)</span>, while the slope
term, <span class="math inline">\(\beta_2\)</span>, in the
Month-direction is estimated to be <span
class="math inline">\(-23.01\)</span>. This change in estimated
coefficiets is due to the presence of the interaction term’s
coefficient, <span class="math inline">\(\beta_3\)</span>, which is
estimated to be <span class="math inline">\(0.2678\)</span>. As you
should notice in the graphic, the interaction model allows the “slopes”
in each direction to change, creating a “curved” surface for the
regression surface instead of a flat surface.</p>
<pre class="r"><code>#Perform the multiple regression
air_lm &lt;- lm(Ozone ~ Temp + Month + Temp:Month, data= airquality)

#Graph Resolution (more important for more complex shapes)
graph_reso &lt;- 0.5

#Setup Axis
axis_x &lt;- seq(min(airquality$Temp), max(airquality$Temp), by = graph_reso)
axis_y &lt;- seq(min(airquality$Month), max(airquality$Month), by = graph_reso)

#Sample points
air_surface &lt;- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F)
air_surface &lt;- air_surface %&gt;% mutate(Z=predict.lm(air_lm, newdata = air_surface))
air_surface &lt;- acast(air_surface, Month ~ Temp, value.var = &quot;Z&quot;) #y ~ x

#Create scatterplot
plot_ly(airquality, 
        x = ~Temp, 
        y = ~Month, 
        z = ~Ozone,
        text = rownames(airquality), 
        type = &quot;scatter3d&quot;, 
        mode = &quot;markers&quot;) %&gt;%
  add_trace(z = air_surface,
            x = axis_x,
            y = axis_y,
            type = &quot;surface&quot;)</code></pre>
<div class="plotly html-widget html-fill-item" id="htmlwidget-5d7376ca2eeea01fe6f3" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-5d7376ca2eeea01fe6f3">{"x":{"visdat":{"1e981e56a7f":["function () ","plotlyVisDat"]},"cur_data":"1e981e56a7f","attrs":{"1e981e56a7f":{"x":{},"y":{},"z":{},"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d"},"1e981e56a7f.1":{"x":[56,56.5,57,57.5,58,58.5,59,59.5,60,60.5,61,61.5,62,62.5,63,63.5,64,64.5,65,65.5,66,66.5,67,67.5,68,68.5,69,69.5,70,70.5,71,71.5,72,72.5,73,73.5,74,74.5,75,75.5,76,76.5,77,77.5,78,78.5,79,79.5,80,80.5,81,81.5,82,82.5,83,83.5,84,84.5,85,85.5,86,86.5,87,87.5,88,88.5,89,89.5,90,90.5,91,91.5,92,92.5,93,93.5,94,94.5,95,95.5,96,96.5,97],"y":[5,5.5,6,6.5,7,7.5,8,8.5,9],"z":[[-0.83442089392809748,0.22012837686894215,1.2746776476659818,2.3292269184630072,3.3837761892600753,4.4383254600571007,5.4928747308541404,6.54742400165118,7.6019732724482196,8.6565225432452735,9.7110718140422989,10.765621084839339,11.820170355636392,12.874719626433418,13.929268897230472,14.983818168027497,16.038367438824551,17.092916709621591,18.147465980418616,19.202015251215684,20.25656452201271,21.311113792809749,22.365663063606789,23.420212334403828,24.474761605200882,25.529310875997908,26.583860146794947,27.638409417592001,28.692958688389027,29.74750795918608,30.802057229983106,31.85660650078016,32.911155771577199,33.965705042374232,35.020254313171272,36.074803583968318,37.129352854765358,38.18390212556239,39.238451396359444,40.293000667156484,41.347549937953517,42.402099208750556,43.456648479547603,44.511197750344643,45.565747021141682,46.620296291938715,47.674845562735769,48.729394833532801,49.783944104329841,50.83849337512688,51.893042645923927,52.947591916720967,54.002141187517999,55.056690458315053,56.111239729112093,57.165788999909125,58.220338270706165,59.274887541503219,60.329436812300244,61.383986083097284,62.438535353894324,63.493084624691377,64.547633895488417,65.602183166285442,66.656732437082482,67.711281707879536,68.765830978676576,69.820380249473615,70.874929520270641,71.929478791067694,72.984028061864734,74.038577332661774,75.093126603458828,76.147675874255853,77.202225145052893,78.256774415849932,79.311323686646986,80.365872957444026,81.420422228241051,82.474971499038091,83.529520769835131,84.58407004063217,85.638619311429238],[-4.8386227932523695,-3.7171209728019647,-2.5956191523515741,-1.4741173319011978,-0.35261551145077874,0.76888630899959765,1.8903881294499882,3.011889949900393,4.1333917703507694,5.2548935908011742,6.3763954112515506,7.4978972317019554,8.619399052152346,9.7409008726027224,10.862402693053141,11.983904513503518,13.105406333953908,14.226908154404299,15.34840997485469,16.469911795305094,17.591413615755471,18.712915436205876,19.834417256656266,20.955919077106643,22.077420897557047,23.198922718007438,24.320424538457829,25.441926358908219,26.56342817935861,27.684929999809015,28.806431820259391,29.927933640709782,31.049435461160186,32.170937281610563,33.292439102060968,34.413940922511358,35.535442742961749,36.656944563412139,37.77844638386253,38.899948204312935,40.021450024763311,41.142951845213702,42.264453665664107,43.385955486114483,44.507457306564888,45.628959127015278,46.750460947465669,47.87196276791606,48.993464588366436,50.114966408816855,51.236468229267231,52.357970049717622,53.479471870168027,54.600973690618403,55.722475511068808,56.843977331519184,57.965479151969589,59.08698097241998,60.208482792870356,61.329984613320761,62.451486433771151,63.572988254221542,64.694490074671933,65.815991895122323,66.937493715572728,68.058995536023104,69.180497356473509,70.301999176923886,71.423500997374276,72.545002817824681,73.666504638275057,74.788006458725462,75.909508279175867,77.031010099626229,78.152511920076634,79.274013740527039,80.395515560977415,81.51701738142782,82.638519201878211,83.760021022328587,84.881522842778992,86.003024663229368,87.124526483679773],[-8.8428246925766274,-7.6543703224728716,-6.4659159523691301,-5.2774615822654027,-4.0890072121616328,-2.9005528420579054,-1.7120984719541639,-0.52364410185040811,0.66481026825331924,1.853264638357075,3.0417190084608166,4.2301733785645581,5.4186277486682997,6.6070821187720412,7.795536488875797,8.9839908589795243,10.172445229083266,11.360899599187022,12.549353969290749,13.737808339394505,14.926262709498246,16.114717079601988,17.303171449705729,18.491625819809471,19.680080189913227,20.868534560016954,22.05698893012071,23.245443300224451,24.433897670328179,25.622352040431949,26.810806410535676,27.999260780639418,29.187715150743173,30.376169520846901,31.564623890950656,32.753078261054398,33.94153263115814,35.129987001261881,36.318441371365608,37.506895741469378,38.695350111573106,39.883804481676847,41.072258851780603,42.26071322188433,43.449167591988086,44.637621962091828,45.826076332195569,47.014530702299311,48.202985072403038,49.391439442506794,50.57989381261055,51.768348182714291,52.956802552818033,54.14525692292176,55.333711293025516,56.522165663129243,57.710620033233013,58.899074403336755,60.087528773440482,61.275983143544224,62.464437513647965,63.652891883751707,64.841346253855477,66.029800623959204,67.218254994062946,68.406709364166687,69.595163734270429,70.78361810437417,71.972072474477898,73.160526844581668,74.348981214685409,75.537435584789151,76.725889954892892,77.91434432499662,79.102798695100361,80.291253065204131,81.479707435307873,82.668161805411614,83.856616175515342,85.045070545619083,86.233524915722825,87.421979285826595,88.610433655930336],[-12.847026591900914,-11.591619672143807,-10.336212752386714,-9.0808058326296361,-7.8253989128725152,-6.5699919931154369,-5.3145850733583444,-4.0591781536012377,-2.8037712338441594,-1.5483643140870527,-0.29295739432996015,0.96244952542713236,2.2178564451842391,3.4732633649413174,4.7286702846984241,5.9840772044555166,7.2394841242126091,8.4948910439697016,9.7502979637267941,11.005704883483901,12.261111803240979,13.516518722998086,14.771925642755178,16.027332562512257,17.282739482269378,18.538146402026456,19.793553321783548,21.048960241540655,22.304367161297733,23.55977408105484,24.815181000811933,26.070587920569025,27.325994840326118,28.58140176008321,29.836808679840317,31.092215599597395,32.347622519354488,33.603029439111594,34.858436358868687,36.113843278625779,37.369250198382872,38.624657118139979,39.880064037897057,41.13547095765415,42.39087787741127,43.646284797168335,44.901691716925441,46.157098636682548,47.412505556439612,48.667912476196733,49.923319395953826,51.178726315710904,52.434133235468011,53.689540155225103,54.944947074982196,56.200353994739288,57.455760914496395,58.711167834253473,59.966574754010566,61.221981673767672,62.477388593524751,63.732795513281857,64.988202433038964,66.243609352796028,67.499016272553135,68.754423192310242,70.009830112067348,71.265237031824427,72.520643951581519,73.776050871338626,75.031457791095704,76.286864710852811,77.542271630609918,78.797678550366982,80.053085470124088,81.308492389881195,82.563899309638273,83.81930622939538,85.074713149152473,86.330120068909551,87.585526988666658,88.840933908423764,90.096340828180843],[-16.851228491225172,-15.528869021814714,-14.20650955240427,-12.884150082993841,-11.561790613583369,-10.23943114417294,-8.9170716747624823,-7.5947122053520388,-6.2723527359416096,-4.9499932665311377,-3.6276337971207084,-2.305274327710265,-0.98291485829980729,0.33944461111062196,1.6618040805210939,2.9841635499315231,4.3065230193419666,5.6288824887524243,6.9512419581628535,8.2736014275733112,9.5959608969837547,10.918320366394198,12.240679835804642,13.563039305215085,14.885398774625543,16.207758244035972,17.530117713446415,18.852477182856887,20.174836652267317,21.497196121677774,22.819555591088204,24.141915060498647,25.464274529909119,26.786633999319548,28.108993468730006,29.431352938140435,30.753712407550879,32.07607187696135,33.39843134637178,34.720790815782237,36.043150285192667,37.36550975460311,38.687869224013554,40.010228693424011,41.332588162834469,42.654947632244898,43.977307101655342,45.299666571065785,46.622026040476243,47.9443855098867,49.26674497929713,50.589104448707573,51.911463918118017,53.233823387528446,54.556182856938932,55.878542326349361,57.200901795759805,58.523261265170248,59.845620734580677,61.167980203991149,62.490339673401593,63.812699142812036,65.13505861222248,66.457418081632909,67.779777551043352,69.102137020453824,70.424496489864268,71.746855959274711,73.069215428685141,74.391574898095584,75.713934367506056,77.036293836916499,78.358653306326943,79.681012775737372,81.003372245147816,82.325731714558287,83.648091183968731,84.970450653379174,86.292810122789604,87.615169592200047,88.937529061610491,90.259888531020962,91.582248000431406],[-20.855430390549458,-19.466118371485635,-18.076806352421855,-16.687494333358075,-15.298182314294252,-13.908870295230471,-12.519558276166649,-11.130246257102868,-9.7409342380390882,-8.3516222189752654,-6.9623101999114851,-5.5729981808476623,-4.1836861617838821,-2.7943741427201019,-1.405062123656279,-0.015750104592498815,1.3735619144712956,2.7628739335351042,4.1521859525988987,5.5414979716626931,6.9308099907264875,8.3201220097902961,9.7094340288540764,11.098746047917871,12.488058066981694,13.877370086045488,15.266682105109268,16.655994124173077,18.045306143236871,19.434618162300666,20.82393018136446,22.213242200428269,23.602554219492049,24.991866238555843,26.381178257619666,27.770490276683461,29.159802295747241,30.54911431481105,31.938426333874844,33.327738352938638,34.717050372002433,36.106362391066241,37.495674410130022,38.884986429193816,40.274298448257639,41.663610467321433,43.052922486385214,44.442234505449022,45.831546524512817,47.220858543576611,48.610170562640405,49.999482581704214,51.388794600767994,52.778106619831789,54.167418638895612,55.556730657959406,56.946042677023186,58.335354696086995,59.724666715150789,61.113978734214569,62.503290753278378,63.892602772342187,65.281914791405967,66.671226810469761,68.06053882953357,69.449850848597379,70.839162867661159,72.228474886724968,73.617786905788762,75.007098924852542,76.396410943916351,77.785722962980159,79.17503498204394,80.564347001107734,81.953659020171543,83.342971039235323,84.732283058299132,86.12159507736294,87.510907096426735,88.900219115490515,90.289531134554323,91.678843153618132,93.068155172681912],[-24.859632289873716,-23.403367721156542,-21.947103152439411,-20.490838583722265,-19.034574015005106,-17.578309446287975,-16.122044877570801,-14.66578030885367,-13.209515740136538,-11.75325117141935,-10.296986602702219,-8.8407220339850596,-7.3844574652679285,-5.9281928965507973,-4.4719283278336093,-3.0156637591164781,-1.5593991903993185,-0.10313462168218734,1.3531299470349438,2.8093945157521318,4.265659084469263,5.7219236531864226,7.1781882219035538,8.6344527906206849,10.090717359337873,11.546981928055004,13.003246496772164,14.459511065489295,15.915775634206454,17.372040202923614,18.828304771640745,20.284569340357905,21.740833909075036,23.197098477792196,24.653363046509355,26.109627615226486,27.565892183943646,29.022156752660777,30.478421321377937,31.934685890095096,33.390950458812227,34.847215027529387,36.303479596246518,37.759744164963678,39.216008733680837,40.672273302397969,42.128537871115114,43.584802439832288,45.041067008549419,46.497331577266579,47.95359614598371,49.409860714700855,50.866125283418029,52.32238985213516,53.77865442085232,55.234918989569451,56.691183558286596,58.14744812700377,59.603712695720901,61.059977264438047,62.516241833155192,63.972506401872337,65.428770970589511,66.885035539306642,68.341300108023788,69.797564676740933,71.253829245458107,72.710093814175252,74.166358382892383,75.622622951609529,77.078887520326674,78.535152089043848,79.991416657760993,81.447681226478124,82.90394579519527,84.360210363912415,85.816474932629589,87.272739501346734,88.729004070063866,90.185268638781011,91.641533207498156,93.09779777621533,94.554062344932476],[-28.863834189197974,-27.340617070827449,-25.817399952456952,-24.294182834086484,-22.77096571571596,-21.247748597345463,-19.724531478974967,-18.201314360604471,-16.678097242233974,-15.15488012386345,-13.631663005492982,-12.108445887122457,-10.585228768751961,-9.0620116503814643,-7.5387945320109679,-6.0155774136404716,-4.4923602952699468,-2.9691431768994789,-1.4459260585289826,0.077291059841542165,1.6005081782120385,3.1237252965825348,4.6469424149530312,6.1701595333235275,7.6933766516940238,9.2165937700645202,10.739810888435045,12.263028006805541,13.786245125176009,15.309462243546534,16.83267936191703,18.355896480287555,19.879113598658023,21.402330717028519,22.925547835399044,24.448764953769512,25.971982072140037,27.495199190510533,29.018416308881029,30.541633427251526,32.064850545622022,33.588067663992547,35.111284782363043,36.634501900733511,38.157719019104036,39.680936137474532,41.204153255845029,42.727370374215525,44.250587492586021,45.773804610956546,47.297021729327014,48.820238847697539,50.343455966068035,51.866673084438503,53.389890202809028,54.913107321179524,56.436324439550049,57.959541557920517,59.482758676291013,61.005975794661509,62.529192913032034,64.05241003140253,65.575627149773027,67.098844268143523,68.622061386513991,70.145278504884516,71.668495623255041,73.191712741625537,74.714929859996005,76.238146978366501,77.761364096737026,79.284581215107551,80.807798333478019,82.331015451848515,83.854232570219025,85.377449688589508,86.900666806960018,88.423883925330529,89.947101043701025,91.470318162071507,92.993535280442018,94.516752398812528,96.03996951718301],[-32.868036088522246,-31.277866420498412,-29.687696752474551,-28.097527084450718,-26.507357416426828,-24.917187748402995,-23.327018080379162,-21.7368484123553,-20.146678744331467,-18.556509076307577,-16.966339408283744,-15.376169740259911,-13.78600007223605,-12.195830404212188,-10.605660736188327,-9.0154910681644935,-7.4253214001406604,-5.8351517321167705,-4.2449820640929374,-2.6548123960690759,-1.0646427280452428,0.52552693997861866,2.1156966080024802,3.7058662760263132,5.2960359440501747,6.8862056120740363,8.4763752800978693,10.066544948121731,11.656714616145564,13.246884284169425,14.837053952193287,16.42722362021712,18.017393288240982,19.607562956264815,21.197732624288705,22.787902292312538,24.378071960336371,25.968241628360232,27.558411296384094,29.148580964407955,30.738750632431788,32.328920300455621,33.919089968479511,35.509259636503344,37.099429304527206,38.689598972551039,40.279768640574872,41.869938308598762,43.460107976622595,45.050277644646457,46.64044731267029,48.230616980694151,49.820786648718013,51.410956316741846,53.001125984765707,54.591295652789569,56.181465320813402,57.771634988837263,59.361804656861096,60.951974324884986,62.542143992908819,64.132313660932653,65.722483328956514,67.312652996980376,68.902822665004237,70.49299233302807,72.083162001051903,73.673331669075765,75.263501337099626,76.853671005123488,78.443840673147321,80.034010341171154,81.624180009195044,83.214349677218877,84.804519345242738,86.394689013266571,87.984858681290433,89.575028349314294,91.165198017338128,92.755367685361989,94.345537353385851,95.935707021409684,97.525876689433545]],"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"surface","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Temp"},"yaxis":{"title":"Month"},"zaxis":{"title":"Ozone"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[67,72,74,62,66,65,59,61,74,69,66,68,58,64,66,57,68,62,59,73,61,61,67,81,79,76,82,90,87,82,77,72,65,73,76,84,85,81,83,83,88,92,92,89,73,81,80,81,82,84,87,85,74,86,85,82,86,88,86,83,81,81,81,82,86,85,87,89,90,90,86,82,80,77,79,76,78,78,77,72,79,81,86,97,94,96,94,91,92,93,93,87,84,80,78,75,73,81,76,77,71,71,78,67,76,68,82,64,71,81,69,63,70,75,76,68],"y":[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9],"z":[41,36,12,18,28,23,19,8,7,16,11,14,18,14,34,6,30,11,1,11,4,32,23,45,115,37,29,71,39,23,21,37,20,12,13,135,49,32,64,40,77,97,97,85,10,27,7,48,35,61,79,63,16,80,108,20,52,82,50,64,59,39,9,16,78,35,66,122,89,110,44,28,65,22,59,23,31,44,21,9,45,168,73,76,118,84,85,96,78,73,91,47,32,20,23,21,24,44,21,28,9,13,46,18,13,24,16,13,23,36,7,14,30,14,18,20],"text":["1","2","3","4","6","7","8","9","11","12","13","14","15","16","17","18","19","20","21","22","23","24","28","29","30","31","38","40","41","44","47","48","49","50","51","62","63","64","66","67","68","69","70","71","73","74","76","77","78","79","80","81","82","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","104","105","106","108","109","110","111","112","113","114","116","117","118","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","151","152","153"],"mode":"markers","type":"scatter3d","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"Ozone","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333334","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"x":[56,56.5,57,57.5,58,58.5,59,59.5,60,60.5,61,61.5,62,62.5,63,63.5,64,64.5,65,65.5,66,66.5,67,67.5,68,68.5,69,69.5,70,70.5,71,71.5,72,72.5,73,73.5,74,74.5,75,75.5,76,76.5,77,77.5,78,78.5,79,79.5,80,80.5,81,81.5,82,82.5,83,83.5,84,84.5,85,85.5,86,86.5,87,87.5,88,88.5,89,89.5,90,90.5,91,91.5,92,92.5,93,93.5,94,94.5,95,95.5,96,96.5,97],"y":[5,5.5,6,6.5,7,7.5,8,8.5,9],"z":[[-0.83442089392809748,0.22012837686894215,1.2746776476659818,2.3292269184630072,3.3837761892600753,4.4383254600571007,5.4928747308541404,6.54742400165118,7.6019732724482196,8.6565225432452735,9.7110718140422989,10.765621084839339,11.820170355636392,12.874719626433418,13.929268897230472,14.983818168027497,16.038367438824551,17.092916709621591,18.147465980418616,19.202015251215684,20.25656452201271,21.311113792809749,22.365663063606789,23.420212334403828,24.474761605200882,25.529310875997908,26.583860146794947,27.638409417592001,28.692958688389027,29.74750795918608,30.802057229983106,31.85660650078016,32.911155771577199,33.965705042374232,35.020254313171272,36.074803583968318,37.129352854765358,38.18390212556239,39.238451396359444,40.293000667156484,41.347549937953517,42.402099208750556,43.456648479547603,44.511197750344643,45.565747021141682,46.620296291938715,47.674845562735769,48.729394833532801,49.783944104329841,50.83849337512688,51.893042645923927,52.947591916720967,54.002141187517999,55.056690458315053,56.111239729112093,57.165788999909125,58.220338270706165,59.274887541503219,60.329436812300244,61.383986083097284,62.438535353894324,63.493084624691377,64.547633895488417,65.602183166285442,66.656732437082482,67.711281707879536,68.765830978676576,69.820380249473615,70.874929520270641,71.929478791067694,72.984028061864734,74.038577332661774,75.093126603458828,76.147675874255853,77.202225145052893,78.256774415849932,79.311323686646986,80.365872957444026,81.420422228241051,82.474971499038091,83.529520769835131,84.58407004063217,85.638619311429238],[-4.8386227932523695,-3.7171209728019647,-2.5956191523515741,-1.4741173319011978,-0.35261551145077874,0.76888630899959765,1.8903881294499882,3.011889949900393,4.1333917703507694,5.2548935908011742,6.3763954112515506,7.4978972317019554,8.619399052152346,9.7409008726027224,10.862402693053141,11.983904513503518,13.105406333953908,14.226908154404299,15.34840997485469,16.469911795305094,17.591413615755471,18.712915436205876,19.834417256656266,20.955919077106643,22.077420897557047,23.198922718007438,24.320424538457829,25.441926358908219,26.56342817935861,27.684929999809015,28.806431820259391,29.927933640709782,31.049435461160186,32.170937281610563,33.292439102060968,34.413940922511358,35.535442742961749,36.656944563412139,37.77844638386253,38.899948204312935,40.021450024763311,41.142951845213702,42.264453665664107,43.385955486114483,44.507457306564888,45.628959127015278,46.750460947465669,47.87196276791606,48.993464588366436,50.114966408816855,51.236468229267231,52.357970049717622,53.479471870168027,54.600973690618403,55.722475511068808,56.843977331519184,57.965479151969589,59.08698097241998,60.208482792870356,61.329984613320761,62.451486433771151,63.572988254221542,64.694490074671933,65.815991895122323,66.937493715572728,68.058995536023104,69.180497356473509,70.301999176923886,71.423500997374276,72.545002817824681,73.666504638275057,74.788006458725462,75.909508279175867,77.031010099626229,78.152511920076634,79.274013740527039,80.395515560977415,81.51701738142782,82.638519201878211,83.760021022328587,84.881522842778992,86.003024663229368,87.124526483679773],[-8.8428246925766274,-7.6543703224728716,-6.4659159523691301,-5.2774615822654027,-4.0890072121616328,-2.9005528420579054,-1.7120984719541639,-0.52364410185040811,0.66481026825331924,1.853264638357075,3.0417190084608166,4.2301733785645581,5.4186277486682997,6.6070821187720412,7.795536488875797,8.9839908589795243,10.172445229083266,11.360899599187022,12.549353969290749,13.737808339394505,14.926262709498246,16.114717079601988,17.303171449705729,18.491625819809471,19.680080189913227,20.868534560016954,22.05698893012071,23.245443300224451,24.433897670328179,25.622352040431949,26.810806410535676,27.999260780639418,29.187715150743173,30.376169520846901,31.564623890950656,32.753078261054398,33.94153263115814,35.129987001261881,36.318441371365608,37.506895741469378,38.695350111573106,39.883804481676847,41.072258851780603,42.26071322188433,43.449167591988086,44.637621962091828,45.826076332195569,47.014530702299311,48.202985072403038,49.391439442506794,50.57989381261055,51.768348182714291,52.956802552818033,54.14525692292176,55.333711293025516,56.522165663129243,57.710620033233013,58.899074403336755,60.087528773440482,61.275983143544224,62.464437513647965,63.652891883751707,64.841346253855477,66.029800623959204,67.218254994062946,68.406709364166687,69.595163734270429,70.78361810437417,71.972072474477898,73.160526844581668,74.348981214685409,75.537435584789151,76.725889954892892,77.91434432499662,79.102798695100361,80.291253065204131,81.479707435307873,82.668161805411614,83.856616175515342,85.045070545619083,86.233524915722825,87.421979285826595,88.610433655930336],[-12.847026591900914,-11.591619672143807,-10.336212752386714,-9.0808058326296361,-7.8253989128725152,-6.5699919931154369,-5.3145850733583444,-4.0591781536012377,-2.8037712338441594,-1.5483643140870527,-0.29295739432996015,0.96244952542713236,2.2178564451842391,3.4732633649413174,4.7286702846984241,5.9840772044555166,7.2394841242126091,8.4948910439697016,9.7502979637267941,11.005704883483901,12.261111803240979,13.516518722998086,14.771925642755178,16.027332562512257,17.282739482269378,18.538146402026456,19.793553321783548,21.048960241540655,22.304367161297733,23.55977408105484,24.815181000811933,26.070587920569025,27.325994840326118,28.58140176008321,29.836808679840317,31.092215599597395,32.347622519354488,33.603029439111594,34.858436358868687,36.113843278625779,37.369250198382872,38.624657118139979,39.880064037897057,41.13547095765415,42.39087787741127,43.646284797168335,44.901691716925441,46.157098636682548,47.412505556439612,48.667912476196733,49.923319395953826,51.178726315710904,52.434133235468011,53.689540155225103,54.944947074982196,56.200353994739288,57.455760914496395,58.711167834253473,59.966574754010566,61.221981673767672,62.477388593524751,63.732795513281857,64.988202433038964,66.243609352796028,67.499016272553135,68.754423192310242,70.009830112067348,71.265237031824427,72.520643951581519,73.776050871338626,75.031457791095704,76.286864710852811,77.542271630609918,78.797678550366982,80.053085470124088,81.308492389881195,82.563899309638273,83.81930622939538,85.074713149152473,86.330120068909551,87.585526988666658,88.840933908423764,90.096340828180843],[-16.851228491225172,-15.528869021814714,-14.20650955240427,-12.884150082993841,-11.561790613583369,-10.23943114417294,-8.9170716747624823,-7.5947122053520388,-6.2723527359416096,-4.9499932665311377,-3.6276337971207084,-2.305274327710265,-0.98291485829980729,0.33944461111062196,1.6618040805210939,2.9841635499315231,4.3065230193419666,5.6288824887524243,6.9512419581628535,8.2736014275733112,9.5959608969837547,10.918320366394198,12.240679835804642,13.563039305215085,14.885398774625543,16.207758244035972,17.530117713446415,18.852477182856887,20.174836652267317,21.497196121677774,22.819555591088204,24.141915060498647,25.464274529909119,26.786633999319548,28.108993468730006,29.431352938140435,30.753712407550879,32.07607187696135,33.39843134637178,34.720790815782237,36.043150285192667,37.36550975460311,38.687869224013554,40.010228693424011,41.332588162834469,42.654947632244898,43.977307101655342,45.299666571065785,46.622026040476243,47.9443855098867,49.26674497929713,50.589104448707573,51.911463918118017,53.233823387528446,54.556182856938932,55.878542326349361,57.200901795759805,58.523261265170248,59.845620734580677,61.167980203991149,62.490339673401593,63.812699142812036,65.13505861222248,66.457418081632909,67.779777551043352,69.102137020453824,70.424496489864268,71.746855959274711,73.069215428685141,74.391574898095584,75.713934367506056,77.036293836916499,78.358653306326943,79.681012775737372,81.003372245147816,82.325731714558287,83.648091183968731,84.970450653379174,86.292810122789604,87.615169592200047,88.937529061610491,90.259888531020962,91.582248000431406],[-20.855430390549458,-19.466118371485635,-18.076806352421855,-16.687494333358075,-15.298182314294252,-13.908870295230471,-12.519558276166649,-11.130246257102868,-9.7409342380390882,-8.3516222189752654,-6.9623101999114851,-5.5729981808476623,-4.1836861617838821,-2.7943741427201019,-1.405062123656279,-0.015750104592498815,1.3735619144712956,2.7628739335351042,4.1521859525988987,5.5414979716626931,6.9308099907264875,8.3201220097902961,9.7094340288540764,11.098746047917871,12.488058066981694,13.877370086045488,15.266682105109268,16.655994124173077,18.045306143236871,19.434618162300666,20.82393018136446,22.213242200428269,23.602554219492049,24.991866238555843,26.381178257619666,27.770490276683461,29.159802295747241,30.54911431481105,31.938426333874844,33.327738352938638,34.717050372002433,36.106362391066241,37.495674410130022,38.884986429193816,40.274298448257639,41.663610467321433,43.052922486385214,44.442234505449022,45.831546524512817,47.220858543576611,48.610170562640405,49.999482581704214,51.388794600767994,52.778106619831789,54.167418638895612,55.556730657959406,56.946042677023186,58.335354696086995,59.724666715150789,61.113978734214569,62.503290753278378,63.892602772342187,65.281914791405967,66.671226810469761,68.06053882953357,69.449850848597379,70.839162867661159,72.228474886724968,73.617786905788762,75.007098924852542,76.396410943916351,77.785722962980159,79.17503498204394,80.564347001107734,81.953659020171543,83.342971039235323,84.732283058299132,86.12159507736294,87.510907096426735,88.900219115490515,90.289531134554323,91.678843153618132,93.068155172681912],[-24.859632289873716,-23.403367721156542,-21.947103152439411,-20.490838583722265,-19.034574015005106,-17.578309446287975,-16.122044877570801,-14.66578030885367,-13.209515740136538,-11.75325117141935,-10.296986602702219,-8.8407220339850596,-7.3844574652679285,-5.9281928965507973,-4.4719283278336093,-3.0156637591164781,-1.5593991903993185,-0.10313462168218734,1.3531299470349438,2.8093945157521318,4.265659084469263,5.7219236531864226,7.1781882219035538,8.6344527906206849,10.090717359337873,11.546981928055004,13.003246496772164,14.459511065489295,15.915775634206454,17.372040202923614,18.828304771640745,20.284569340357905,21.740833909075036,23.197098477792196,24.653363046509355,26.109627615226486,27.565892183943646,29.022156752660777,30.478421321377937,31.934685890095096,33.390950458812227,34.847215027529387,36.303479596246518,37.759744164963678,39.216008733680837,40.672273302397969,42.128537871115114,43.584802439832288,45.041067008549419,46.497331577266579,47.95359614598371,49.409860714700855,50.866125283418029,52.32238985213516,53.77865442085232,55.234918989569451,56.691183558286596,58.14744812700377,59.603712695720901,61.059977264438047,62.516241833155192,63.972506401872337,65.428770970589511,66.885035539306642,68.341300108023788,69.797564676740933,71.253829245458107,72.710093814175252,74.166358382892383,75.622622951609529,77.078887520326674,78.535152089043848,79.991416657760993,81.447681226478124,82.90394579519527,84.360210363912415,85.816474932629589,87.272739501346734,88.729004070063866,90.185268638781011,91.641533207498156,93.09779777621533,94.554062344932476],[-28.863834189197974,-27.340617070827449,-25.817399952456952,-24.294182834086484,-22.77096571571596,-21.247748597345463,-19.724531478974967,-18.201314360604471,-16.678097242233974,-15.15488012386345,-13.631663005492982,-12.108445887122457,-10.585228768751961,-9.0620116503814643,-7.5387945320109679,-6.0155774136404716,-4.4923602952699468,-2.9691431768994789,-1.4459260585289826,0.077291059841542165,1.6005081782120385,3.1237252965825348,4.6469424149530312,6.1701595333235275,7.6933766516940238,9.2165937700645202,10.739810888435045,12.263028006805541,13.786245125176009,15.309462243546534,16.83267936191703,18.355896480287555,19.879113598658023,21.402330717028519,22.925547835399044,24.448764953769512,25.971982072140037,27.495199190510533,29.018416308881029,30.541633427251526,32.064850545622022,33.588067663992547,35.111284782363043,36.634501900733511,38.157719019104036,39.680936137474532,41.204153255845029,42.727370374215525,44.250587492586021,45.773804610956546,47.297021729327014,48.820238847697539,50.343455966068035,51.866673084438503,53.389890202809028,54.913107321179524,56.436324439550049,57.959541557920517,59.482758676291013,61.005975794661509,62.529192913032034,64.05241003140253,65.575627149773027,67.098844268143523,68.622061386513991,70.145278504884516,71.668495623255041,73.191712741625537,74.714929859996005,76.238146978366501,77.761364096737026,79.284581215107551,80.807798333478019,82.331015451848515,83.854232570219025,85.377449688589508,86.900666806960018,88.423883925330529,89.947101043701025,91.470318162071507,92.993535280442018,94.516752398812528,96.03996951718301],[-32.868036088522246,-31.277866420498412,-29.687696752474551,-28.097527084450718,-26.507357416426828,-24.917187748402995,-23.327018080379162,-21.7368484123553,-20.146678744331467,-18.556509076307577,-16.966339408283744,-15.376169740259911,-13.78600007223605,-12.195830404212188,-10.605660736188327,-9.0154910681644935,-7.4253214001406604,-5.8351517321167705,-4.2449820640929374,-2.6548123960690759,-1.0646427280452428,0.52552693997861866,2.1156966080024802,3.7058662760263132,5.2960359440501747,6.8862056120740363,8.4763752800978693,10.066544948121731,11.656714616145564,13.246884284169425,14.837053952193287,16.42722362021712,18.017393288240982,19.607562956264815,21.197732624288705,22.787902292312538,24.378071960336371,25.968241628360232,27.558411296384094,29.148580964407955,30.738750632431788,32.328920300455621,33.919089968479511,35.509259636503344,37.099429304527206,38.689598972551039,40.279768640574872,41.869938308598762,43.460107976622595,45.050277644646457,46.64044731267029,48.230616980694151,49.820786648718013,51.410956316741846,53.001125984765707,54.591295652789569,56.181465320813402,57.771634988837263,59.361804656861096,60.951974324884986,62.542143992908819,64.132313660932653,65.722483328956514,67.312652996980376,68.902822665004237,70.49299233302807,72.083162001051903,73.673331669075765,75.263501337099626,76.853671005123488,78.443840673147321,80.034010341171154,81.624180009195044,83.214349677218877,84.804519345242738,86.394689013266571,87.984858681290433,89.575028349314294,91.165198017338128,92.755367685361989,94.345537353385851,95.935707021409684,97.525876689433545]],"text":["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153"],"mode":"markers","type":"surface","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>And here is that same plot as a contour plot.</p>
<pre class="r"><code>air_surface &lt;- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS=F)
air_surface$Z &lt;- predict.lm(air_lm, newdata = air_surface)
mycolorpalette &lt;- colorRampPalette(c(&quot;skyblue2&quot;, &quot;orange&quot;))
filled.contour(x=axis_x, y=axis_y, z=matrix(air_surface$Z, length(axis_x), length(axis_y)), col=mycolorpalette(27))</code></pre>
</p>
</div>
<div id="LearnMoreHDModel" class="tabcontent" style="display:none;">
<p>
<table>
<tr>
<td>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-71-1.png" width="144" /></p>
</td>
<td style="text-align: center;padding-left:15px;">
<p><span class="math display">\[
  Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}
+ \ldots + \beta_{p-1}X_{p-1,i}}_{E\{Y_i\}}}^\text{&quot;High
Dimensional Models&quot;} + \epsilon_i
\]</span></p>
</td>
</tr>
</table>
<p>The so called “HD”, or “High Dimensional”, regression model uses
three or more different quantitative x-variables, an <span
class="math inline">\(X_{1i}\)</span>, an <span
class="math inline">\(X_{2i}\)</span>, and at least an <span
class="math inline">\(X_{3i}\)</span>, but could use many, many other
variables as well. Unlike the 3D model where the final regression could
be shown as either a contour plot or a 3D-graphic, the high dimensional
model exists in 4 or more dimensions. Thus, it is impossible to graph
this model in its full form. Further, it isn’t really even possible to
“mentally connect” with this type of model is it exists beyond what our
3D minds can really comprehend.</p>
<table>
<colgroup>
<col width="12%" />
<col width="87%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\beta_0\)</span></td>
<td>Y-intercept of the Model</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_1\)</span></td>
<td>Slope of the line in the <span class="math inline">\(X_1\)</span>
direction.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_2\)</span></td>
<td>Slope of the line in the <span class="math inline">\(X_2\)</span>
direction.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(...\)</span></td>
<td>Slopes in other directions depending on how many other variables are
included in the model.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\beta_{p-1}\)</span></td>
<td>Final term in the model where there are <span
class="math inline">\(p\)</span> total <span
class="math inline">\(\beta\)</span>’s. The reason for the <span
class="math inline">\(p-1\)</span> on the last term is because we
started with <span class="math inline">\(\beta_0\)</span> for the first
term, leaving <span class="math inline">\(\beta_{p-1}\)</span> as the
last term.</td>
</tr>
</tbody>
</table>
<p><strong>An Example</strong></p>
<p>Suppose we used three x-variables of <code>Wind</code>,
<code>Temp</code>, and <code>Solar.R</code> to predict the y-variable of
<code>Ozone</code>.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{Ozone} \underbrace{=}_{\sim}
\overbrace{\beta_0}^{\stackrel{\text{y-int}}{\text{baseline}}} +
\overbrace{\beta_1}^{\stackrel{\text{slope in}}{\text{Wind Direction}}}
\underbrace{X_{1i}}_\text{Wind} +
\overbrace{\beta_2}^{\stackrel{\text{slope in}}{\text{Temp
Direction}}}  \underbrace{X_{2i}}_\text{Temp} +
\overbrace{\beta_3}^{\stackrel{\text{slope in}}{\text{Solar.R
Direction}}}  \underbrace{X_{3i}}_\text{Solar.R} + \epsilon_i
\]</span></p>
<pre class="r"><code>air_lm &lt;- lm(Ozone ~ Wind + Temp + Solar.R, data= airquality)
pander(air_lm$coefficients)</code></pre>
<table style="width:57%;">
<colgroup>
<col width="19%" />
<col width="12%" />
<col width="11%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">(Intercept)</th>
<th align="center">Wind</th>
<th align="center">Temp</th>
<th align="center">Solar.R</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-64.34</td>
<td align="center">-3.334</td>
<td align="center">1.652</td>
<td align="center">0.05982</td>
</tr>
</tbody>
</table>
<p>Notice how the slope, <span class="math inline">\(\beta_1\)</span>,
in the “Wind” direction is estimated to be -3.334. The slope in the
“Temp” direction, <span class="math inline">\(\beta_2\)</span>, is
estimated to be 1.652. The slope in the “Solar.R” direction, <span
class="math inline">\(\beta_3\)</span>, is estimated to be 0.05982.
Also, the y-intercept, <span class="math inline">\(\beta_0\)</span>, is
estimated to be -64.34.</p>
<p>Visualizing this model is not really possible in its full form.
However, we can draw the regression from three different angles or
vantage points. This is a limited view of the full regression model, but
at least provides some visual understanding. To do this, we draw <span
class="math inline">\(Y\)</span> against each <span
class="math inline">\(X\)</span>-variable in separate scatterplots, one
for each <span class="math inline">\(X\)</span>-variable used in our
model.</p>
<pre class="r"><code>b &lt;- coef(air_lm)

par(mfrow=c(1,3))

  plot(Ozone ~ Wind, data=airquality)
  curve(b[1] + b[2]*x + b[3]*79 + b[4]*205, add=TRUE, col=&quot;skyblue&quot;)
  # The x-variable of this plot is &quot;Wind&quot;
  # The values of Temp=79 and Solar.R=205 are fixed at some interesting value,
  # in this case, their respective medians.

  plot(Ozone ~ Temp, data=airquality)
  curve(b[1] + b[2]*9.7 + b[3]*x + b[4]*205, add=TRUE, col=&quot;orange&quot;)
  # The x-variable of this plot is &quot;Temp&quot;
  # The values of Wind=9.7 and Solar.R=205 are fixed at some interesting value,
  # in this case, their respective medians.
  
  plot(Ozone ~ Solar.R, data=airquality)
  curve(b[1] + b[2]*9.7 + b[3]*79 + b[4]*x, add=TRUE, col=&quot;firebrick&quot;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<pre class="r"><code>  # The x-variable of this plot is &quot;Solar.R&quot;
  # The values of Wind = 9.7 and Temp=79 are fixed at some interesting value,
  # in this case, their respective medians.</code></pre>
</p>
</div>
<hr />
<p>The coefficient <span class="math inline">\(\beta_j\)</span> is
interpreted as the change in the expected value of <span
class="math inline">\(Y\)</span> for a unit increase in <span
class="math inline">\(X_{j}\)</span>, holding all other variables
constant, for <span class="math inline">\(j=1,\ldots,p-1\)</span>.
However, this interpretation breaks down when higher order terms (like
<span class="math inline">\(X^2\)</span>) or interaction terms (like
<span class="math inline">\(X1:X2\)</span>) are included in the
model.</p>
<p>See the <strong>Explanation</strong> tab for details about possible
hypotheses here.</p>
<hr />
</div>
</div>
<div id="r-instructions-1" class="section level3">
<h3>R Instructions</h3>
<div style="padding-left:125px;">
<p><strong>NOTE</strong>: These are general R Commands for <em>all</em>
types of multiple linear regressions. See the “Overview” section for R
Commands details about a specific multiple linear regression model.</p>
<p><strong>Console</strong> Help Command: <code>?lm()</code></p>
<p><strong>Finding Variables</strong></p>
<a href="javascript:showhide('PairsPlot')">
<div class="hoverchunk">
<p><span class="tooltipr"> pairs( <span class="tooltiprtext">A function
in R that creates all possible two-variable scatterplots from a data
set. It requires that all columns of the data set be either numeric or
factor classes. (Character classes will throw an error.)</span>
</span><span class="tooltipr"> cbind( <span class="tooltiprtext">This is
the “column (c) bind” function and it joins together things as
columns.</span> </span><span class="tooltipr"> Res =  <span
class="tooltiprtext">This is just any name you come up with, but Res is
a good abbreviation for Residuals.</span> </span><span class="tooltipr">
mylm$residuals,  <span class="tooltiprtext">This pulls out the residuals
from the current regression and adds them as a new column inside the
cbind data set.</span> </span><span class="tooltipr"> YourDataSet),
<span class="tooltiprtext">This puts the original data set along side
the residuals.</span> </span><span class="tooltipr">
 panel=panel.smooth,  <span class="tooltiprtext">This places a lowess
smoothing line on each scatterplot.</span> </span><span
class="tooltipr"> col =  <span class="tooltiprtext">specifies the colors
of the dots.</span> </span><span class="tooltipr">
as.factor(YourDataSet$Xvar) <span class="tooltiprtext">This causes the
coloring of the points in the plot to be colored according to the groups
found in Xvar. Using palette(c(“color1”,“color2”, and so on)) prior to
the plotting code allows you to specify the colors pairs will pick from
when choosing colors.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis for the pairs function.</span>
</span></p>
</div>
<p></a></p>
<div id="PairsPlot" style="display:none;">
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
</div>
<p><strong>Perform the Regression</strong></p>
<p>Everything is the same as in simple linear regression except that
more variables are allowed in the call to <code>lm()</code>.</p>
<a href="javascript:showhide('multiplelm')">
<div class="hoverchunk">
<p><span class="tooltipr"> mylm &lt;- lm( <span
class="tooltiprtext"><code>mylm</code> is some name you come up with to
store the results of the <code>lm()</code> test. Note that
<code>lm()</code> stands for “linear model.”</span> </span><span
class="tooltipr"> Y <span class="tooltiprtext"><code>Y</code> must be a
“numeric” vector of the quantitative response variable.</span>
</span><span class="tooltipr">  ~  <span class="tooltiprtext">Formula
operator in R.</span> </span><span class="tooltipr"> X1 + X2 <span
class="tooltiprtext"><code>X1</code> and <code>X2</code> are the
explanatory variables. These can either be quantitative or qualitative.
Note that R treats “numeric” variables as quantitative and “character”
or “factor” variables as qualitative. R will automatcially recode
qualitative variables to become “numeric” variables using a 0,1
encoding. See the Explanation tab for details.</span> </span><span
class="tooltipr">  + X1:X2 <span class="tooltiprtext"><code>X1:X2</code>
is called the interaction term. See the Explanation tab for
details.</span> </span><span class="tooltipr">  + …, <span
class="tooltiprtext">* <code>...</code> emphasizes that as many
explanatory variables as are desired can be included in the
model.</span> </span><span class="tooltipr">  data = YourDataSet) <span
class="tooltiprtext"><code>YourDataSet</code> is the name of your data
set.</span> </span><br/><span class="tooltipr"> summary( <span
class="tooltiprtext">The summary(…) function displays the results of an
lm(…) in R.</span> </span><span class="tooltipr"> mylm <span
class="tooltiprtext">The name of your lm that was performed
earlier.</span> </span><span class="tooltipr"> ) <span
class="tooltiprtext">Closing parenthesis for summary(…) function.</span>
</span></p>
</div>
<p></a></p>
<div id="multiplelm" style="display:none;">
<p>Example output from a regression. Hover each piece to learn more.</p>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Call:<br/> lm(formula = mpg ~ hp + am +
hp:am, data = mtcars) <span class="tooltiprouttext">This is simply a
statement of your original lm(…) “call” that you made when performing
your regression. It allows you to verify that you ran what you thought
you ran in the lm(…).</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td colspan="2">
<span class="tooltiprout"> Residuals: <span
class="tooltiprouttext">Residuals are the vertical difference between
each point and the line, <span class="math inline">\(Y_i -
\hat{Y}_i\)</span>. The residuals are supposed to be normally
distributed, so a quick glance at their five-number summary can give us
insight about any skew present in the residuals. </span>
</td>
</tr>
<tr>
<td align="right">
<span class="tooltiprout"> min<br/>   -4.3818 <span
class="tooltiprouttext">“min” gives the value of the residual that is
furthest below the regression line. Ideally, the magnitude of this value
would be about equal to the magnitude of the largest positive residual
(the max) because the hope is that the residuals are normally
distributed around the line.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 1Q<br/>   -2.2696 <span
class="tooltiprouttext">“1Q” gives the first quartile of the residuals,
which will always be negative, and ideally would be about equal in
magnitude to the third quartile.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> Median<br/>   0.1344 <span
class="tooltiprouttext">“Median” gives the median of the residuals,
which would ideally would be about equal to zero. Note that because the
regression line is the least squares line, the mean of the residuals
will ALWAYS be zero, so it is never included in the output summary. This
particular median value of -0.0191 is a little smaller than zero than we
would hope for and suggests a right skew in the data because the mean
(0) is greater than the median (-0.0191) witnessing the residuals are
right skewed. This can also be seen in the maximum being much larger in
magnitude than the minimum.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 3Q<br/>   1.7058 <span
class="tooltiprouttext">“3Q” gives the third quartile of the residuals,
which would ideally would be about equal in magnitude to the first
quartile. In this case, it is pretty close, which helps us see that the
first quartile of residuals on either side of the line is behaving
fairly normally.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> Max</br>   5.8752 <span
class="tooltiprouttext">“Max” gives the maximum positive residuals,
which would ideally would be about equal in magnitude to the minimum
residual. In this case, it is much larger than the minimum, which helps
us see that the residuals are likely right skewed.</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td colspan="2">
<span class="tooltiprout"> Coefficients: <span
class="tooltiprouttext">Notice that in your lm(…) you used only <span
class="math inline">\(Y\)</span> and <span
class="math inline">\(X\)</span>. You did type out any coefficients,
i.e., the <span class="math inline">\(\beta_0\)</span> or <span
class="math inline">\(\beta_1\)</span> of the regression model. These
coefficients are estimated by the lm(…) function and displayed in this
part of the output along with standard errors, t-values, and
p-values.</span> </span>
</td>
</tr>
<tr>
<td align="left">
</td>
<td align="right">
<span class="tooltiprout">   Estimate <span class="tooltiprouttext">To
learn more about the “Estimates” of the “Coefficients” see the
“Explanation” tab, “Estimating the Model Parameters” section for
details.</span>
</td>
<td align="right">
<span class="tooltiprout">   Std. Error <span class="tooltiprouttext">To
learn more about the “Standard Errors” of the “Coefficients” see the
“Explanation” tab, “Inference for the Model Parameters” section.</span>
</span>
</td>
<td align="right">
<span class="tooltiprout">   t value <span class="tooltiprouttext">To
learn more about the “t value” of the “Coefficients” see the
“Explanation” tab, “Inference for the Model Parameters” section.</span>
</span>
</td>
<td align="right">
<span class="tooltiprout">   Pr(&gt;|t|) <span
class="tooltiprouttext">The “Pr” stands for “Probability” and the “(&gt;
|t|)” stands for “more extreme than the observed t-value”. Thus, this is
the p-value for the hypothesis test of each coefficient being zero.<br/>
To learn more about the “p-value” of the “Coefficients” see the
“Explanation” tab, “Inference for the Model Parameters” section. </span>
</span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> (Intercept) <span
class="tooltiprouttext">This always says “Intercept” for any lm(…) you
run in R. That is because R always assumes there is a y-intercept for
your regression function.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   26.6248479 <span
class="tooltiprouttext">This is the estimate of the y-intercept, <span
class="math inline">\(\beta_0\)</span>. It is called <span
class="math inline">\(b_0\)</span>. It is the average y-value when all
X-variables are zero.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   2.1829432 <span
class="tooltiprouttext">This is the standard error of <span
class="math inline">\(b_0\)</span>. It estimates how much <span
class="math inline">\(b_0\)</span> varies from sample to sample. The
closer to zero, the more reliable the estimate of the intercept.</span>
</span>
</td>
<td align="right">
<span class="tooltiprout"> 12.197 <span class="tooltiprouttext">This is
the test statistic t for the test of <span class="math inline">\(\beta_0
= 0\)</span>. It is calculated by dividing the “Estimate” of the
intercept (26.6248479) by its standard error (2.1829432). It gives the
“number of standard errors” away from zero that the “estimate” has
landed. In this case, the estimate of 26.6248479 is t=12.197 standard
errors away from zero, which is a fairly surprising distance as shown by
the p-value.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 1.01e-12 <span class="tooltiprouttext">This
is the p-value of the test of the hypothesis that <span
class="math inline">\(\beta_0 = 0\)</span>. It measures the probability
of observing a t-value as extreme as the one observed. To compute it
yourself in R, use
<code>pt(-abs(your t-value), df of your regression)*2</code>.</span>
</span>
</td>
<td align="left">
<span class="tooltiprout"> *** <span class="tooltiprouttext">This is
called a “star”. Three stars means significant at the 0 level of <span
class="math inline">\(\alpha\)</span>.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> hp <span class="tooltiprouttext">This is
always the name of your first X-variable in your lm(Y ~ X1 + …).</span>
</span>
</td>
<td align="right">
<span class="tooltiprout">   -0.0591370 <span
class="tooltiprouttext">This is the estimate of <span
class="math inline">\(\beta_1\)</span> in the regression model. It is
called <span class="math inline">\(b_1\)</span>. Interpreting this value
depends on your choice of regression model.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   0.0129449 <span
class="tooltiprouttext">This is the standard error of <span
class="math inline">\(b_1\)</span>. It estimates how much <span
class="math inline">\(b_1\)</span> varies from sample to sample. The
closer to zero, the more precise the estimate.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> -4.568 <span class="tooltiprouttext">This is
the test statistic t for the test of <span class="math inline">\(\beta_1
= 0\)</span>. It is calculated by dividing the “Estimate” by its
standard error. It gives the “number of standard errors” away from zero
that the “estimate” has landed.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 9.02e-05 <span class="tooltiprouttext">This
is the p-value of the test of the hypothesis that <span
class="math inline">\(\beta_1 = 0\)</span>. To compute it yourself in R,
use <code>pt(-abs(your t-value), df of your regression)*2</code></span>
</span>
</td>
<td align="left">
<span class="tooltiprout"> *** <span class="tooltiprouttext">This is
called a “star”. Three stars means significant at the 0.01 level of
<span class="math inline">\(\alpha\)</span>.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> am <span class="tooltiprouttext">This is the
second X-variable of your regression model in lm(Y ~ X1 + X2 +
…).</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   5.2176534 <span
class="tooltiprouttext">This is the estimated value for <span
class="math inline">\(\beta_2\)</span> and is called <span
class="math inline">\(b_2\)</span>.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   2.6650931 <span
class="tooltiprouttext">This is the standard error of <span
class="math inline">\(b_2\)</span>. It estimates how much <span
class="math inline">\(b_2\)</span> will vary from sample to
sample.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 1.958 <span class="tooltiprouttext">Test
statistic (t) for the test of <span class="math inline">\(\beta_2 =
0\)</span>. It represents the number of standard errors that <span
class="math inline">\(b_2\)</span> is from 0.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 0.0603 <span class="tooltiprouttext">The
p-value for the test of <span class="math inline">\(\beta_2 =
0\)</span>.</span> </span>
</td>
<td align="left">
<span class="tooltiprout"> . <span class="tooltiprouttext">The dot “.”
implies the result is significant at the 0.1 level.</span> </span>
</td>
</tr>
<tr>
<td align="left">
<span class="tooltiprout"> hp:am <span class="tooltiprouttext">This is
the interaction of <span class="math inline">\(X1\)</span> and <span
class="math inline">\(X2\)</span>. Not all regression models require an
interaction term, and they can include more than one interaction term.
This is just an example of what an interaction term would look
like.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   0.0004029 <span
class="tooltiprouttext">This is the estimate of the coefficient of the
interaction term.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">   0.0164602 <span
class="tooltiprouttext">Estimated standard error of the interaction
term.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 0.024 <span class="tooltiprouttext">Test
statistic for the test that <span class="math inline">\(\beta_3 =
0\)</span>.</span> </span>
</td>
<td align="right">
<span class="tooltiprout"> 0.9806 <span class="tooltiprouttext">P-value
for the test that <span class="math inline">\(\beta_3 =
0\)</span>.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span> --- </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’
0.05 ‘.’ 0.1 ‘ ’ 1 <span class="tooltiprouttext">These “codes” explain
what significance level the p-value is smaller than based on how many
“stars” * the p-value is labeled with in the Coefficients table
above.</span> </span>
</td>
</tr>
</table>
<p><br/></p>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Residual standard error: <span
class="tooltiprouttext">This is the estimate of <span
class="math inline">\(\sigma\)</span> in the regression model <span
class="math inline">\(Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\)</span>
where <span class="math inline">\(\epsilon_i \sim
N(0,\sigma^2)\)</span>. It is the square root of the MSE.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  2.939 <span class="tooltiprouttext">For this
particular regression, the estimate of <span
class="math inline">\(\sigma\)</span> is 2.939. Squaring this number
gives you the MSE, which is the estimate of <span
class="math inline">\(\sigma^2\)</span>.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  on 28 degrees of freedom <span
class="tooltiprouttext">This is <span class="math inline">\(n-p\)</span>
where <span class="math inline">\(n\)</span> is the sample size and
<span class="math inline">\(p\)</span> is the number of parameters in
the regression model. In this case, there is a sample size of 32 and two
parameters, <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>, so 32-4 = 28.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> Multiple R-squared: <span
class="tooltiprouttext">This is <span
class="math inline">\(R^2\)</span>, the percentage of variation in <span
class="math inline">\(Y\)</span> that is explained by the regression
model. It is equal to the SSR/SSTO or, equivalently, 1 -
SSE/SSTO.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  0.7852, <span class="tooltiprouttext">In
this particular regression, 78.52% of the variation in stopping distance
<code>dist</code> is explained by the regression model using speed of
the car.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  Adjusted R-squared: <span
class="tooltiprouttext">The adjusted R-squared will always be at least
slightly smaller than <span class="math inline">\(R^2\)</span>. The
closer to R-squared that it is, the better. When it differs dramatically
from <span class="math inline">\(R^2\)</span>, it is a sign that the
regression model is over-fitting the data.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  0.7621 <span class="tooltiprouttext">In this
case, the value of 0.7621 is quite close to the original <span
class="math inline">\(R^2\)</span> value, so there is no fear of
over-fitting with this particular model. That is good.</span> </span>
</td>
</tr>
</table>
<table class="rconsole">
<tr>
<td>
<span class="tooltiprout"> F-statistic: <span
class="tooltiprouttext">The F-statistic is the test statistic for the
test of <span class="math inline">\(\beta_1 = \beta_2 = \beta_3 = \ldots
= 0\)</span>. In other words, it tests that ALL coefficients are zero
against the alternative that “at least one is not.”</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  34.11 <span class="tooltiprouttext">This is
the value of the F-statistic that should be compared to an
F-distribution with 3 and 28 degrees of freedom.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  on 3 and 28 DF, <span
class="tooltiprouttext">These two numbers give the two parameters
(degrees of freedom 1 and degrees of freedom 2) of the F-distribution.
Knowing these parameters and the value of the F-statistic allows the
computation of the p-value for the test that all regression coefficients
are zero.</span> </span>
</td>
<td align="right">
<span class="tooltiprout">  p-value: 1.73e-09 <span
class="tooltiprouttext">The p-value of the test that all regression
coefficients are zero. If this p-value is significant, then it can be
determined that “at least one” of the variables included in the
regression gives significant insight about the average y-value.</span>
</span>
</td>
</tr>
</table>
</div>
<p><br/></p>
<p><strong>Plotting the Regression Lines</strong></p>
<p>See each of the “Overview” sections for details on how to plot the
various types of multiple linear regression models.</p>
<p><br/></p>
<p><strong>Making Predictions</strong></p>
<a href="javascript:showhide('predict2')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R
function predict(…) allows you to use an lm(…) object to make
predictions for specified x-values.</span> </span><span
class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a
previously performed lm(…) that was saved into the name
<code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">
newdata = data.frame( <span class="tooltiprtext">To specify the values
of <span class="math inline">\(x\)</span> that you want to use in the
prediction, you have to put those x-values into a data set, or more
specifally, a data.frame(…).</span> </span><span class="tooltipr"> <span
class="math inline">\(X_1\)</span>= <span class="tooltiprtext">The value
for <code>X=</code> should be whatever x-variable name was used in the
original regression. For example, if
<code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data=mtcars)</code> was the
original regression, then this code would read <code>hp =</code> instead
of <code>X1 =</code>… Further, the value of <span
class="math inline">\(X_{1h}\)</span> should be some specific number,
like <code>hp=123</code> for example.</span> </span><span
class="tooltipr"> <span class="math inline">\(X_{1h}\)</span>, <span
class="tooltiprtext">The value of <span
class="math inline">\(X_{1h}\)</span> should be some specific number,
like <code>123</code>, as in <code>hp=123</code> for example.</span>
</span><span class="tooltipr"> <span class="math inline">\(X_2\)</span>=
<span class="tooltiprtext">This is the value of the second x-variable,
say <code>am</code>.</span> </span><span class="tooltipr"> <span
class="math inline">\(X_{2h}\)</span>) <span class="tooltiprtext">Since
the <code>am</code> column can only be a 1 or 0, we would try
<code>am=1</code> for example, or <code>am=0</code>.</span> </span><span
class="tooltipr"> ) <span class="tooltiprtext">Closing
parenthesis.</span> </span></p>
</div>
<p></a></p>
<div id="predict2" style="display:none;">
<p><code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data = mtcars)</code></p>
<p><code>predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, type = "response")</code></p>
<pre><code>##        1 
## 24.79441</code></pre>
<p>The value given is the “fitted-value” or “predicted-value” for the
specified x-value. In this case, a car with a speed of 12 is predicted
to have a stopping distance of 29.60981 feet.</p>
</div>
<a href="javascript:showhide('predict2Interval')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R
function predict(…) allows you to use an lm(…) object to make
predictions for specified x-values.</span> </span><span
class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a
previously performed lm(…) that was saved into the name
<code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">
 newdata=data.frame( <span class="tooltiprtext">To specify the values of
<span class="math inline">\(x\)</span> that you want to use in the
prediction, you have to put those x-values into a data set, or more
specifally, a data.frame(…).</span> </span><span class="tooltipr"> X1=
<span class="tooltiprtext">The <code>X1=</code> should be replaced with
whatever x-variable name was used in the original regression. For
example, if <code>mylm &lt;- lm(dist ~ speed, data=cars)</code> was the
original regression, then this code would read <code>speed =</code>
instead of <code>X1=</code>… Further, the value of <span
class="math inline">\(X_{1h}\)</span> should be some specific number,
like <code>12</code> so that it reads <code>speed=12</code>, for
example.</span> </span><span class="tooltipr"> <span
class="math inline">\(X_{1h}\)</span>, <span class="tooltiprtext">The
value of <span class="math inline">\(X_{1h}\)</span> should be some
specific number, like <code>12</code>, as in <code>speed=12</code> for
example.</span> </span><span class="tooltipr"> X2= <span
class="tooltiprtext">If a regression of lm(Y ~ X1 + X2 + …) was
performed, then X2 is the name of the second x-variable used in the
regression.</span> </span><span class="tooltipr"> <span
class="math inline">\(X_{2h}\)</span>), <span class="tooltiprtext">A
number should be specified for <span
class="math inline">\(X_{2h}\)</span>, something that would be
meaningful for X2 to be equal to.</span> </span><span class="tooltipr">
interval = “prediction”) <span class="tooltiprtext">This causes the
prediction to include the lower bound and upper bound of the prediction
interval for <span class="math inline">\(Y_i\)</span> for the given X1,
X2, and so on values that have been specified.</span> </span></p>
</div>
<p></a></p>
<div id="predict2Interval" style="display:none;">
<p><code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data = mtcars)</code></p>
<p><code>predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = "prediction")</code></p>
<pre class="r"><code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data = mtcars)
predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = &quot;prediction&quot;)</code></pre>
<pre><code>##        fit      lwr      upr
## 1 24.79441 18.49923 31.08959</code></pre>
<p>The “fit” is the predicted value. The “lwr” is the lower bound. The
“upr” is the upper bound.</p>
<p>In this case, a car with a speed of 12 mph is predicted to have a
stopping distance of 29.60981 feet. However, we are wise enough to
recognize that the stopping distance for individual cars will vary
anywhere from -1.749529 (or 0 because distance can’t go negative) feet
to 60.96915 feet.</p>
</div>
<a href="javascript:showhide('predict2Confidence')">
<div class="hoverchunk">
<p><span class="tooltipr"> predict( <span class="tooltiprtext">The R
function predict(…) allows you to use an lm(…) object to make
predictions for specified x-values.</span> </span><span
class="tooltipr"> mylm, <span class="tooltiprtext">This is the name of a
previously performed lm(…) that was saved into the name
<code>mylm &lt;- lm(...)</code>.</span> </span><span class="tooltipr">
data.frame( <span class="tooltiprtext">To specify the values of <span
class="math inline">\(x\)</span> that you want to use in the prediction,
you have to put those x-values into a data set, or more specifally, a
data.frame(…).</span> </span><span class="tooltipr"> X1= <span
class="tooltiprtext">The <code>X1=</code> should be replaced with
whatever x-variable name was used in the original regression. For
example, if <code>mylm &lt;- lm(dist ~ speed, data=cars)</code> was the
original regression, then this code would read <code>speed =</code>
instead of <code>X1=</code>… Further, the value of <span
class="math inline">\(X_{1h}\)</span> should be some specific number,
like <code>12</code> so that it reads <code>speed=12</code>, for
example.</span> </span><span class="tooltipr"> <span
class="math inline">\(X_{1h}\)</span>, <span class="tooltiprtext">The
value of <span class="math inline">\(X_{1h}\)</span> should be some
specific number, like <code>12</code>, as in <code>speed=12</code> for
example.</span> </span><span class="tooltipr"> X2= <span
class="tooltiprtext">If a regression of lm(Y ~ X1 + X2 + …) was
performed, then X2 is the name of the second x-variable used in the
regression.</span> </span><span class="tooltipr"> <span
class="math inline">\(X_{2h}\)</span>), <span class="tooltiprtext">A
number should be specified for <span
class="math inline">\(X_{2h}\)</span>, something that would be
meaningful for X2 to be equal to.</span> </span><span class="tooltipr">
interval = “confidence”) <span class="tooltiprtext">This causes the
prediction to include the lower and upper bound of a confidence interval
for <span class="math inline">\(E{Y_i}\)</span> for the given <span
class="math inline">\(X\)</span>-values.</span> </span></p>
</div>
<p></a></p>
<div id="predict2Confidence" style="display:none;">
<p><code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data = mtcars)</code></p>
<p><code>predict(mylm, data.frame(hp = 120, am = 1), data = mtcars, interval = "confidence")</code></p>
<pre class="r"><code>mylm &lt;- lm(mpg ~ hp + am + hp:am, data = mtcars)
predict(mylm, data.frame(hp = 120, am = 1), interval = &quot;confidence&quot;)</code></pre>
<pre><code>##        fit      lwr      upr
## 1 24.79441 23.10635 26.48247</code></pre>
<p>The “fit” is the predicted value. The “lwr” is the lower bound. The
“upr” is the upper bound.</p>
<p>In this case, cars with a speed of 12 mph are predicted to have an
average stopping distance of 29.60981 feet, where the average could be
anywhere from 24.39514 feet to 34.82448 feet.</p>
</div>
<hr />
</div>
</div>
<div id="explanation-1" class="section level3">
<h3>Explanation</h3>
<div style="padding-left:125px;">
<div id="assessing-the-model-fit-expand" class="section level4">
<h4>Assessing the Model Fit
<a href="javascript:showhide('assessingFit2')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span
class="math inline">\(R^2\)</span>, adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC…</span></p>
<div id="assessingFit2" style="display:none;">
<p>There are many measures of the quality of a regression model. One of
the most popular measurements is the <span
class="math inline">\(R^2\)</span> value (“R-squared”). The <span
class="math inline">\(R^2\)</span> value is a measure of the proportion
of variation of the <span class="math inline">\(Y\)</span>-variable that
is explained by the model. Specifically, <span class="math display">\[
  R^2 = \frac{\text{SSR}}{\text{SSTO}} =
1-\frac{\text{SSE}}{\text{SSTO}}
\]</span> The range of <span class="math inline">\(R^2\)</span> is
between 0 and 1. Values close to 1 imply a very good model. Values close
to 0 imply a very poor model.</p>
<p>One difficulty of <span class="math inline">\(R^2\)</span> in
multiple regression is that it will always get larger when more
variables are included in the regression model. Thus, in multiple linear
regression, it is best to make an adjustment to the <span
class="math inline">\(R^2\)</span> value to protect against this
difficulty. The value of the adjusted <span
class="math inline">\(R^2\)</span> is given by <span
class="math display">\[
  R^2_{adj} = 1 - \frac{(n-1)}{(n-p)}\frac{\text{SSE}}{\text{SSTO}}
\]</span> The interpretation of <span
class="math inline">\(R^2_{adj}\)</span> is essentially the same as the
interpretation of <span class="math inline">\(R^2\)</span>, with the
understanding that a correction has been made for the number of
parameters included in the model, <span
class="math inline">\((n-p)\)</span>.</p>
<p>Consider the models below. The value of <span
class="math inline">\(R^2\)</span> always gets higher as the model adds
more parameters. However, the value of <span
class="math inline">\(R^2_{adj}\)</span> sometimes goes down,
emphasizing the idea that the model is becoming more complex than needed
to capture the pattern in Y.</p>
<pre class="r"><code>par(mfrow=c(1,5), mai=c(0,.1,.4,.1))
plot(dist ~ speed, data=cars, pch=16, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, cex=2, xlim=c(0,27), main=&quot;Simple Linear&quot;)
lm1 &lt;- lm(dist ~ speed, data=cars)
b &lt;- coef(lm1)
curve(b[1] + b[2]*x, add=TRUE, col=&quot;orange&quot;, lwd=2)
text(1,110,bquote(R^2 ==  .(round(summary(lm1)$r.squared,3))),pos=4)
text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4)


plot(dist ~ speed, data=cars, pch=16, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, cex=2, xlim=c(0,27), main=&quot;Quadratic&quot;)
lm1 &lt;- lm(dist ~ speed + I(speed^2), data=cars)
b &lt;- coef(lm1)
curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col=&quot;orange&quot;, lwd=2)
text(1,110,bquote(R^2 ==  .(round(summary(lm1)$r.squared,3))),pos=4)
text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4)


plot(dist ~ speed, data=cars, pch=16, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, cex=2, xlim=c(0,27), main=&quot;Cubic&quot;)
lm1 &lt;- lm(dist ~ speed + I(speed^2) + I(speed^3), data=cars)
b &lt;- coef(lm1)
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, add=TRUE, col=&quot;orange&quot;, lwd=2)
text(1,110,bquote(R^2 ==  .(round(summary(lm1)$r.squared,3))),pos=4)
text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4)


plot(dist ~ speed, data=cars, pch=16, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, cex=2, xlim=c(0,27), main=&quot;Quartic&quot;)
lm1 &lt;- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4), data=cars)
b &lt;- coef(lm1)
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4, add=TRUE, col=&quot;orange&quot;, lwd=2)
text(1,110,bquote(R^2 ==  .(round(summary(lm1)$r.squared,3))),pos=4)
text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4)


plot(dist ~ speed, data=cars, pch=16, col=&quot;skyblue&quot;, yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, cex=2, xlim=c(0,27), main=&quot;Quintic&quot;)
lm1 &lt;- lm(dist ~ speed + I(speed^2) + I(speed^3) + I(speed^4) + I(speed^5), data=cars)
b &lt;- coef(lm1)
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5, add=TRUE, col=&quot;orange&quot;, lwd=2)
text(1,110,bquote(R^2 ==  .(round(summary(lm1)$r.squared,3))),pos=4)
text(1,100,bquote(R[adj]^2 == .(round(summary(lm1)$adj.r.squared,3))),pos=4)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-78-1.png" width="672" /></p>
<p><br/></p>
<p>The “simplest” but “best” model of those shown above would be the
Quadratic. This is because it has the best <span
class="math inline">\(R^2_{adj}\)</span> (0.653) other than the far more
complicated Quartic model (0.655). But the <span
class="math inline">\(R^2_{adj}\)</span> for the Quadratic model is a
good improvement over that of the <span
class="math inline">\(R^2_{adj}\)</span> for the Simple Linear model,
with a value of 0.653 compared to 0.644, respectively. So moving to the
complexity of the Quadratic model is justified over the Simple Linear
Model. But there is not enough of an improvement in the <span
class="math inline">\(R^2_{adj}\)</span> to warrant moving to the
complexity of the Quartic Model. Further, the pattern in the Quadratic
seems to generalize better to data outside the range of the current data
than does the Quartic model.</p>
<p><span class="math display">\[
  \text{\emph{Quadratic Model}:}\quad Y_i = \beta_0 + \beta_1 X_i +
\beta_2 X_i^2 + \epsilon_i
\]</span></p>
<p><span class="math display">\[
  \text{\emph{Quartic Model}:}\quad Y_i = \beta_0 + \beta_1 X_i +
\beta_2 X_i^2 + \underbrace{\beta_3 X_i^3 + \beta_4 X_i^4}_\text{Cubic
and Quartic Terms} + \epsilon_i
\]</span></p>
<p><strong>AIC and BIC</strong></p>
<p>Two other measurements, or information criterion, are popular for use
in the model selection process. These are the Akaike Information
Criterion (AIC) and the Bayesian Information Criterion (BIC). These are
easily computed in R using <code>AIC(yourlm)</code> and
<code>BIC(yourlm)</code>.</p>
<p>The formula for each are given in different, but equivalent ways
depending on which source you obtain the equation. Perhaps the easiest
formulation to understand is that given by Kutner, Nachtsheim, and Neter
in their book <em>Applied Linear Regression Models</em> (4th edition,
page 360)</p>
<p><span class="math display">\[
\text{AIC:} \quad n \ln(SSE) - n \ln(n) + 2p
\]</span> where SSE is the usual <span
class="math inline">\(\sum_{i=1}^n (Y_i - \hat{Y}_i)^2\)</span> of the
current regression model under consideration, <span
class="math inline">\(n\)</span> is the sample size, and <span
class="math inline">\(p\)</span> is the number of parameters in the
current regression model.</p>
<p><span class="math display">\[
\text{BIC:} \quad n \ln(SSE) - n \ln(n) + p\ln(n)
\]</span></p>
<p>This shows how the BIC differs only from the AIC in the final term,
where AIC uses <span class="math inline">\(2p\)</span> and BIC uses
<span class="math inline">\(p\ln(n)\)</span>. Since <span
class="math inline">\(\ln(n) \geq 2\)</span> for <span
class="math inline">\(n\geq8\)</span>, then BIC enforces a larger
penalty than the AIC for extra model parameters (<span
class="math inline">\(p\)</span>) when the sample size is 8 or larger,
i.e., most data sets.</p>
<p>The AIC was formulated by <a
href="https://www.ism.ac.jp/editsec/aism/pdf/023_2_0163.pdf">Hirotugu
Akaike in 1971</a>. (Here is a <a
href="http://www.garfield.library.upenn.edu/classics1981/A1981MS54100001.pdf">short
commentary</a> by Akaike about how he developed this information
criterion. Note that he named it “an information criterion (AIC)” when
he published the method and other people later began calling it the
“Akaike Information Criterion.”)</p>
</div>
<p><br/></p>
</div>
<div id="model-selection-expand" class="section level4">
<h4>Model Selection
<a href="javascript:showhide('modelselection')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">pairs plots, added variable plots, and
pattern recognition…</span></p>
<div id="modelselection" style="display:none;">
<p>Model selection is an exploratory analysis tool that is useful for
proposing possible regression models for a given response variable <span
class="math inline">\(Y\)</span>. They should always be followed up by
confirmatory analysis that tests the theories proposed by the selected
model. However, when confirmatory studies are not possible, model
validation is a meaningful tool that can be used to attempt to confirm
the utility of a model.</p>
<div id="pairs-plots" class="section level5 tabset tabset-pills">
<h5 class="tabset tabset-pills">Pairs Plots</h5>
<p>A useful visualization tool for model selection is the “pairs plot.”
This plot shows all possible 2D scatterplots that can be created from a
given dataset.</p>
<p>Here is a pairs plot of the <code>mtcars</code> data set in R.</p>
<div id="basic-view" class="section level6">
<h6>Basic View</h6>
<pre class="r"><code>pairs(mtcars, panel=panel.smooth)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-79-1.png" width="672" /></p>
</div>
<div id="more-detailed-view" class="section level6">
<h6>More Detailed View</h6>
<pre class="r"><code>pairs(mtcars, panel=panel.smooth)</code></pre>
<p><img src="Images/pairsPlotGuidance.png" /></p>
</div>
</div>
<div id="section-1" class="section level5">
<h5></h5>
<p>Notice that…</p>
<ul>
<li>the y-axis of each plot is found by locating the variable name (like
“mpg”) that is found to the left or right of the current plot.</li>
<li>the x-axis of each plot is found by locating the variable name (like
“disp”) that is found above or below each plot.</li>
<li>the LOWESS curves have been added to each plot to visualize the type
of regression model that would best fit each plot.</li>
</ul>
</div>
<div id="selecting-a-model" class="section level5 tabset tabset-pills">
<h5 class="tabset tabset-pills">Selecting a Model</h5>
<p>Suppose now that we are trying to come up with a good regression
model for predicting the gas mileage of a car, <span
class="math inline">\(Y=\)</span><code>mpg</code>.</p>
<p><span class="math display">\[
  \underbrace{Y_i}_\text{mpg} = \underbrace{?}_\text{Our model} +
\  \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \sigma^2)
\]</span> To find meaningful x-variables that could predict our chosen
y-variable of <code>mpg</code>, we look at all plots that have
<code>mpg</code> as the y-axis of the plot. This happens to be the first
row of the pairs plot.</p>
<p>When looking at the graph, we are looking for variables that show a
strong change in the average y-value (i.e., the LOWESS curve should show
steep slope or a meaningful trend). While all variables in the
<code>mtcars</code> data set seem to have some relationship with
<code>mpg</code>, the strongest relationships appear to e with
<code>cyl</code>, <code>disp</code>, <code>hp</code>, <code>wt</code>,
<code>vs</code>, <code>am</code>, and <code>gear</code>.</p>
<div id="basic-view-1" class="section level6">
<h6>Basic View</h6>
<pre class="r"><code>pairs(mtcars, panel=panel.smooth)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-81-1.png" width="672" /></p>
</div>
<div id="more-detailed-view-1" class="section level6">
<h6>More Detailed View</h6>
<pre class="r"><code>pairs(mtcars, panel=panel.smooth)</code></pre>
<p><img src="Images/pairsPossibleX.png" /></p>
</div>
</div>
<div id="section-2" class="section level5">
<h5></h5>
<p>Also worth noting is that the relationship of <code>mpg</code> with
each of <code>disp</code>, <code>hp</code>, and <code>wt</code> are all
similar, they each look to be an exponential decay type of model. This
tells us that we had better check to see if <code>disp</code>,
<code>hp</code>, and <code>wt</code> are related to each other. If they
are, then we should only use one of them in the regression model as the
other two likely wouldn’t give any new information about
<code>mpg</code>.</p>
<p>Sure enough, the pairs plot shows that there is a fairly strong
relationship between <code>disp</code> and <code>hp</code>,
<code>hp</code> and <code>wt</code>, and <code>disp</code> and
<code>wt</code>.</p>
<p><img src="Images/pairsPossibleRelatedX.png" /></p>
<p>Now, with all of this in mind, we could start looking at a few
possible regression models. Let’s start with perhaps the simplest and
strongest trend we saw with <code>mpg</code> and any of the x-variables,
<code>wt</code>.</p>
<pre class="r"><code>plot(mpg ~ wt, data=mtcars)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-83-1.png" width="672" /></p>
<pre class="r"><code>lm.wt &lt;- lm(mpg ~ wt, data=mtcars)
summary(lm.wt) %&gt;% pander()</code></pre>
<table style="width:89%;">
<colgroup>
<col width="25%" />
<col width="15%" />
<col width="18%" />
<col width="13%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">37.29</td>
<td align="center">1.878</td>
<td align="center">19.86</td>
<td align="center">8.242e-19</td>
</tr>
<tr class="even">
<td align="center"><strong>wt</strong></td>
<td align="center">-5.344</td>
<td align="center">0.5591</td>
<td align="center">-9.559</td>
<td align="center">1.294e-10</td>
</tr>
</tbody>
</table>
<table style="width:88%;">
<caption>Fitting linear model: mpg ~ wt</caption>
<colgroup>
<col width="20%" />
<col width="30%" />
<col width="12%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span
class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">32</td>
<td align="center">3.046</td>
<td align="center">0.7528</td>
<td align="center">0.7446</td>
</tr>
</tbody>
</table>
</div>
</div>
<p><br/></p>
</div>
<div id="model-validation-expand" class="section level4">
<h4>Model Validation
<a href="javascript:showhide('validation')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">Verifying a model’s ability to
generalize to new data…</span></p>
<div id="validation" style="display:none;">
<p>The following graph shows three things: (1) a true regression model,
(2) a simple linear regression model that doesn’t quite capture the full
pattern in the data, and (3) a complicated model that seems to overly
fit the data as it fits better than even the true model.</p>
<pre class="r"><code>set.seed(123) #gives us the same randomness 
n &lt;- 20 #sample size
x &lt;- runif(n, -1.5, 3.8) #uniform X from -1.5 to 3.8
# Coefficients for the true model:
beta0 &lt;- 2
beta1 &lt;- -2.5
beta2 &lt;- 1
beta3 &lt;- 3
beta4 &lt;- -0.8
# Get y-value using a true model
y &lt;- beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4 + rnorm(n, 0, 0.5) #normal errors

thedata &lt;- data.frame(y, x)

# Plot it
par(mai=c(.1,.5,.2,.1))
plot(y ~ x, data=thedata, pch=21, col=&quot;lightgray&quot;, bg=&quot;steelblue&quot;, cex=1.3, ylim=c(-5,22), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylab=&quot;&quot;, xlab=&quot;&quot;)
mtext(side=3, text=&quot;Original Data (Training Data)&quot;, cex=0.7, at=-.8, line=.1)

# Draw true model
curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4)
lmt &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later

# Draw simple linear model
lms &lt;- lm(y ~ x, data=thedata)
b &lt;- coef(lms)
curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2)

# Draw overly complicated model
lmo &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata)
b &lt;- coef(lmo)
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2)

# Add legend
legend(&quot;topleft&quot;, legend=c(&quot;True Model&quot;, &quot;Simple Model&quot;, &quot;Complicated Model&quot;), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty=&#39;n&#39;)</code></pre>
<p><img src="LinearRegression_files/figure-html/problem-1.png" width="480" /></p>
<pre class="r"><code>my_output_table &lt;- data.frame(Model = c(&quot;True&quot;, &quot;Simple&quot;, &quot;Complicated&quot;), `R^2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Adjusted R^2` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared))

colnames(my_output_table) &lt;- c(&quot;Model&quot;, &quot;$R^2$&quot;, &quot;Adjusted $R^2$&quot;)

knitr::kable(my_output_table)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="right"><span class="math inline">\(R^2\)</span></th>
<th align="right">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">True</td>
<td align="right">0.9958725</td>
<td align="right">0.9947718</td>
</tr>
<tr class="even">
<td align="left">Simple</td>
<td align="right">0.8114836</td>
<td align="right">0.8010105</td>
</tr>
<tr class="odd">
<td align="left">Complicated</td>
<td align="right">0.9984527</td>
<td align="right">0.9941204</td>
</tr>
</tbody>
</table>
<p>Now, let’s remind ourselves why we use regression models in the first
place. The main goal is to capture the “essence” of the data. In other
words, the general pattern is what we are after. We want a model that
tells us how “all such” data is created, not just the specific data we
have sampled. So, the great test of a model is to see how well it works
on a new sample of data.</p>
<p>This is precisely <strong>model validation</strong>, the verification
that a model fit on one sample of data, continues to perform well on a
new sample of data.</p>
<pre class="r"><code>set.seed(14551) #get same random sample
# Get a new sample of data from the true model
Xnew &lt;- runif(n, -1.4, 3.7) #uniform X from -1.5 to 3.8
Ynew &lt;- beta0 + beta1*Xnew + beta2*Xnew^2 + beta3*Xnew^3 + beta4*Xnew^4 + rnorm(n, 0, 0.5) #normal errors

thedata2 &lt;- data.frame(y=Ynew, x=Xnew)

# Plot it
par(mai=c(.1,.5,.2,.1))
plot(y ~ x, data=thedata, pch=21, col=rgb(.827451,.827451,.827451, .1), bg=rgb(.2745098,.5098039,.7058824, .2), cex=1.3, ylim=c(-5,22), yaxt=&#39;n&#39;, xaxt=&#39;n&#39;, ylab=&quot;&quot;, xlab=&quot;&quot;)
mtext(side=3, text=&quot;New Data (Testing Data)&quot;, cex=0.7, at=-.8, line=.1)

# Draw true model
curve(beta0 + beta1*x + beta2*x^2 + beta3*x^3 + beta4*x^4, add=TRUE, col=rgb(0.2745098, 0.5098039, 0.7058824, .5), lwd=4)
lmt &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=thedata) #for later

# Draw simple linear model
lms &lt;- lm(y ~ x, data=thedata)
b &lt;- coef(lms)
curve(b[1] + b[2]*x, add=TRUE, col=rgb(1,0.6470588,0, .3), lwd=2)

# Draw overly complicated model
lmc &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14), data=thedata)
b &lt;- coef(lmc)
curve(b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3 + b[5]*x^4 + b[6]*x^5 + b[7]*x^6 + b[8]*x^7 + b[9]*x^8 + b[10]*x^9 + b[11]*x^10 + b[12]*x^11 + b[13]*x^12 + b[14]*x^13 + b[15]*x^14, add=TRUE, col=rgb(0.6980392, 0.133333, 0.133333, .2), lwd=2)

# Add new data to plot
points(y ~ x, data=thedata2, pch=21, col=rgb(.827451,.827451,.827451, .5), bg=&quot;orange&quot;, cex=1.3)

# Add legend
legend(&quot;topleft&quot;, legend=c(&quot;True Model&quot;, &quot;Simple Model&quot;, &quot;Complicated Model&quot;), lwd=c(4,2,2), col=c(rgb(0.2745098, 0.5098039, 0.7058824, .5), rgb(1,0.6470588,0, .3), rgb(0.6980392, 0.133333, 0.133333, .2)), bty=&#39;n&#39;)

# Add dot legend
legend(&quot;bottomright&quot;, legend=c(&quot;Original Sample&quot;, &quot;New Sample&quot;), pch=16, col=c(rgb(.2745098,.5098039,.7058824, .2),&quot;orange&quot;), bty=&#39;n&#39;)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-84-1.png" width="480" /></p>
<pre class="r"><code># Compute R-squared for each validation

  # Get y-hat for each model on new data.
  yht &lt;- predict(lmt, newdata=thedata2)
  yhs &lt;- predict(lms, newdata=thedata2)
  yhc &lt;- predict(lmc, newdata=thedata2)
  
  # Compute y-bar
  ybar &lt;- mean(thedata2$y) #Yi is given by Ynew from the new sample of data
  
  # Compute SSTO
  SSTO &lt;- sum( (thedata2$y - ybar)^2 )
  
  # Compute SSE for each model using y - yhat
  SSEt &lt;- sum( (thedata2$y - yht)^2 )
  SSEs &lt;- sum( (thedata2$y - yhs)^2 )
  SSEc &lt;- sum( (thedata2$y - yhc)^2 )
  
  # Compute R-squared for each
  rst &lt;- 1 - SSEt/SSTO
  rss &lt;- 1 - SSEs/SSTO
  rsc &lt;- 1 - SSEc/SSTO
  
  # Compute adjusted R-squared for each
  n &lt;- length(thedata2$y) #sample size
  pt &lt;- length(coef(lmt)) #num. parameters in model
  ps &lt;- length(coef(lms)) #num. parameters in model
  pc &lt;- length(coef(lmc)) #num. parameters in model
  rsta &lt;- 1 - (n-1)/(n-pt)*SSEt/SSTO
  rssa &lt;- 1 - (n-1)/(n-ps)*SSEs/SSTO
  rsca &lt;- 1 - (n-1)/(n-pc)*SSEc/SSTO
  

my_output_table2 &lt;- data.frame(Model = c(&quot;True&quot;, &quot;Simple&quot;, &quot;Complicated&quot;), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmo)$r.squared), `Orig. Adj. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmo)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca))

colnames(my_output_table2) &lt;- c(&quot;Model&quot;, &quot;Original $R^2$&quot;, &quot;Original Adj. $R^2$&quot;, &quot;Validation $R^2$&quot;, &quot;Validation Adj. $R^2$&quot;)

knitr::kable(my_output_table2, escape=TRUE, digits=4)</code></pre>
<table>
<colgroup>
<col width="13%" />
<col width="17%" />
<col width="23%" />
<col width="19%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="right">Original <span class="math inline">\(R^2\)</span></th>
<th align="right">Original Adj. <span
class="math inline">\(R^2\)</span></th>
<th align="right">Validation <span
class="math inline">\(R^2\)</span></th>
<th align="right">Validation Adj. <span
class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">True</td>
<td align="right">0.9959</td>
<td align="right">0.9948</td>
<td align="right">0.9928</td>
<td align="right">0.9908</td>
</tr>
<tr class="even">
<td align="left">Simple</td>
<td align="right">0.8115</td>
<td align="right">0.8010</td>
<td align="right">0.8002</td>
<td align="right">0.7891</td>
</tr>
<tr class="odd">
<td align="left">Complicated</td>
<td align="right">0.9985</td>
<td align="right">0.9941</td>
<td align="right">0.8686</td>
<td align="right">0.5008</td>
</tr>
</tbody>
</table>
<p>Notice how the <span class="math inline">\(R^2\)</span> for the
complicated model dropped fairly dramatically from its original value of
0.9985 to 0.8686, and the adjusted <span
class="math inline">\(R^2\)</span> dropped from 0.994 to 0.501! On the
other hand, the <span class="math inline">\(R^2\)</span> and adjusted
<span class="math inline">\(R^2\)</span> values for the True and Simple
model were relatively unchanged. This is clear evidence that the
“complicated model” is overfitting the original data. It does not
capture the “essence” of the data, so it is not a generalizable model.
It does not fit new data very well, even though it fit the original
sample of data quite well. This is what we mean by <strong>over
fitting</strong> a model to a particular sample of data.</p>
</div>
<p><br/></p>
</div>
<div id="interpretation-expand" class="section level4">
<h4>Interpretation
<a href="javascript:showhide('interpretationMultiple')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption"><span
class="math inline">\(\beta_j\)</span> is the change in the average
y-value…</span></p>
<div id="interpretationMultiple" style="display:none;">
<p>The only change to interpretation from the simple linear regression
model is that each coefficient, <span
class="math inline">\(\beta_j\)</span> <span
class="math inline">\(j=1,\ldots,p\)</span>, represents the change in
the <span class="math inline">\(E\{Y\}\)</span> for a unit change in
<span class="math inline">\(X_j\)</span>, <em>holding all other
variables constant.</em></p>
</div>
<p><br /></p>
</div>
<div id="added-variable-plots-expand" class="section level4">
<h4>Added Variable Plots
<a href="javascript:showhide('addedVariablePlots')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">When to add another <span
class="math inline">\(X\)</span>-variable to the model…</span></p>
<div id="addedVariablePlots" style="display:none;">
<p>The assumptions of multiple linear regression are nearly identical to
simple linear regression, with the addition of one new assumption.</p>
<ol style="list-style-type: decimal">
<li>The regression relation between <span
class="math inline">\(Y\)</span> and <span
class="math inline">\(X\)</span> is linear.</li>
<li>The error terms are normally distributed with <span
class="math inline">\(E\{\epsilon_i\}=0\)</span>.</li>
<li>The variance of the error terms is constant over all <span
class="math inline">\(X\)</span> values.</li>
<li>The <span class="math inline">\(X\)</span> values can be considered
fixed and measured without error.</li>
<li>The error terms are independent.</li>
<li>All important variables are included in the model.</li>
</ol>
<p><br /></p>
<div id="check" class="section level4">
<h4>Checking the Assumptions</h4>
<p>The process of checking assumptions is the same for multiple linear
regression as it is for simple linear regression, with the addition of
one more tool, the added variable plot. Added variable plots can be used
to determine if a new variable should be included in the model.</p>
<table width="90%">
<tr>
<td with="15%">
<img src="LinearRegression_files/figure-html/unnamed-chunk-85-1.png" width="144" />
</td>
<td width="75%">
<p>Let <span class="math inline">\(X_{new}\)</span> be a new explanatory
variable that could be added to the current multiple regression model.
Plotting the residuals from the current linear regression against <span
class="math inline">\(X_{new}\)</span> allows us to determine if <span
class="math inline">\(X_{new}\)</span> has any information to add to the
current model. If there is a trend in the plot, then <span
class="math inline">\(X_{new}\)</span> should be added to the model. If
there is no trend in the plot, then the <span
class="math inline">\(X_{new}\)</span> should be left out.</p>
<p>|
<a href="javascript:showhide('addedvariableplots')" style="font-size:.8em;color:steelblue2;">Show
Examples</a> |</p>
</td>
</tr>
</table>
<div id="addedvariableplots" style="display:none;">
<p><a href="javascript:showhide('addedvariableplotsread')" style="font-size:.8em;color:skyblue;">(Read
more…)</a></p>
<div id="addedvariableplotsread" style="display:none;">
<p>An added variable plot checks to see if a new variable has any
information to add to the current multiple regression model.</p>
<p>The plot is made by taking the residuals from the current multiple
regression model (<span class="math inline">\(y\)</span>-axis) and
plotting them against the new explanatory variable (<span
class="math inline">\(x\)</span>-axis).</p>
<ul>
<li><p>If there is a trend in the added variable plot, then the new
explanatory variable contains extra information that is not already
contained in the current multiple regression. The new variable should be
included in the model.</p></li>
<li><p>If there is no trend in the added variable plot, then the
information provided by the new explanatory variable is already
contained in the current multiple regression model. The new variable
should continue to be left out of the model.</p></li>
</ul>
<p>The left column of plots below show scenarios where the new
explanatory variable should be included in the model. The right column
of plots show scenarios where the new explanatory variable should not be
included in the model.</p>
</div>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-86-1.png" width="672" /><img src="LinearRegression_files/figure-html/unnamed-chunk-86-2.png" width="672" /></p>
</div>
</div>
</div>
<p><br /></p>
</div>
<div id="outlier-analysis-expand" class="section level4">
<h4>Outlier Analysis
<a href="javascript:showhide('outlierAnalysis')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">Cook’s Distances and Leverage
Values…</span></p>
<div id="outlierAnalysis" style="display:none;">
<p>The presence of outlying points in a regression can bias the
regression estimates substantially. In simple linear regressions, the
outlier are usually quite visible in a residuals vs. fitted-values plot.
However, in higher dimensional regression models, it can become very
difficult to locate points that are negatively effecting the regression.
Here are two measurements that are helpful in identifying points that
are negatively impacting an estimated regression model.</p>
<div id="cooks-distances" class="section level5">
<h5>Cook’s Distances</h5>
<p>The idea behind Cook’s Distance is to measure the impact each
individual point has on the regression estimates <span
class="math inline">\(b_i\)</span> for each <span
class="math inline">\(\beta_i\)</span>. As found in the original article
<a
href="http://www.stat.ucla.edu/~nchristo/statistics100C/1268249.pdf">“Detection
of Influential Observation in Linear Regression” (Dennis Cook, 1977)</a>
the formula Cook developed for measuring this effect is given by (when
adapted to fit the notation of this book)</p>
<p><span class="math display">\[
  D_i = \frac{\sum_{j=1}^n (\widehat{Y}_{j} -
\widehat{Y}_{j(i)})^2}{p\cdot MSE}
\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of
parameters in the regression model, <span
class="math inline">\(MSE\)</span> is the estimate of <span
class="math inline">\(\sigma^2\)</span> (the mean squared error), and
<span class="math inline">\(\hat{Y}_{j(i)}\)</span> represents the
residual for point <span class="math inline">\(j\)</span> when the <span
class="math inline">\(i\)</span>th point was removed from the
regression.</p>
<p>To understand this formula, let’s focus first on the numerator: <span
class="math inline">\(\sum_{j=1}^n \widehat{Y}_j -
\widehat{Y}_{j(i)}\)</span>. Here, we are comparing the residual from
the original regression for point <span
class="math inline">\(j\)</span>, <span
class="math inline">\(\widehat{Y}_j\)</span> to the modified value of
that same residual when point <span class="math inline">\(i\)</span> is
removed from the regression. See the image below for a visual
explanation.</p>
<pre class="r"><code>X &lt;- c(2,3,5,6,8,13)
Y &lt;- c(3,5,7,9,8,12)

plot(Y ~ X, pch=16, col=&quot;skyblue&quot;, ylim=c(0,14))
points(X[4],Y[4], pch=16, cex=1.1, col=&quot;orange&quot;)
lm1 &lt;- lm(Y ~ X)
lm2 &lt;- lm(Y ~ X, w=c(1,1,1,0,1,1))
abline(lm1, col=&quot;skyblue&quot;, lwd=2)
abline(lm2, col=&quot;orange&quot;, lwd=2)
legend(&quot;topleft&quot;, legend=c(&quot;All Points Included&quot;, &quot;Orange Point Removed&quot;), lty=1, col=c(&quot;skyblue&quot;,&quot;orange&quot;), bty=&quot;n&quot;)

for (i in 1:6){
  lines(c(X[i]+.03,X[i]+.03), c(Y[i], lm1$fit[i]), lty=1, col=&quot;skyblue&quot;)
  lines(c(X[i]-.03,X[i]-.03), c(Y[i], lm2$fit[i]), lty=1, col=&quot;orange&quot;)
}</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-87-1.png" width="672" /></p>
<pre class="r"><code>pander(round(rbind(`Original Residuals` = lm1$residuals, `Orange Point Removed` = lm2$residuals, Difference = lm1$residuals - lm2$residuals),2))</code></pre>
<table>
<colgroup>
<col width="36%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Original Residuals</strong></td>
<td align="center">-1.23</td>
<td align="center">0.02</td>
<td align="center">0.53</td>
<td align="center">1.79</td>
<td align="center">-0.7</td>
<td align="center">-0.42</td>
</tr>
<tr class="even">
<td align="center"><strong>Orange Point Removed</strong></td>
<td align="center">-0.86</td>
<td align="center">0.4</td>
<td align="center">0.9</td>
<td align="center">2.15</td>
<td align="center">-0.35</td>
<td align="center">-0.09</td>
</tr>
<tr class="odd">
<td align="center"><strong>Difference</strong></td>
<td align="center">-0.38</td>
<td align="center">-0.37</td>
<td align="center">-0.36</td>
<td align="center">-0.36</td>
<td align="center">-0.35</td>
<td align="center">-0.33</td>
</tr>
</tbody>
</table>
<p>Squaring the sum of the “differences” in the residuals from the
original regression and the one where point <span
class="math inline">\(i\)</span> (the orange dot) has been removed gives
<span class="math inline">\(0.77186\)</span>. Then, noting that the MSE
for the original regression was <span
class="math inline">\(1.418605\)</span>, and that <span
class="math inline">\(p=2\)</span> because there were two parameters, we
find the Cook’s Distance for Point #4 comes out to be</p>
<p><span class="math display">\[
  D_4 = \frac{\sum_{j=1}^n (\widehat{Y}_{j} -
\widehat{Y}_{j(4)})^2}{p\cdot MSE} \approx \frac{0.77186}{2\cdot
1.418605} \approx 0.272
\]</span></p>
<p>Similar calculations show the Cook’s Distances for each point to
be</p>
<pre class="r"><code>pander(round(cooks.distance(lm1),3), caption=&quot;Cook&#39;s Distances for each Point 1, ..., 6&quot;)</code></pre>
<table style="width:61%;">
<colgroup>
<col width="11%" />
<col width="5%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.551</td>
<td align="center">0</td>
<td align="center">0.028</td>
<td align="center">0.272</td>
<td align="center">0.057</td>
<td align="center">0.807</td>
</tr>
</tbody>
</table>
<p>In R, it is simple to calculate Cook’s Distances using the code
<code>cooks.distance(lmObject)</code>. Also, a graph of Cook’s Distances
can be obtained using <code>plot(lmObject, which=4)</code> as shown
here:</p>
<pre class="r"><code>plot(lm1, which=4)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-89-1.png" width="672" /></p>
</div>
<div id="leverage-values" class="section level5">
<h5>Leverage Values</h5>
<p>The leverage value of a point is a measurement that lives between 0
and 1 where values close to 1 imply the point has a lot of “leverage”
and is “pulling” the regression toward itself. A value near 0 implies
the point is just “one of many” and that it is not unduly influencing
the regression line.</p>
<p>It is difficult to understand leverage values mathematically unless
we look at regression from a linear algebra (matrix) perspective.</p>
<p>To do this, first recall the simple linear regression model</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\]</span></p>
<p>This could be expanded to explicity list out each value of <span
class="math inline">\(i\)</span> in the model using vector notation:</p>
<p><span class="math display">\[
  \left[ \begin{array}{c} Y_1 \\ Y_2 \\ \vdots \\ Y_n\end{array}\right]
= \beta_0 \left[ \begin{array}{c} 1  \\
  1  \\
  \vdots \\ 1  \end{array}\right] + \beta_1 \left[\begin{array}{c} X_1
\\ X_2 \\ \vdots \\ X_n  \end{array}\right] + \left[\begin{array}{c}
\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{array}\right]
\]</span></p>
<p>We could then rewrite this in matrix notation using</p>
<p><span class="math display">\[
  \left[ \begin{array}{c} Y_1 \\ Y_2 \\ \vdots \\ Y_n\end{array}\right]
=  \left[ \begin{array}{cc} 1 &amp; X_1\\
  1  &amp; X_2 \\
  \vdots &amp; \vdots \\
  1 &amp; X_n \end{array}\right] \left[\begin{array}{c} \beta_0 \\
\beta_1 \end{array}\right] + \left[\begin{array}{c} \epsilon_1 \\
\epsilon_2 \\ \vdots \\ \epsilon_n \end{array}\right]
\]</span></p>
<p>Or, more concisely as</p>
<p><span class="math display">\[
  \vec{Y} = \mathbf{X}\vec{\beta} + \vec{\epsilon}
\]</span></p>
<p>The goal of regression is to choose values for <span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> that “minimize” the sum of the
squared errors. Mathematically this would be written as</p>
<p><span class="math display">\[
  \sum_{i=1}^n \epsilon_i ^2
\]</span></p>
<p>If you are familiar with vectors then you would see that this could
be written with the notation</p>
<p><span class="math display">\[
  \vec{\epsilon}^t \vec{\epsilon} = \sum_{i=1}^n \epsilon_i ^2
\]</span></p>
<p>And since we can also write</p>
<p><span class="math display">\[
  \vec{\epsilon} = \vec{Y} - \mathbf{X}\vec{\beta}
\]</span></p>
<p>then we have</p>
<p><span class="math display">\[
\sum_{i=1}^n \epsilon_i^2 = \vec{\epsilon}^t \vec{\epsilon} = (\vec{Y} -
\mathbf{X}\vec{\beta})^t (\vec{Y} - \mathbf{X}\vec{\beta})
\]</span></p>
<p>To choose the values of <span
class="math inline">\(\vec{\beta}\)</span> that minimize the above
equation, we will take the derivative with respect to <span
class="math inline">\(\vec{\beta}\)</span> which turns out to give</p>
<p><span class="math display">\[
\frac{d}{d\vec{\beta}}(\vec{Y} - \mathbf{X}\vec{\beta})^t (\vec{Y} -
\mathbf{X}\vec{\beta}) = -2\mathbf{X}^t(\vec{Y} - \mathbf{X}\vec{\beta})
\]</span></p>
<p>Setting the derivative equal to the zero vector <span
class="math inline">\(\vec{0}\)</span> and solving, we obtain</p>
<p><span class="math display">\[
-2\mathbf{X}^t(\vec{Y} - \mathbf{X}\vec{\beta}) = \vec{0} \\
-2\mathbf{X}^t\vec{Y} = -2\mathbf{X}^t\mathbf{X}\vec{\beta}) \\
\mathbf{X}^t\vec{Y} = \mathbf{X}^t\mathbf{X}\vec{\beta}
\]</span></p>
<p>Since <span class="math inline">\(\mathbf{X}^t\mathbf{X}\)</span> is
a square matrix, it is invertible. This allows us to solve for <span
class="math inline">\(\vec{\beta}\)</span> by</p>
<p><span class="math display">\[
(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\vec{Y} = \vec{\beta}
\]</span></p>
<p>However, at this point istead of pretending we have found the true
<span class="math inline">\(\beta\)</span>’s, we change the equation
to</p>
<p><span class="math display">\[
\vec{b} = (\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\vec{Y}
\]</span></p>
<p>Then, if we use the equation for <span
class="math inline">\(\hat{Y}_i\)</span> in vector notation, we get</p>
<p><span class="math display">\[
  \hat{\vec{Y}} = \mathbf{X}\vec{b}
\]</span></p>
<p>and substituting into <span class="math inline">\(\vec{b}\)</span>
gives</p>
<p><span class="math display">\[
  \hat{\vec{Y}} =
\mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t\vec{Y}
\]</span></p>
<p>This shows the <span class="math inline">\(\hat{Y}\)</span> values
are a matrix transformation of the <span
class="math inline">\(Y\)</span> values, often called a projection of
<span class="math inline">\(Y\)</span> onto the <span
class="math inline">\(\hat{Y}\)</span> surface. But now we have arrived
at the thing we wanted to look at in order to talk about leverage, the
“hat matrix” <span class="math inline">\(\mathbf{H}\)</span>:</p>
<p><span class="math display">\[
  \mathbf{H} = \mathbf{X}(\mathbf{X}^t\mathbf{X})^{-1}\mathbf{X}^t
\]</span></p>
<p>This allows us to write</p>
<p><span class="math display">\[
\hat{\vec{Y}} = \mathbf{H}\vec{Y}
\]</span></p>
<p>The diagonal elements of <span
class="math inline">\(\mathbf{H}\)</span> are the “leverage values” and
are notated as the <span class="math inline">\(h_{ii}\)</span> values.
Essentially each of these values explain how much <span
class="math inline">\(\hat{Y}_i\)</span> is being pulled towards <span
class="math inline">\(Y_i\)</span> by each <span
class="math inline">\(Y_i\)</span>, where values of <span
class="math inline">\(h_{ii}\)</span> close to 1 represent a “lot of
pull,” and values close to 0 represent “little pull.”</p>
<p>In R these values are obtained by the <code>hatvalues(...)</code>
function:</p>
<pre class="r"><code>hatvalues(lm1) %&gt;% pander()</code></pre>
<table style="width:74%;">
<colgroup>
<col width="12%" />
<col width="12%" />
<col width="12%" />
<col width="11%" />
<col width="12%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.3869</td>
<td align="center">0.2939</td>
<td align="center">0.1839</td>
<td align="center">0.167</td>
<td align="center">0.2093</td>
<td align="center">0.759</td>
</tr>
</tbody>
</table>
<p>Or, graphically depicted by <code>plot(lmObject, which=5)</code></p>
<pre class="r"><code>plot(lm1, which=5)</code></pre>
<p><img src="LinearRegression_files/figure-html/unnamed-chunk-91-1.png" width="672" /></p>
<p>Points with “lots of leverage” and a large “Cook’s Distance” are
points that should be investigated for accuracy and possibly removed (or
downweighted) in the regression.</p>
</div>
</div>
<p><br /></p>
</div>
<div id="inference-for-the-model-parameters-expand-1"
class="section level4">
<h4>Inference for the Model Parameters
<a href="javascript:showhide('inferenceMultiple')" style="font-size:.6em;color:skyblue;">(Expand)</a></h4>
<p><span class="expand-caption">t Tests and F tests in multiple
regression…</span></p>
<div id="inferenceMultiple" style="display:none;">
<p>Inference in the multiple regression model can be for any of the
model coefficients, <span class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span>, <span
class="math inline">\(\ldots\)</span>, <span
class="math inline">\(\beta_p\)</span> or for several coefficients
simultaneously.</p>
<p><br /></p>
<div id="t-tests" class="section level5">
<h5>t Tests</h5>
<p>The most typical tests for multiple regression are t Tests for a
single coefficient. The hypotheses for these t Tests are written as
<span class="math display">\[
  H_0: \beta_j = 0
\]</span> <span class="math display">\[
  H_a: \beta_j \neq 0
\]</span> Note that these hypotheses assume that all other variables
(and coefficients) are already in the model. The significance of the
single variable is thus assessed after accounting for the effect of all
other variables. If a t Test of a single coefficient is significant,
then that variable should remain in the model. If the t Test for a
single coefficient is not significant, then the other variables in the
model provide the same information that the variable being tested
provides. Removing it from the model may be appropriate. However,
whenever a single variable is removed from the model the other variables
can change in their significance.</p>
<p><br /></p>
</div>
<div id="f-tests" class="section level5">
<h5>F Tests</h5>
<p>Another approach to testing hypotheses about coefficients is to use
an F Test. The F Test allows a single test for any group of hypotheses
simultaneously.</p>
<p>The most commonly used F Test is the one given by the hypotheses
<span class="math display">\[
  H_0: \beta_1 = \cdots = \beta_p = 0
\]</span> <span class="math display">\[
  H_a: \beta_j \neq 0 \ \text{for at least one}\ j \in \{1,\ldots,p\}
\]</span> However, any subset of coefficients could be tested in a
similar way using a customized F Test. The details of how to do this are
somewhat involved and are beyond the scope of this class.</p>
</div>
</div>
<p><br /> <br /></p>
<hr />
</div>
</div>
</div>
</div>
<div id="section-3" class="section level2">
<h2></h2>
<div style="padding-left:125px;">
<p><strong>Examples:</strong> <a
href="./Analyses/Linear%20Regression/Examples/CivicVsCorollaMLR.html">Civic
Vs Corolla</a> <a
href="./Analyses/Linear%20Regression/Examples/cadillacsMLR.html">cadillacs</a></p>
</div>
<hr />
<footer>
</footer>
</div>
</div>
</div>
</div>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
